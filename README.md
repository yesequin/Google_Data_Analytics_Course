# GOOGLE DATA ANALYTICS

[Notion Version](https://www.notion.so/GOOGLE-DATA-ANALYTICS-e082e3eea7dd4dbaa209e978c51000b5) ![notion](Google_Data_Analytics_images/notion.PNG)

- GLOSSARY
    
    “##”: it tells the server (BigQuery) that this is a description and not part of the code.
    
    A/B testing: The process of testing two variations of the same web page to determine which page is more successful at attracting user traffic and generating revenue
    
    Absolute reference: a reference that is locked so that rows and columns won't change when copied.
    
    Aggregation: collecting or gathering many separate pieces into a whole.
    
    Alternative text: Provides a textual alternative to non-text content.
    
    Analysis: the process used to make sense of the data collected.
    
    Analytical skills: Qualities and characteristics associated with using facts to solve problems
    Analytical thinking: The process of identifying and defining a problem, then solving it by using data in an organized, step-by-step manner
    
    Annotation: Text that briefly explains data or helps focus the audience on a particular aspect of the data in a visualization.
    
    Array: A collection of values in cells
    
    Big data: Large, complex datasets typically involving long periods of time, which enable data
    analysts to address far-reaching business problems
    
    Borders: Lines that can be added around two or more cells on a spreadsheet
    
    Business task: A business task is the question or problem data analysis answers for business.
    
    Calculated field: A new field within a pivot table that carries out certain calculations based on the values of other fields.
    
    CASE statement: The CASE statement goes through one or more conditions and returns a value as soon as a condition is met.
    
    Case study: A common way for employers to assess job skills and gain insight into how you approach common data related challenges.
    
    Causation: Occurs when an action directly leads to an outcome. It is also called a cause-effect relationship.
    
    Cell reference: A cell or a range of cells in a worksheet typically used in formulas and functions.
    
    Changelog: a file containing a chronologically ordered list of modifications made to a project.
    
    Clean data: data that's complete, correct, and relevant to the problem you're trying to solve.
    
    Coding: writing instructions to the compter in the syntax of a specific programming language.
    
    Compatibility: how well two or more datasets are able to work together.
    
    Computer programming: Giving instructions to a computer to perform an action or set of actions.
    
    CONCAT: allows you to join multiple text strings from multiple sources
    
    CONCATENATE: a function that joins multiple text strings into a single string.
    
    Conditional formatting: a spreadsheet tool that changes how cells appear when values meet specific conditions.
    
    Confidence interval: The range of possible values that the population’s result would be at the confidence level of the study. This range is the sample result +/- the margin of error.
    
    Confidence level: How confident you are in the survey results. Confidence level is targeted before you start your study because it will affect how big your margin of error is at the end of your study.
    
    Context: The condition in which something exists or happens
    
    Correlation: in statistics is the measure of the degree to which two variables move in relationship to each other.
    
    COUNTA: counts the total number of values within a specified range.
    
    COUNTIF: a function that returns the number of cells that match a specified value.
    
    CSV: Comma-separated values. A CSV file saves data in a table format
    
    Dashboard: A tool that monitors live, incoming data. A tool that organizes information from multiple datasets into one central location for tracking, analysis, and simple visualization.
    
    Dashboard filter: A tool for showing only the data that meets a specific criteria while hiding the rest.
    
    Data: A collection of facts
    
    Data aggregation: the process of gathering data from multiple sources in order to combine it into a single summarized collection.
    
    Data analysis: The collection, transformation, and organization of data in order to draw conclusions, make predictions, and drive informed decision-making
    
    Data analyst: Someone who collects, transforms, and organizes data in order to drive informed decision-making
    
    Data analytics: The science of data
    
    Data blending: A Tableau method that combines data from multiple data sources
    
    Data composition: combining the individual parts in a visualization and displaying them together as a whole.
    
    Data elements: pieces of information, such as people's names, account numbers, and addresses
    
    Data integrity: is the accuracy, completeness, consistency, and trustworthiness of data throughout its lifecycle.
    
    Data model: A model that is used for organizing data elements and how they relate to one another.
    
    Database: A database is a collection of data stored in a computer system. When you maintain a database of customer information, ensuring data integrity, credibility, and privacy are all important concerns.
    Data-driven decision-making: Using facts to guide business strategy. Data-driven decision-making is when facts that have been discovered through data analysis are used to guide business strategy.
    Data ecosystem: The various elements that interact with one another in order to produce,
    manage, store, organize, analyze, and share data
    
    Data engineers: transform data into a useful format for analysis and give it a reliable infrastructure.
    
    Data mapping: the process of matching fields from one database to another.
    
    Data merging: the process of combining two or more datasets into a single dataset.
    Data science: A field of study that uses raw data to create new ways of modeling and understanding the unknown
    Dataset: A collection of data that can be manipulated or analyzed as one unit
    
    Data validation: a tool for checking the accuracy and quality of data before adding or importing it.
    
    Data validation process: Checking and rechecking the quality of your data so that it is complete, accurate, secure, and consistent.
    
    Data visualization: The graphic representation and presentation of data.
    
    Data warehousing specialist: develop processes and procedures to effectively store and organize data.
    
    Delimiter: a character that indicates the beginning or end of a data item.
    
    Deliverables:  are items or tasks you will complete before you can finish the project.
    
    Design thinking: a process used to solve complex problems in a user-centric way.
    
    Dirty data: data that's incomplete, incorrect, or irrelevant to the problem you're trying to solve.
    
    Documentation: the process of tracking changes, additions, deletions, and errors involved in your data cleaning effort.
    
    Elevator pitch: A short statement describing an idea or concept
    
    Engagement: Capturing and holding someone’s interest and attention during a data presentation
    
    EXTRACT command (SQL): Lets us pull one part of a given data to use
    
    Historical data: Data that already exist. We use this kind of data when you need an answer immediately.
    
    Fairness: Fairness means ensuring that your analysis doesn't create or reinforce bias. In other words, as a data analyst, you want to help create systems that are fair and inclusive to everyone. Fairness also means crafting questions that make sense to everyone.
    
    Field: a single piece of information from a row or column of a spreadsheet.
    
    Field length: a tool for determining how many characters can be keyed into a field.
    
    Filtering: showing only the data that meets a specific criteria while hiding the rest.
    
    Find and replace: a tool that looks for a specified search term in a spreadsheet and allows you to replace it with something else.
    
    First-party data: Data collected by an individual or group using their own resources. It is typically the preferred method because you know exactly where it came from.
    
    Float: a number that contains a decimal
    
    Foreign key: a field within a table that is a primary key in another table.
    
    Formula: A set of instructions used to perform a calculation using the data in a spreadsheet
    
    Framework: The context a presentation needs to create logical connections that tie back to the business task and metrics
    Function: A preset command that automatically performs a specified process or task using the data in a spreadsheet
    
    Function (R): A body of reusable code used to perform specific tasks in R
    
    Gap analysis: A method for examining and evaluating the current state of a process in order to identify opportunities for improvement in the future
    
    HAVING statement: allows you to add a filter to your query instead of the underlying table when you're working with aggregate functions.
    
    Header: The first row in a spreadsheet that labels the type of data in each column
    
    HTML: The set of markups symbols or codes used to create a webpage
    
    Hypothesis: The theory you’re trying to prove or disprove with data
    
    Hypothesis testing: is a way to see if a survey or experiment has meaningful results.
    
    Integrated Development Enviroment (IDE): A software application that brings together all the tools you may want to use in a single place.
    
    Interoperability: the ability of data systems and services to openly connect and share data.
    
    Issue: An issue is a topic or subject to investigate.
    
    JOIN: a SQL clause that's used to combine rows from two or more tables based on a related column.
    
    Leading question: A question that steers people toward a certain response
    
    LEFT: a function that gives you a set number of characters from the left side of a text string.
    
    LEN: a function that tells you the length of the text string by counting the number of characters it contains.
    
    Markdown: A syntax for formatting plain text files.
    
    MATCH: A function used to locate the position of a specific lookup value.
    
    Margin of error: Since a sample is used to represent a population, the sample’s results are expected to differ from what the result would have been if you had surveyed the entire population. This difference is called the margin of error. The smaller the margin of error, the closer the results of the sample are to what the result would have been if you had surveyed the entire population.
    
    Mental model: your thought process and the way you approach a problem
    
    Merge: an agreement that unites two organizations into a single new one.
    
    Metadata: is data about data.
    
    Metric goal: A metric goal is a measurable goal set by a company and evaluated using metrics. And just like there are a lot of possible metrics, there are lots of possible goals too.
    
    MID: a function that gives you a segment from the middle of a text string.
    
    Milestones: Milestones are significant tasks you will confirm along your timeline to help everyone know the project is on track.
    
    Modulo: An operator (%) that returns the remainder when one number is divided by another
    
    Null: an indication that a value does not exist in a data set. It is not the same as a zero.
    
    Open source: Code that is freely available and may be modified and shared by the people who use it.
    
    Openness (or open data): Free access, usage, and sharing of data.
    
    Operator: A symbol that names the type of operation or calculation to be performed in a formula.
    
    Outliers: are data points that are very different from similarly collected data and might not be reliable values.
    
    Pivot Table: A pivot table is a data summarization tool that is used in data processing. Pivot tables are used to summarize, sort, re-organize, group, count, total, or average data stored in a database. It allows its users to transform columns into rows and rows into columns.
    
    Population: all possible data values in a certain data set.
    
    Portfolio: Collection of case studies that can be shared with potential employers.
    
    Primary key: references a column in which each value is unique
    
    Problem: a problem is an obstacle or complication that needs to be worked out.
    
    Problem domain: the specific area of analysis that encompasses every activity affecting or affected by the problem.
    
    Problem types: The various problems that data analysts encounter, including categorizing things, discovering connections, finding patterns, identifying themes, making predictions, and spotting something unusual
    
    Professional relationship building: Building relationships by meeting people both in person and online (Refer to Networking)
    
    Profit margin: a percentage that indicates how many cents of profit have been generated for each dollar of sale.
    
    Programming languages: The words and symbols we use to write instructions for computers to follow
    
    Quartile: A quartile divides data points into four equal parts or quarters.
    
    Query language: A computer programming language used to communicate with a database
    
    Question: A question is designed to discover information
    
    R: A programming language frequently used for statistical anaysis, visualization, and other data analysis.
    
    Random sampling: a way of selecting a sample from a population so that every possible type of the sample has an equal chance of being chosen.
    
    Reframing: Restating a problem or challenge, then redirecting it toward a potential resolution
    
    Relational database: a database that contains a series of tables that can be connected to form relationships.
    
    Remove duplicates: A tool that automatically searches for and eliminates duplicate entries from a spreadsheet.
    
    Reports: Reports notify everyone as you finalize deliverables and meet milestones.
    
    Revenue: The total amount of income generated by the sale of goods or services
    
    RIGHT: a function that gives you a set number of characters from the right side of a text string.
    
    Root cause: The reason why a problem occurs
    
    Sample: a part of a population that is representative of the population.
    
    Sampling bias: is when a sample isn’t representative of the population as a whole. This means some members of the population are being overrepresented or underrepresented.
    
    Schema: a way of describing how something is organized.
    
    Second-party data: data collected by a group directly from its audience and then sold.
    
    Syntax: a predetermined structure that includes all required information and its proper placement.
    
    SMART methodology: A tool for determining a question’s effectiveness based on whether it is
    specific, measurable, action-oriented, relevant, and time-bound
    
    Soft skills: non-technical traits and behaviors that relate to how you work.
    
    Sorting: arranging data into a meaningful order to make it easier to understand, analyze, and visualize.
    
    Sort sheet: All of the data in a spreadsheet is sorted by the ranking of a specific sorted column - data across rows is kept together.
    
    Sort range: Nothing else on the spreadsheet is rearranged besides the specified cells in a column.
    
    Spotlighting: Scanning through data to quickly identify the most important insights.
    
    Split: a tool that divides a text string around the specified character and puts each fragment into a new and separate cell.
    
    Spreadsheet: A digital worksheet
    
    Statistical power: the probability of getting meaningful results from a test.
    
    Statistical significance: The determination of whether your result could be due to random chance or not. The greater the significance, the less due to chance.
    
    Stakeholders: People who have invested time and resources into a project and are interested in the outcome.
    
    Structured Query Language: A computer programming language used to communicate with a
    database
    Structured thinking: The process of recognizing the current problem or situation, organizing
    available information, revealing gaps and opportunities, and identifying options
    
    Subquery: is a SQL query that is nested inside of a larger query.
    
    SUMPRODUCT: a function that multiplies arrays and returns the sum of those products.
    
    Tableau: A business intelligence and analytics platform that helps people see, understand, and make decisions with data.
    
    Technical mindset: The ability to break things down into smaller steps or pieces and work with them in an orderly and logical way
    
    Temporary table: a database table that is created and exists temporarily on a database server
    
    Text string: a group of characters within a cell, most often composed of letters, numbers or both.
    
    Third-party data: Data collected from outside sources who did not collect it directly. This data might have come from a number of different sources before you investigated it. 
    
    Tidy data (R): A way of standardizing the organization of data within R.
    
    Time-bound question: A question that specifies a timeframe to be studied
    
    Transferable skills: skills and qualities that can transfer from one job or industry to another.
    
    TRIM: a function that removes leading, trailing, and repeated spaces in data.
    
    Turnover rate: the rate at which employees leave a company.
    
    Typecasting: Converting data from one type to another.
    
    Underscores: Lines used to underline words and connect text characters.
    
    Unfair question: A question that makes assumptions or is difficult to answer honestly
    
    VALUE: A function that converts a text string that represents a  number to a numerical value.
    
    Verification: a process to confirm that a data cleaning effort was well- executed and the resulting data is accurate and reliable.
    
    VLOOKUP (stands for vertical lookup): a function that searches for a certain value in a column to return a corresponding piece of information.
    
    YAML: A language for data that translates it so it’s readable
    
- 1 - FOUNDATIONS
    
    ## INTRODUCTION
    
    "Data! Data! Data! I can't make bricks without clay." This line was said by Sherlock Holmes, the famous detective created by Sir Arthur Conan Doyle. What Doyle meant was that Holmes couldn't draw any conclusions, which would be the bricks he mentioned without data, or the clay.
    Data is basically a collection of facts or information, and through analysis, you'll learn how to use the data to draw conclusions, and make predictions, and decisions.
    
    Phases of the data analysis process: Ask, prepare, process, analyze, share, and act.
    
    Data analysis is the collection, transformation, and organization of data in order to draw conclusions, make predictions, and drive informed decision-making.
    
    **People analytics:** is the practice of collecting and analyzing data on the people who make up a company’s workforce in order to gain insights to improve how the company operates.
    
    Being a people analyst involves using data analysis to gain insights about employees and how they experience their work lives. The insights are used to define and create a more productive and empowering workplace. This can unlock employee potential, motivate people to perform at their best, and ensure a fair and inclusive company culture.
    
    ### Steps of the data analysis process:
    
    1. **Ask** questions and define the problem.: Ask effective questions to define what the project would look like and what would qualify as a succesful result. The analysts asked questions to define both the issue to be solved and what would equal a successful result.
    2. **Prepare** data by collecting and storing the information.: solid preparation. Build a timeline. Also, we need to identify what data we need to achieve the succesful result we identified in the previous step
    3. **Process** data by cleaning and checking the information.: Great analysts know how to respect both their data and the people who provide it. The data analysts also made sure employees understood how their data would be collected, stored, managed, and protected. Collecting and using data ethically is one of the responsibilities of data analysts. They processed the data by cleaning it to make sure it was complete, correct, relevant, and free of errors and outliers.
    4. **Analyze** data to find patterns, relationships, and trends.: here you will discover and document what the analyst found  in the analysis
    5. **Share** data with your audience.: Just as they made sure the data was carefully protected, the analysts were also careful **sharing the report.** This process gave managers an opportunity to **communicate the results** with the right context. As a result, they could have productive team conversations about next steps to improve employee engagement.
    6. **Act** on the data and use the analysis results.: to work with leaders within their company and decide how best to **implement changes and take actions** based on the findings.
    
    **Decision Intelligence:** is a combination of applied data science and the social and managerial sciences. It is all about harnessing the power and beauty of data.
    
    Data science, the discipline of making data useful, is an umbrella term that encompasses three disciplines: machine learning, statistics, and analytics.
    
    - Statistics: if you want to make a few important decisions under uncertainty. Statisticians are essentially philosophers, epistemologists. They are very, very careful about protecting decision-makers from coming to the wrong conclusion.
    - Machine learning and AI: If you want to automate, in other words, make many, many, many decisions under uncertainty. Performance is the excellence of the machine learning and AI engineer.
    - Analytics: You want to encounter your unknown unknowns. You want to understand your world. The excellence of an analyst is speed. Don't worry about right answers. See how quickly you can unwrap this gift and find out if there is anything fun in there.
    
    ## DATA ECOSYSTEM
    
    An ecosystem is a group of elements that interact with one another. Ecosystems can be large, like the jungle in a tropical rainforest or the Australian outback. Or, tiny, like tadpoles in a puddle, or bacteria on your skin. Data lives inside its own ecosystem too.
    
    Data ecosystems are made up of various elements that interact with one another in order to produce, manage, store, organize, analyze, and share data. These elements include hardware and software tools, and the people who use them. Data can also be found in something called the cloud.
    
    **The cloud:** The cloud is a place to keep data online, rather than on a computer hard drive. So instead of storing data somewhere inside your organization's network, that data is accessed over the internet. So the cloud is just a term we use to describe the virtual location. The cloud plays a big part in the data ecosystem
    
    | Data Scientist | Data Analyst |
    | --- | --- |
    | Data science is defined as creating new ways of modeling and understanding the unknown by using raw data. | When you think about data, data analysis and the data ecosystem, it's important to understand that all of these things fit under the data analytics umbrella. |
    | Data scientists create new questions using data | Analysts find answers to existing questions by creating insights from data sources. |
    
    **Data Analysis:** is the collection, transformation, and organization of data in order to draw conclusions, make predictions, and drive informed decision-making.
    
    **Data analytics:** is the science of data.
    
    ### DATA-DRIVEN-DECISION-MAKING
    
    it is defined as using facts to guide business strategy. Organizations in many different industries are empowered to make better, data-driven decisions by data analysts all the time.
    
    **Steps:**
    
    1. Figuring out the business need: Usually this is a problem that needs to be solved
    2. Whatever the problem is, once it's defined, a data analyst finds data, analyzes it and uses it to uncover trends, patterns and relationships.
    
    Data-driven decision-making can be so powerful, it can make entire business methods obsolete.
    
    By ensuring that data is built into every business strategy, data analysts play a critical role in their companies' success, but it's important to note that no matter how valuable data-driven decision-making is, data alone will never be as powerful as data combined with human experience, observation, and sometimes even intuition.
    
    To get the most out of data-driven decision-making, it's important to include insights from people who are familiar with the business problem. These people are called **subject matter experts**, and they have the ability to look at the results of data analysis and identify any inconsistencies, make sense of gray areas, and eventually validate choices being made.
    
    ## DATA AND GUT INSTINCT
    
    Detectives and data analysts have a lot in common. Your job is all about following steps to collect and understand facts.
    
    Analysts use data-driven decision-making and follow a step-by-step process, but there are other factors that influence the decision-making process. Gut instinct is an intuitive understanding of something with little or no explanation. This isn’t always something conscious; we often pick up on signals without even realizing. You just have a “feeling” it’s right.
    
    ### Why gut instinct can be a problem
    
    - If you ignore data by preferring to make decisions based on your own experience, your decisions may be biased
    - Decisions based on gut instinct without any data to back them up can cause mistakes.
    
    **Data + business knowledge = mystery solved**
    
    The key is figuring out the exact mix for each particular project. A lot of times, it will depend on the goals of your analysis. That is why analysts often ask, “How do I define success for this project?”
    
    In addition, try asking yourself these questions about a project to help find the perfect balance:
    
    - What kind of results are needed?
    - Who will be informed?
    - Am I answering the question being asked?
    - How quickly does a decision need to be made?
    
    ## ORIGINS OF THE DATA ANALYSIS PROCESS
    
    Data analysis is rooted in statistics, which has a pretty long history itself. Archaeologists mark the start of statistics in ancient Egypt with the building of the pyramids. The ancient Egyptians were masters of organizing data. They documented their calculations and theories on papyri (paper-like materials), which are now viewed as the earliest examples of spreadsheets and checklists. Today’s data analysts owe a lot to those brilliant scribes, who helped create a more technical and efficient process.
    
    ### Data analysis life cycle
    
    The process of going from data to decision. There might not be one single architecture that’s uniformly followed by every data analysis expert, but there are some shared fundamentals in every data analysis process.
    
    The process presented as part of the Google Data Analytics Certificate is one that will be valuable to you as you keep moving forward in your career:
    
    1. **Ask**: Business Challenge/Objective/Question
    2. **Prepare**: Data generation, collection, storage, and data management
    3. **Process**: Data cleaning/data integrity
    4. **Analyze**: Data exploration, visualization, and analysis
    5. **Share**: Communicating and interpreting results
    6. **Act**: Putting your insights to work to solve the problem
    
    ![Untitled](Google_Data_Analytics_images/Untitled.png)
    
    ### EMC's data analysis life cycle
    
    EMC Corporation's data analytics life cycle is cyclical with six steps:
    
    1. Discovery
    2. Pre-processing data
    3. Model planning
    4. Model building
    5. Communicate results
    6. Operationalize
    
    EMC Corporation is now Dell EMC. This model, created by David Dietrich, reflects the cyclical nature of real-world projects.
    
    ### SAS's iterative life cycle
    
    An iterative life cycle was created by a company called **SAS**, a leading data analytics solutions provider. It can be used to produce repeatable, reliable, and predictive results:
    
    1. Ask
    2. Prepare
    3. Explore
    4. Model
    5. Implement
    6. Act
    7. Evaluate
    
    it includes a step after the act phase designed to help analysts evaluate their solutions and potentially return to the ask phase again.
    
    ### Project-based data analytics life cycle
    
    1. Identifying the problem
    2. Designing data requirements
    3. Pre-processing data
    4. Performing data analysis
    5. Visualizing data
    
    This data analytics project life cycle was developed by Vignesh Prajapati. It doesn’t include the sixth phase, or what we have been referring to as the Act phase.
    
    ### Big data analytics life cycle
    
    Authors Thomas Erl, Wajid Khattak, and Paul Buhler proposed a big data analytics life cycle in their book, **Big Data Fundamentals: Concepts, Drivers & Techniques**. Their life cycle suggests phases divided into nine steps:
    
    1. Business case evaluation
    2. Data identification
    3. Data acquisition and filtering
    4. Data extraction
    5. Data validation and cleaning
    6. Data aggregation and representation
    7. Data analysis
    8. Data visualization
    9. Utilization of analysis results
    
    It emphasizes the individual tasks required for gathering, preparing, and cleaning data before the analysis phase.
    
    ## ANALYTICAL SKILLS
    
    Analytical skills are qualities and characteristics associated with solving problems using facts. We will focus on 5 essential points or aspects:
    
    1. Curiosity. Is all about wanting to learn something. Curious people usually seek out new challenges and experiences. This leads to knowledge.
    2. Understanding context. Context is the condition in which something exists or happens. This can be a structure or an environment. Listening and trying to understand the full picture is critical.
    3. having a technical mindset. A technical mindset involves the ability to break things down into smaller steps or pieces and work with them in an orderly and logical way. When you take something that seems like a single task, like paying your bills, and break it into smaller steps with an orderly process, that's using a technical mindset.
    4. Data design. Data design is how you organize information. As a data analyst, design typically has to do with an actual database.
    5. Data strategy. Data strategy is the management of the people, processes, and tools used in data analysis.
    
    ## THINKING ANALYTICALLY
    
    Analytical thinking involves identifying and defining a problem and then solving it by using data in an organized, step-by-step manner.
    
    ### The five key aspects of analytical thinking:
    
    1. Visualization: is the graphical representation of information. For example, graphs, maps or design elements. Visualization is important because visuals can help data analysts understand and explain information more effectively.
    2. Strategy: Strategizing helps data analysts see what they want to achieve with the data and how they can get there. Strategy also helps improve the quality and usefulness of the data we collect. By strategizing, we know all our data is valuable and can help us accomplish our goals.
    3. Problem-orientation: Data analysts use a problem- oriented approach in order to identify, describe, and solve problems. It's all about keeping the problem top of mind throughout the entire project. Data analysts also ask a lot of questions. This helps improve communication and saves time while working on a solution.
    4. Correlation: being able to identify a correlation between two or more pieces of data. A correlation is like a relationship. You can find all kinds of correlations in data. Correlation does not equal causation. In other words, just because two pieces of data are both trending in the same direction, that doesn't necessarily mean they are all related.
    5. big-picture and detail-oriented thinking: This means being able to see the big picture as well as the details. Big-picture thinking is like looking at a complete puzzle. You can enjoy the whole picture without getting stuck on every tiny piece that went into making it. If you only focus on individual pieces, you wouldn't be able to see past that, which is why big-picture thinking is so important. It helps you zoom out and see possibilities and opportunities. This leads to exciting new ideas or innovations. On the flip side, detail-oriented thinking is all about figuring out all of the aspects that will help you execute a plan. In other words, the pieces that make up your puzzle.
    
    The more ways you can think, the easier it is to think outside the box and come up with fresh ideas. But why is it important to think in different ways? in data analysis, solutions are almost never right in front of you. You need to think **critically** to find out the right questions to ask. But you also need to think **creatively** to get new and unexpected answers.
    
    Some of the questions data analysts ask when they're on the hunt for a solution are:
    
    - What is the root cause of a problem?. A root cause is the reason why a problem occurs. If we can identify and get rid of a root cause, we can prevent that problem from happening again. A simple way to wrap your head around root causes is with the process called the Five Whys.
    - where are the gaps in our process?. For this, many people will use something called gap analysis. Gap analysis lets you examine and evaluate how a process works currently in order to get where you want to be in the future. The general approach to gap analysis is understanding where you are now compared to where you want to be. Then you can identify the gaps that exist between the current and future state and determine how to bridge them.
    - what did we not consider before?. This is a great way to think about what information or procedure might be missing from a process, so you can identify ways to make better decisions and strategies moving forward.
    
    ### Five Whys:
    
    you ask "why" five times to reveal the root cause. The fifth and final answer should give you some useful and sometimes surprising insights.
    
    The way data analysts think and ask questions plays a big part in how businesses make decisions. That's why analytical thinking and understanding how to ask the right questions can have such a huge impact on the overall success of a business.
    
    ## USING DATA TO DRIVE SUCCESSFUL OUTCOMES
    
    ### Data-driven decision-making:
    
    It gives you greater confidence about your choice and your abilities to address business challenges. It helps you become more proactive when an opportunity presents itself, and it saves you time and effort when working towards a goal.
    
    First, think about curiosity and context. The more you learn about the power of data, the more curious you're likely to become. You'll start to see patterns and relationships in everyday life
    
    The analysts take their thinking a step further by using context to make predictions, research answers, and eventually draw conclusions about what they've discovered. This natural process is a great first step in becoming more data-driven. Having a technical mindset comes next.
    
    Data analysts have gut feeling, but they've trained themselves to build on those feelings and use a more technical approach to explore them. They do this by always seeking out the facts, putting them to work through analysis, and using the insights they gain to make informed decisions.
    
    Next, we come to data design, which has a strong connection to data-driven decision-making. Designing your data so that it is organized in a logical way makes it easy for data analysts to access, understand, and make the most of available information. And it's important to keep in mind that data design doesn't just apply to databases. This kind of thinking can work with all sorts of real-life situations too.
    
    The basic idea is: If you make decisions that are informed by data, you are more likely to make more informed and effective decisions.
    
    The final ability is data strategy, which incorporates the people, processes, and tools used to solve a problem. This is a big one to remember because data strategy gives you a high-level view of the path you need to take to achieve your goals.
    
    data strategy gives you a high-level view of the path you need to take to achieve your goals. Also, data-driven decision-making isn't a one-person job. It's much more likely to be successful if everyone is on board and on the same page, so it's important to make sure specific procedures are in place and that your technology being used is aligned with your data-driven strategy.
    
    ## DATA LIFE CYCLE
    
    The data life cycle starts with the right data analysis tools. These include spreadsheets, databases, query languages, and visualization software. 
    
    The life cycle of data is plan, capture, manage, analyze, archive and destroy.
    
    1. **Planning:** This actually happens well before starting an analysis project. During planning, a business decides what kind of data it needs, how it will be managed throughout its life cycle, who will be responsible for it, and the optimal outcomes.
    2. **Capture:** This is where data is collected from a variety of different sources and brought into the organization. With so much data being created everyday, the ways to collect it are truly endless. One common method is getting data from outside resources. Another way to get data is from a company's own documents and files, which are usually stored inside a database.
    3. **Manage:** how we care for our data, how and where it's stored, the tools used to keep it safe and secure, and the actions taken to make sure that it's maintained properly. This phase is very important to data cleansing.
    4. **Analyze:** In this phase, the data is used to solve problems, make great decisions, and support business goals.
    5. **Archive:** Keep relevant data stored for long-term and future reference. Archiving means storing data in a place where it's still available, but may not be used again. It makes way more sense to archive it than to keep it around.
    6. **Destroy:** Remove data from storage and delete any shared copies of the data. To destroy it, the company would use a secure data erasure software. If there were any paper files, they would be shredded too. This is important for protecting a company's private information, as well as private data about its customers.
    
    ### Different ways to see the data life cycle:
    
    Although data life cycles vary, one data management principle is universal. Govern how data is handled so that it is accurate, secure, and available to meet your organization's needs.
    
    - U.S. Fish and Wildlife Service:
        1. Plan
        2. Acquire
        3. Maintain
        4. Access
        5. Evaluate
        6. Archive
    - The U.S. Geological Survey (USGS):
        1. Plan
        2. Acquire
        3. Process
        4. Analyze
        5. Preserve
        6. Publish/Share
    - Financial institutions:
        1. Capture
        2. Qualify
        3. Transform
        4. Utilize
        5. Report
        6. Archive
        7. Purge
    - Harvard Business School (HBS):
        1. Generation
        2. Collection
        3. Processing
        4. Storage
        5. Management
        6. Analysis
        7. Visualization
        8. Interpretation
    
    ## PHASES OF DATA ANALYSIS
    
    Data analysis isn't a life cycle. It's the process of analyzing data.
    
    1. **Ask:** In this phase, we do two things. We define the problem to be solved and we make sure that we fully understand stakeholder expectations. **Stakeholders** hold a stake in the project. They are people who have invested time and resources into a project and are interested in the outcome. First, defining a problem means you look at the current state and identify how it's different from the ideal state. Usually there's an obstacle we need to get rid of or something wrong that needs to be fixed. Another important part of the ask phase is understanding stakeholder expectations. The first step here is to determine who the stakeholders are. That may include your manager, an executive sponsor, or your sales partners. There can be lots of stakeholders. But what they all have in common is that they help make decisions, influence actions and strategies, and have specific goals they want to meet. This part of the ask phase helps you keep focused on the problem itself, not just its symptoms.
        
        You want to ask all of the right questions at the beginning of the engagement so that you better understand what your leaders and stakeholders need from this analysis.
        
        - what is the problem that we're trying to solve?
        - What is the purpose of this analysis?
        - What are we hoping to learn from it?
    2. **Prepare:** This is where data analysts collect and store data they'll use for the upcoming analysis process. Any decisions made from your analysis should always be based on facts and be fair and impartial.
        
        We need to be thinking about the type of data we need in order to answer the questions that we've set out to answer based on what we learned when we asked the right questions. We also need to be thinking about how we're going to collect that data or if we need to collect that data.
        
    3. **Process:** Here, data analysts find and eliminate any errors and inaccuracies that can get in the way of results. This usually means cleaning data, transforming it into a more useful format, combining two or more datasets to make information more complete and removing outliers, which are any data points that could skew the information. This phase is all about getting the details right. So you'll also fix typos, inconsistencies, or missing and inaccurate data.
        
        This is where you get a chance to understand its structure, its quirks, its nuances, and you really get a chance to understand deeply what type of data you're going to be working with and understanding what potential that data has to answer all of your questions. we will be running through all of our quality assurance checks, for example:
        
        - do we have all of the data that we anticipated we would have?
        - Are we missing data at random or is it missing in a systematic way such that maybe something went wrong with our data collection effort?
        - If needed, did we code all of our data the right way?
        - Are there any outliers that we need to treat differently?
        
        This is the part where we spend a lot of time really digging deeply into the structure and nuance of the data to make sure that you're able to analyze it appropriately and responsibly.
        
    4. **Analyze:** Analyzing the data you've collected involves using tools to transform and organize that information so that you can draw useful conclusions, make predictions, and drive informed decision-making.
        
        This is the point where we have to take a step back and let the data speak for itself. As data analysts, we are storytellers, but we also have to keep in mind that it is not our story to tell. That story belongs to the data, and it is our job as analysts to amplify and tell that story in as unbiased and objective a way as possible.
        
    5. **Share:** Here you'll learn how data analysts interpret results and share them with others to help stakeholders make effective data-driven decisions. In the share phase, visualization is a data analyst's best friend.
    6. **Act:** This is the exciting moment when the business takes all of the insights you, the data analyst, have provided and puts them to work in order to solve the original business problem and will be acting on what you've learned throughout this program.
        
        This is where we use all of those data-driven insights to decide what types of interventions we want to introduce, not only at the organizational level, but also at the team level as well.
        
    
    ![Untitled](Google_Data_Analytics_images/Untitled%201.png)
    
    ## THE DATA ANALYST TOOLS
    
    ### Spreadsheets:
    
    two popular options are Microsoft Excel and Google Sheets. A spreadsheet is a digital worksheet. It stores, organizes, and sorts data. When you put your data into a spreadsheet, you can see patterns, group information and easily find the information you need. Spreadsheets also have some really useful features called formulas and functions.
    
    Spreadsheets structure data in a meaningful way by letting you
    
    - Collect, store, organize, and sort information
    - Identify patterns and piece the data together in a way that works for each specific data project
    - Create excellent data visualizations, like graphs and charts.
    
    A formula is a set of instructions that performs a specific calculation using the data in a spreadsheet.
    
    A function is a preset command that automatically performs a specific process or task using the data in a spreadsheet. Functions can help make you more efficient.
    
    ### Query language
    
    A query language is a computer programming language that allows you to retrieve and manipulate data from a database.
    
    SQL is a language that lets data analysts communicate with a database. SQL is the most widely used structured query language for a couple of reasons:
    
    - Allow analysts to isolate specific information from a database(s)
    - Make it easier for you to learn and understand the requests made to databases
    - Allow analysts to select, create, add, or download data from a database for analysis
    
    With SQL, data analysts can access the data they need by making a query. A query means question, but it is more like a request.
    
    ### Data visualization
    
    Is the graphical representation of information. Some examples include graphs, maps, and tables. They help data analysts communicate their insights to others, in an effective and compelling way. This makes it easier for stakeholders to draw conclusions, make decisions, and come up with strategies. Also, these tools turn complex numbers into a story that people can understand. Some popular visualization tools are Tableau and Looker. Data analysts like using Tableau because it helps them create visuals that are very easy to understand. 
    
    A career as a data analyst also involves using programming languages, like R and Python, which are used a lot for statistical analysis, visualization, and other data analysis.
    
    Depending on which phase of the data analysis process you’re in, you will need to use different tools.
    
    ![Untitled](Google_Data_Analytics_images/Untitled%202.png)
    
    You don’t have to choose one or the other because each serves its own purpose. Generally, data analysts work with a combination of the two, as both tools are very useful in data analytics.
    
    ## SPREADSHEET AND SQL
    
    Spreadsheets are a big part of data analytics.
    
    ### Three main features of a spreadsheet:
    
    - Cells: when you talk about a specific cell, you name it by combining the column letter and the row number where the cell is located.
    - Rows: the rows are organized horizontally and are ordered by number. A row is also called an observation. An observation includes all of the attributes for something contained in a row of a data table.
    - Columns: columns are organized vertically in a spreadsheet and are ordered by letter.
    
    ![Untitled](Google_Data_Analytics_images/Untitled%203.png)
    
    ### Attribute
    
    Adding labels to the top of the columns will make it easier to reference and find data later on when you're doing analysis. These column labels are usually called attributes. An attribute is a characteristic or quality of data used to label a column in a table. More commonly, attributes are referred to as column names, column labels, headers, or the header row.
    
    ### Formula
    
    A formula is a set of instructions that performs a specific action using the data in a spreadsheet. To do this, the formula uses cell references for the values it's calculating. All formulas begin with =
    
    ### SQL
    
    Structured Query Language (or SQL, often pronounced “sequel”) enables data analysts to talk to their databases. Remember, SQL can do lots of the same things with data that spreadsheets can do. You can use it to store, organize and analyze your data, among other things. But like any good sequel, it's on a larger scale, bigger, more action-packed. SQL needs a database that will understand its language.
    
    **Query:** A query is a request for data or information from a database.
    
    ### Basic Structure of a SQL query:
    
    ![Untitled](Google_Data_Analytics_images/Untitled%204.png)
    
    ### Syntax
    
    Every programming language, including SQL, follows a unique set of guidelines known as syntax. Syntax is the predetermined structure of a language that includes all required words, symbols, and punctuation, as well as their proper placement. As soon as you enter your search criteria using the correct syntax, the query starts working to pull the data you’ve requested from the target database.
    
    The syntax of every SQL query is the same:
    
    - Use **SELECT** to choose the columns you want to return. You can separate the columns with (,).
    - Use **FROM** to choose the tables where the columns you want are located.
    - Use **WHERE** to filter for certain information. You connect the conditions with (AND), (OR) or (NOT).
    
    ![Untitled](Google_Data_Analytics_images/Untitled%205.png)
    
    The most important thing to remember is how to use SELECT, FROM, and WHERE in a query.
    
    ### SQL good practices - TIPS:
    
    - using capitalization and indentation can help you read the information more easily.
    - The semicolon is a statement terminator and is part of the American National Standards Institute (ANSI) SQL-92 standard, which is a recommended common syntax for adoption by all SQL databases. However, not all SQL databases have adopted or enforce the semicolon, so it’s possible you may come across some SQL statements that aren’t terminated with a semicolon. If a statement works without a semicolon, it’s fine.
    - the LIKE clause is very powerful because it allows you to tell the database to look for a certain pattern
    - The percent sign (%) is used as a wildcard to match one or more characters.
    - with SELECT * , you would be selecting all of the columns in the table, but be careful, selecting too much data can cause a query to run slowly.
    - You can place comments alongside your SQL to help you remember what the name represents. Comments are text placed between certain characters, /* and */, or after two dashes (--)
        - Comments can also be added outside of a statement as well as within a statement. You can use this flexibility to provide an overall description of what you are going to do, step-by-step notes about how you achieve it, and why you set different parameters/conditions.
        
        ![Untitled](Google_Data_Analytics_images/Untitled%206.png)
        
    - We can assign alias to the columns to reduce the ammount of comments. An alias doesn’t change the actual name of a column or table in the database.
    - <> means "does not equal”
    - Pulling the data, analyzing it, and implementing a solution might ultimately help improve employee satisfaction and loyalty. That makes SQL a pretty powerful tool.
    
    ## DATA VISUALIZATION
    
    Because of the importance of data visualization, most data analytics tools (such as spreadsheets and databases) have a built-in visualization component while others (such as Tableau) specialize in visualization as their primary value-add.
    
    Data visualizations are pictures, they are a wonderful way to take very basic ideas around data and data points and make them come alive.
    
    ### Steps to plan a data visualization:
    
    1. Explore the data for patterns
    2. Plan your visuals
    3. Create your visuals
    
    Data visualization toolkit:
    
    - Spreadsheets: Microsoft Excel or Google Sheets
    - Visualization software: Tableau. Tableau is a popular data visualization tool that lets you pull data from nearly any system and turn it into compelling visuals or actionable insights. The platform offers built-in visual best practices, which makes analyzing and sharing data fast, easy, and (most importantly) useful.
    - Programming language: R with RStudio. As with Tableau, you can create dashboard-style data visualizations using RStudio.
    
    ## FAIRNESS
    
    Fairness means ensuring that your analysis doesn't create or reinforce bias. In other words, as a data analyst, you want to help create systems that are fair and inclusive to everyone.
    
    Sometimes conclusions based on data can be true and unfair.
    
    Most common factors: industry, tools, location, travel, and culture.
    
    The data analyst role is one of many job titles that contain the word “analyst.”
    
    - Business analyst — analyzes data to help businesses improve processes, products, or services
    - Data analytics consultant — analyzes the systems and models for using data
    - Data engineer — prepares and integrates data from different sources for analytical use
    - Data scientist — uses expert skills in technology and social science to find trends through data analysis
    - Data specialist — organizes or converts data for use in databases or software systems
    - Operations analyst — analyzes data to assess the performance of business operations and workflows
    
    ![Untitled](Google_Data_Analytics_images/Untitled%207.png)
    
    Other industry-specific specialist positions that you might come across in your data analyst job search include:
    
    - Marketing analyst — analyzes market conditions to assess the potential sales of products and services
    - HR/payroll analyst — analyzes payroll data for inefficiencies and errors
    - Financial analyst — analyzes financial status by collecting, monitoring, and reviewing data
    - Risk analyst — analyzes financial documents, economic conditions, and client data to help companies determine the level of risk involved in making a particular business decision
    - Healthcare analyst — analyzes medical data to improve the business aspect of hospitals and medical facilities
    
    ### Interview best practices:
    
    - Think about a time where you've used data to solve a problem, whether it's in your professional or personal projects
    - increase your professional network.
    - It's really important to have your LinkedIn updated along with websites like GitHub, where you can showcase a lot of the data analysts projects you've done.
    - Prepare questions for the interviewer.
    - Sometimes there is no right answer, and a lot of times interviewers are looking to see your thought process and the way you get to your solution.
    - Look for the recruiter. Look for the hiring manager online. See if you can reach out to them and set up a coffee chat or send them your resume directly.
    
- 2 - **ASK** QUESTIONS TO MAKE DATA-DRIVEN DECISIONS
    
    This is related to the ASK part of the analysis phase
    
    **Structured thinking:** is the process of recognizing the current problem or situation, organizing available information, revealing gaps and opportunities, and identifying the options. In this process, you address a vague, complex problem by breaking it down into smaller steps, and then those steps lead you to a logical solution.
    
    key things to think about when choosing an advertising method:
    
    1. your target audience.
    2. your budget
    
    Take action with data:
    
    1. Zoom out and look at the whole situation in context. That way we can be sure that she was focusing on the real problem and not just its symptoms.
    2. Collaborating with stakeholders and understanding their needs. Takeholders included the owner, the vice president of communications, and the director of marketing and finance.
    3. Prepare phase, where we collect the data for the upcoming analysis process. Here we have to understand the company’s target audience, collect the data to look for patterns.
    4. Process step. Clean the data to eliminate any errors or inaccuracies that could get in the way of the result. When you clean data, you transform it into a more useful format, create more complete information and remove outliers.
    5. Analyze. 
    6. Share the recommendation so the company could make a data driven decision. summarize the results using clear and compelling visuals of the analysis. This helped her stakeholders understand the solution to the original problem.
    
    ## THE SIX DATA ANALYSIS PHASES
    
    There are six data analysis phases that will help you make seamless decisions: ask, prepare, process, analyze, share, and act. Keep in mind, these are different from the data life cycle, which describes the changes data goes through over its lifetime.
    
    ### Step 1: Ask.
    
    It’s impossible to solve a problem if you don’t know what it is. These are some things to consider:
    
    - Define the problem you’re trying to solve
    - Make sure you fully understand the stakeholder’s expectations
    - Focus on the actual problem and avoid any distractions
    - Collaborate with stakeholders and keep an open line of communication
    - Take a step back and see the whole situation in context
        
        **Questions to ask yourself in this step:**
        
        1. What are my stakeholders saying their problems are?
        2. Now that I’ve identified the issues, how can I help the stakeholders resolve their questions?
    
    ### Step 2: Prepare.
    
    You will decide what data you need to collect in order to answer your questions and how to organize it so that it is useful. You might use your business task to decide:
    
    - What metrics to measure
    - Locate data in your database
    - Create security measures to protect that data
        
        **Questions to ask yourself in this step:**
        
        1. What do I need to figure out how to solve this problem?
        2. What research do I need to do?
        
    
    ### Step 3: Process.
    
    Clean data is the best data and you will need to clean up your data to get rid of any possible errors, inaccuracies, or inconsistencies. This might mean:
    
    - Using spreadsheet functions to find incorrectly entered data
    - Using SQL functions to check for extra spaces
    - Removing repeated entries
    - Checking as much as possible for bias in the data
        
        **Questions to ask yourself in this step:**
        
        1. What data errors or inaccuracies might get in my way of getting the best possible answer to the problem I am trying to solve?
        2. How can I clean my data so the information I have is more consistent?
    
    ### Step 4: Analyze
    
    You will want to think analytically about your data. At this stage, you might sort and format your data to make it easier to:
    
    - Perform calculations
    - Combine data from multiple sources
    - Create tables with your results
        
        **Questions to ask yourself in this step:**
        
        1. What story is my data telling me?
        2. How will my data help me solve this problem?
        3. Who needs my company’s product or service? What type of person is most likely to use it?
    
    ### Step 5: Share.
    
    Everyone shares their results differently so be sure to summarize your results with clear and enticing visuals of your analysis using data via tools like graphs or dashboards. This is your chance to show the stakeholders you have solved their problem and how you got there. Sharing will certainly help your team:
    
    - Make better decisions
    - Make more informed decisions
    - Lead to stronger outcomes
    - Successfully communicate your findings
        
        **Questions to ask yourself in this step:**
        
        1. How can I make what I present to the stakeholders engaging and easy to understand?
        2. What would help me understand this if I were the listener?
    
    ### Step 6: Act.
    
    Now it’s time to act on your data. You will take everything you have learned from your data analysis and put it to use. This could mean providing your stakeholders with recommendations based on your findings so they can make data-driven decisions.
    
    **Questions to ask yourself in this step:**
    
    1. How can I use the feedback I received during the share phase (step 5) to actually meet the stakeholder’s needs and expectations?
    
    These six steps can help you to break the data analysis process into smaller, manageable parts, which is called **structured thinking**. This process involves four basic activities:
    
    1. Recognizing the current problem or situation
    2. Organizing available information
    3. Revealing gaps and opportunities
    4. Identifying your options
    
    When you are starting out in your career as a data analyst, it is normal to feel pulled in a few different directions with your role and expectations. Following processes like the ones outlined here and using structured thinking skills can help get you back on track, fill in any gaps and let you know exactly what you need.
    
    ### SOLVE PROBLEMS WITH DATA
    
    Data analysts work with a variety of problems. There are 6 common problem types:
    
    1. **making predictions:** involves using data to make an informed decision about how things may be in the future
        - Problem: how to determine the best advertising method for anywhere gaming repair's target audience. nobody can see the future but the data helped them make an informed decision about how things would likely work out.
    2. **categorizing things:** assigning information to different groups or clusters based on common features.
        - Problem: How to improve customer satisfaction levels. Analysts might classify customer service calls based on certain keywords or scores. They could identify certain key words or phrases that come up during the phone calls and then assign them to categories such as politeness, satisfaction, dissatisfaction, empathy, and more. Categorizing these key words gives us data that lets the company identify top performing customer service representatives, and those who might need more coaching.
    3. **spotting something unusual:** In this problem type, data analysts identify data that is different from the norm.
        - Analysts who have analyzed aggregated health data can help product developers determine the right algorithms to spot and set off alarms when certain data doesn't trend normally.
    4. **identifying themes:** Identifying themes takes categorization as a step further by grouping information into broader concepts.
        - Problem: How to improve users experience.
        - Themes are most often used to help researchers explore certain aspects of data. In a user study, user beliefs, practices, and needs are examples of themes.
        - The process here is kind of like finding categories for keywords and phrases in customer service conversations.
    5. **discovering connections:** enables data analysts to find similar challenges faced by different entities, and then combine data and insights to address them.
        - Problem: How to reduce wait time.
        - A third-party logistics company working with another company to get shipments delivered to customers on time is a problem requiring analysts to discover connections. By analyzing the wait times at shipping hubs, analysts can determine the appropriate schedule changes to increase the number of on-time deliveries.
    6. **Finding patterns:** Data analysts use data to find patterns by using historical data to understand what happened in the past and is therefore likely to happen again.
        - Problem: how to stop machines from breaking down.
        - Minimizing downtime caused by machine failure is an example of a problem requiring analysts to find patterns in data. For example, by analyzing maintenance data, they might discover that most failures happen if regular maintenance is delayed by more than a 15-day window.
    
    ## CRAFT EFFECTIVE QUESTIONS
    
    The more questions you ask, the more you'll learn about your data and the more powerful your insights will be at the end of the day. Knowing the difference between effective and ineffective questions is essential for your future career as a data analyst.
    
    ### Things to avoid when asking questions:
    
    - Leading questions: it's leading you to answer in a certain way
    - Closed-ended questions: questions that ask for a one-word or brief response only. These kinds of questions rarely lead to valuable insights. It's too vague and lacks context.
    - Vague questions: questions that aren’t specific or don’t provide context.
    
    ### Effective questions follow the SMART methodology.
    
    - S: Specific. Specific questions are simple, significant and focused on a single topic or a few closely related ideas. This helps us collect information that's relevant to what we're investigating. If a question is too general, try to narrow it down by focusing on just one element.
    - M: Measurable. Measurable questions can be quantified and assessed.
    - A: Action-oriented. Action-oriented questions encourage change. This brings you answers you can act on
    - R: Relevant. Relevant questions matter, are important and have significance to the problem you're trying to solve.
    - T: Time-bound. Time-bound questions specify the time to be studied. This limits the range of possibilities and enables the data analyst to focus on relevant data
    
    ![Untitled](Google_Data_Analytics_images/Untitled%208.png)
    
    Questions should be open-ended. This is the best way to get responses that will help you accurately qualify or disqualify potential solutions to your specific problem.
    
    ## UNDERSTAND THE POWER OF DATA - data-inspired decision-making - Quantitive and qualitative data
    
    Data-inspired decision-making explores different data sources to find out what they have in common.
    
    An algorithm is a process or set of rules to be followed for a specific task.
    
    Responsibly gathering data is only part of the process. We also have to turn data into knowledge that helps us make better solutions.
    
    ![Untitled](Google_Data_Analytics_images/Untitled%209.png)
    
    The goal of all data analysts is to use data to draw accurate conclusions and make good recommendations. That all starts with having complete, correct, and relevant data.
    
    - **When data is interpreted incorrectly, it can lead to huge losses.**
    - **When data is used strategically, businesses can transform and grow their revenue.**
    
    As a data analyst, your own skills and knowledge will be the most important part of any analysis project. It is important for you to keep a data-driven mindset, ask lots of questions, experiment with many different possibilities, and use both logic and creativity along the way. You will then be prepared to interpret your data with the highest levels of care and accuracy.
    
    Analyzing user preferences to customize movie recommendations and analyzing product purchases to create better promotions are examples of using data to achieve business results.
    
    There are 2 kinds of data:
    
    ### Quantitative data
    
    Quantitative data is all about the specific and objective measures of numerical facts. This can often be the what, how many, and how often about a problem. In other words, things you can measure.
    
    Quantitative data is a specific and objective measure, such as a number, quantity or range.
    
    With quantitative data, we can see numbers visualized as charts or graphs.
    
    Tools:
    
    - Structured interviews
    - Surveys
    - Polls
    
    ### Qualitative data
    
    qualitative data describes subjective or explanatory measures of qualities and characteristics or things that can't be measured with numerical data. Qualitative data is great for helping us answer why questions.
    
    Qualitative data can then give us a more high-level understanding of why the numbers are the way they are.
    
    **Tools:**
    
    - Focus groups
    - Social media text analysis
    - In-person interviews
    
    It's your job as a data detective to know which questions to ask to find the right solution. Then you can start thinking about cool and creative ways to help stakeholders better understand the data.
    
    Usually, qualitative data can help analysts better understand their quantitative data by providing a reason or more thorough explanation. In other words, quantitative data generally gives you the what, and qualitative data generally gives you the why.
    
    ## FOLLOW THE EVIDENCE - metrics, reports, dashboards
    
    here are all kinds of tools out there to help you visualize and share your data analysis with stakeholders. Here, we'll talk about two data presentation tools, reports and dashboards. Reports and dashboards are both useful for data visualization. But there are pros and cons for each of them.
    
    ### Reports
    
    A report is a static collection of data given to stakeholders periodically.
    
    | pros | cons |
    | --- | --- |
    | Reports are great for giving snapshots of high level historical data for an organization. | Reports need regular maintenance. |
    | They can be designed and sent out periodically, often on a weekly or monthly basis, as organized and easy to reference information. | They are not very visually appealing. |
    | They're quick to design and easy to use as long as you continually maintain them. | Because they aren't automatic or dynamic, reports don't show live, evolving data. |
    | Because reports use static data or data that doesn't change once it's been recorded, they reflect data that's already been cleaned and sorted. |  |
    
    ### Dashboards
    
    A dashboard monitors live, incoming data.
    
    Data analysts use dashboards to track, analyze, and visualize data in order to answer questions and solve problems.
    
    | Pros | Cons |
    | --- | --- |
    | they give your team more access to information being recorded, you can interact through data by playing with filters, and because they're dynamic, they have long-term value. | they take a lot of time to design |
    | If stakeholders need to continually access information, a dashboard can be more efficient than having to pull reports over and over | can actually be less efficient than reports, if they're not used very often. (can be confusing). |
    | Nice to look at | If the base table breaks at any point, they need a lot of maintenance to get back up and running again. |
    | A dashboard organizes information from multiple datasets into one central location, offering huge time-savings. | Dashboards can sometimes overwhelm people with information too. |
    
    ![Untitled](Google_Data_Analytics_images/Untitled%2010.png)
    
    ### How to create a Dashboard:
    
    1. Identify the stakeholders who need to see the data and how they will use it: To get started with this, you need to ask effective questions. Check out the PDF.
    2. Design the dashboard (what should be displayed): Use these tips to help make your dashboard design clear, easy to follow, and simple:
        - Use a clear header to label the information
        - Add short text descriptions to each visualization
        - Show the most important information at the top
    3. Create mock-ups if desired: This is optional, but a lot of data analysts like to sketch out their dashboards before creating them.
    4. Select the visualizations you will use on the dashboard: it all depends on what data story you are telling. If you need to show a change of values over time, line charts or bar graphs might be the best choice. If your goal is to show how each part contributes to the whole amount being reported, a pie or donut chart is probably a better choice.
    5. Create filters as needed.
    
    ### Types of dashboards:
    
    The three most common categories are:
    
    - **Strategic**: focuses on long term goals and strategies at the highest level of metrics. A wide range of businesses use strategic dashboards when evaluating and aligning their strategic goals. These dashboards provide information over the longest time frame—from a single financial quarter to years.
    - **Operational:** short-term performance tracking and intermediate goals. It is the most common type of dashboard. Because these dashboards contain information on a time scale of days, weeks, or months, they can provide performance insight almost in real-time. This allows businesses to track and maintain their immediate operational processes in light of their strategic goals. The operational dashboard below focuses on customer service.
    
    ![Untitled](Google_Data_Analytics_images/Untitled%2011.png)
    
    - **Analytical:** consists of the datasets and the mathematics used in these sets. These dashboards contain the details involved in the usage, analysis, and predictions made by data scientists. Certainly the most technical category, analytic dashboards are usually created and maintained by data science teams and rarely shared with upper management as they can be very difficult to understand.
    
    ![Untitled](Google_Data_Analytics_images/Untitled%2012.png)
    
    ### Data VS Metrics
    
    Data starts as a collection of raw facts, until we organize them into individual metrics that represent a single type of data.
    
    | Data | Metrics |
    | --- | --- |
    | Data contains a lot of raw details about the problem we're exploring. | A metric is a single, quantifiable type of data that can be used for measurement. |
    |  | Metrics can also be combined into formulas that you can plug your numerical data into. Metrics usually involve simple math. |
    
    We see metrics used in marketing too. For example, metrics can be used to help calculate customer retention rates, or a company's ability to keep its customers over time. Customer retention rates can help the company compare the number of customers at the beginning and the end of a period to see their retention rates.
    
    By using metrics to focus on individual aspects of your data, you can start to see the story your data is telling. Metric goals and formulas are great ways to measure and understand data.
    
    ### ROI:
    
    ROI, or Return on Investment is essentially a formula designed using metrics that let a business know how well an investment is doing.
    
    The ROI is made up of two metrics, the net profit over a period of time and the cost of investment. By comparing these two metrics, profit and cost of investment, the company can analyze the data they have to see how well their investment is doing.
    
    ## CONNECTING THE DATA DOTS - Mathematical thinking, Big and small data
    
    Mathematical thinking is a powerful skill you can use to help you solve problems and see new solutions.
    
    It means looking at a problem and logically breaking it down step-by-step, so you can see the relationship of patterns in your data, and use that to analyze your problem. This kind of thinking can also help you figure out the best tools for analysis because it lets us see the different aspects of a problem and choose the best logical approach.
    
    There are a lot of factors to consider when choosing the most helpful tool for your analysis. One way you could decide which tool to use is by the size of your dataset. When working with data, you'll find that there's big and small data.
    
    ### Small data
    
    These kinds of data tend to be made up of datasets concerned with specific metrics over a short, well defined period of time. Like how much water you drink in a day. Small data can be useful for making day-to-day decisions, like deciding to drink more water. But it doesn't have a huge impact on bigger frameworks like business operations.
    
    Small data is typically stored in a Spreadsheet. Big data is typically stored in a Database.
    
    ### Big Data
    
    Big data on the other hand has larger, less specific datasets covering a longer period of time. They usually have to be broken down to be analyzed. Big data is useful for looking at large- scale questions and problems, and they help companies make big decisions.
    
    | Challenges | Benefits |
    | --- | --- |
    | A lot of organizations deal with data overload and way too much unimportant or irrelevant information. | When large amounts of data can be stored and analyzed, it can help companies identify more efficient ways of doing business and save a lot of time and money. |
    | Important data can be hidden deep down with all of the non-important data, which makes it harder to find and use. This can lead to slower and more inefficient decision-making time frames. | Big data helps organizations spot the trends of customer buying patterns and satisfaction levels, which can help them create new products and solutions that will make customers happy. |
    | The data you need isn’t always easily accessible. | By analyzing big data, businesses get a much better understanding of current market conditions, which can help them stay ahead of the competition. |
    | Current technology tools and solutions still struggle to provide measurable and reportable data. This can lead to unfair algorithmic bias. | Big data helps companies keep track of their online presence—especially feedback, both good and bad, from customers. This gives them the information they need to improve and protect their brand. |
    | There are gaps in many big data business solutions. |  |
    
    ### The three (or four) V words for big data:
    
    When thinking about the benefits and challenges of big data, it helps to think about the three Vs:
    
    - Volume: Describes the amount of data.
    - Variety: describes the different kinds of data.
    - Velocity: describes how fast the data can be processed.
    
    Some data analysts also consider a fourth V:
    
    - Veracity: refers to the quality and reliability of the data.
    
    ![Untitled](Google_Data_Analytics_images/Untitled%2013.png)
    
    ## WORKING WITH SPREADSHEETS
    
    Spreadsheets are a powerful and versatile tool, which is why they're a big part of pretty much everything we do as data analysts.
    
    Spreadsheets can do both basic and complex calculations automatically. Not only does this help you work more efficiently, but it also lets you see the results and understand how you got them.
    
    ### Spreadsheet tasks:
    
    - Organize your data with the task you've been given.
        - Pivot table
            - Sort and filter. The first steps a data analyst takes when working with data in a spreadsheet are to sort and filter the data.
    - Perform some calculations to learn more about it.
        - Formulas
        - Functions
    
    ### Spreadsheet and the data life cycle:
    
    - Plan: This can mean formatting your cells, the headings you choose to highlight, the color scheme, and the way you order your data points. When you take the time to set these standards, you will improve communication, ensure consistency, and help people be more efficient with their time.
    - Capture: by connecting spreadsheets to other data sources, such as an online survey application or a database. This data will automatically be updated in the spreadsheet. That way, the information is always as current and accurate as possible.
    - Manage: This can involve storing, organizing, filtering, and updating information. Spreadsheets also let you decide who can access the data, how the information is shared, and how to keep your data safe and secure.
    - Analyze: Some of the most common spreadsheet analysis tools include formulas to aggregate data or create reports, and pivot tables for clear, easy-to-understand visuals.
    - Archive: This is especially useful if you want to store historical data before it gets updated.
    - Destroy: Keep in mind, lots of businesses are required to follow certain rules or have measures in place to make sure data is destroyed properly.
    
    ### Definitions on a Spreadsheet:
    
    - Attribute: is a characteristic or quality of data used to label a column in a table.
    - Comment on your spreadsheet: Google Sheets lets you and your collaborators add comments to your sheet and reply to those comments. As a data analyst, this is a great way to share feedback with your teammates. To direct your comment to a specific person, enter an at sign (@) followed by their email address. You can add as many people as you want.
    
    ### Basic Step-by-step process:
    
    1. Create a tittle: Make your title short, clear, and have it state exactly what the data in the spreadsheet is about.
    2. Move it: Creating a folder on your computer specifically for spreadsheets and related files can also make it easier to find them.
    - There's a few different ways data analysts get data they work with. Depending on the job, you might use data from an open source, you might be given data to work with or you might be asked to find your own data
    1. Make the columns wider
    2. Organize the data attributes. We can stand it out from the rest of the rows by selecting it and filling it with color. We'll also make the labels bold.
    3. Adding borders to highlight the area around cells in order to see spreadsheet data more clearly.
    
    ## FORMULAS IN SPREADSHEETS
    
    One of the most valuable spreadsheet features is a formula. Formulas are built on operators which are symbols that name the type of operation or calculation to be performed. For example, a plus sign is a common operator. The formulas you use as a data analyst will usually include at least one operator.
    
    These are all examples of expressions:
    
    - 3 minus 1
    - 15 plus 8 divided by 2
    - 846 times 513.
    
    When you create a formula using an expression in a spreadsheet, you start the formula with an equal sign.
    
    In spreadsheets, the symbols used in a formula to perform a specific calculation are called operators. The operators you will use to complete formulas 
    
    - Subtraction – minus sign ( - )
    - Addition – plus sign ( + )
    - Division – forward-slash ( / )
    - Multiplication – asterisk ( * )
    
    If you already have data in your spreadsheet, you can use cell references in your formulas instead. A cell reference is a single cell or range of cells in a worksheet that can be used in a formula. Cell references contain the letter of the column and the number of the row where the data is. 
    
    A range of cells is a collection of two or more cells. A range can include cells from the same row or column, or from different columns and rows collected together. The great thing about using cell references is that they also automatically update when a formula is copied to a new cell. Talk about a time-saver.
    
    A formula has to be air tight. If there's something wrong with one of the cell references, it won't work.
    
    ### Important definitios
    
    - **Auto-Filling:**
    
    The lower-right corner of each cell has a fill handle. It is a small green square in Microsoft Excel and a small blue square in Google Sheets.
    
    ![Untitled](Google_Data_Analytics_images/Untitled%2014.png)
    
    - **Absolute referencing:**
    
    is marked by a dollar sign ($). For example, =$A$10 has absolute referencing for both the column and the row value. To easily switch between absolute and relative referencing in the formula bar, highlight the reference you want to change and press the F4 key
    
    - **Data range:**
    
    When you click into your formula, the colored ranges let you see which cells are being used in your spreadsheet. There are different colors for each unique range in your formula. In a lot of spreadsheet applications, you can press the F2 (or Enter) key to highlight the range of data in the spreadsheet that is referenced in a formula.
    
    - **Combining with functions:**
    
    Combining formulas and functions allows you to do more work with a single command. For example, COUNTIF() is a formula and a function. This means the function runs based on criteria set by the formula. In this case, COUNT is the formula; it will be executed IF the conditions you create are true.
    
    - **Spreadsheets errors and fixes:**
    
    | ERROR | WHY? | HOW TO FIX IT |
    | --- | --- | --- |
    | #DIV/0! | when a formula is trying to divide a value in a cell by zero or by an empty cell. | With IFERROR function |
    | #ERROR | tells us the formula can't be interpreted as it is input. This is also known as a parsing error. | Check delimiters or typing errors in the formula |
    | #N/A | the data in your formula can't be found by the spreadsheet. Generally, this means the data doesn't exist. This error most often occurs when using functions such as VLOOKUP | check typo errors |
    | #NAME? | when a formula's name isn't recognized or understood. | Check typo errors (example, The name of a function is misspelled) |
    | #NUM! | a formula's calculation can't be performed as specified by the data. The data doesn't make sense for that calculation. |  |
    | #VALUE! | can indicate a problem with a formula or referenced cells. It's often not clear right away what the problem is, so this error might take a little more effort to fix. | There could be problems with spaces or text, or with referenced cells in a formula; you may have additional work to find the source of the problem. |
    | #REF! | when cells being referenced in a formula have been deleted, thus making the formula unable to perform the calculation. | Example: A cell used in a formula was in a column that was deleted |
    
    Troubleshooting is a big part of data analysis, so being able to find solutions is a key skill for data analysts.
    
    ### Best practices and helpful tips:
    
    These strategies will help you avoid spreadsheet errors to begin with, making your life in analytics a whole lot less stressful:
    
    1. Filter data to make your spreadsheet less complex and busy.
    2. Use and freeze headers so you know what is in each column, even when scrolling.
    3. When multiplying numbers, use an asterisk (*) not an X.
    4. Start every formula and function with an equal sign (=).
    5. Whenever you use an open parenthesis, make sure there is a closed parenthesis on the other end to match.
    6. Change the font to something easy to read.
    7. Set the border colors to white so that you are working in a blank sheet.
    8. Create a tab with just the raw data, and a separate tab with just the data you need.
    - Conditional formatting can be used to highlight cells a different color based on their contents. This feature can be extremely helpful when you want to locate all errors in a large spreadsheet.
    
    ## FUNCTIONS IN SPREADSHEETS
    
    Formulas are a great way to become more efficient when using spreadsheets, especially when you add shortcuts like copying and pasting, into the mix. Functions give data analysts the ability to do calculations, which can be anything from simple arithmetic to complex equations.
    
    While functions are closely related to formulas, they're not exactly the same.
    
    In the world of spreadsheets a function is a preset command that automatically performs a specific process or task using the data.
    
    A formula is a set of instructions used to perform a calculation using the data in a spreadsheet. A function is a preset command that automatically performs a specific process or task using the data in a spreadsheet.
    
    ### Tips:
    
    - A colon between the cell references shows that you're using a range.
    - Functions can be copied and pasted into other cells in the same column.
    - Spreadsheets have something called a fill handle. It's a little box that appears in the lower right-hand corner when you click on a cell.
    - Different functions perform different calculations, but they work in the same way. Keep in mind, not every calculation you'll come across has its own function to help you.
    - Highlight key data. Add color to the cell in your data set to make it stand out. Colored data ranges help prevent you from getting lost in complex functions.
    - Just like formulas, start all of your functions with an equal sign
    
    ### Popular functions
    
    - Keyboard shortcuts like cut, save, and find are actually functions. Using shortcuts lets you do more with less effort. They can make you more efficient and productive because you are not constantly reaching for the mouse and navigating menus.
    
    ![Untitled](Google_Data_Analytics_images/Untitled%2015.png)
    
    ![Untitled](Google_Data_Analytics_images/Untitled%2016.png)
    
    ### Relative, absolute, and mixed references
    
    Relative references (cells referenced without a dollar sign, like A2) will change when you copy and paste the function into a different cell. With relative references, the location of the cell that contains the function determines the cells used by the function.
    
    Absolute references (cells fully referenced with a dollar sign, like $A$2) will not change when you copy and paste the function into a different cell. With absolute references, the cells referenced always remain the same.
    
    Mixed references (cells partially referenced with a dollar sign, like $A2 or A$2) will change when you copy and paste the function into a different cell. With mixed references, the location of the cell that contains the function determines the cells used by the function, but only the row or column is relative (not both).
    
    In spreadsheets, you can press the F4 key to toggle between relative, absolute, and mixed references in a function. Click the cell containing the function, highlight the referenced cells in the formula bar, and then press F4 to toggle between and select relative, absolute, or mixed referencing.
    
    ## SAVE TIME WITH STRUCTURED THINKING
    
    If you define the problem clearly from the start, it'll be easier to solve, which saves a lot of time, money, and resources. In the data world, we call this first piece the problem domain: the specific area of analysis that encompasses every activity affecting or affected by the problem.
    
    Before we can do anything else, we need to understand the problem domain and all of its parts and relationships so that we can discover the whole story.
    
    Data analysts aren't always given the complete picture at the start of a project. A big part of their job is to develop a structured approach and use critical thinking to find the best solution. That starts with understanding the problem domain. This is where structured thinking comes into play. To successfully solve a problem as a data analyst, you need to train your brain to think structurally.
    
    Structures thinking in other words, it's a way of being super prepared. It's having a clear list of what you are expected to deliver, a timeline for major tasks and activities, and checkpoints so the team knows you're making progress. Structured thinking will help you understand problems at a high level so that you can identify areas that need deeper investigation and understanding.
    
    ### Tips:
    
    - The starting place for structured thinking is the **problem domain**. Once you know the specific area of analysis, you can set your base and lay out all your requirements and hypotheses before you start investigating.
    - With a solid base in place, you'll be ready to deal with any obstacles that come up. Missing variables can lead to inaccurate conclusions.
    
    ### Scope of work (SOW)
    
    using a scope of work. Scope of work or SOW is an agreed- upon outline of the work you're going to perform on a project. For many businesses, this includes things like work details, schedules, and reports that the client can expect. Now, as a data analyst, your scope of work will be a bit more technical and include those basic items we just mentioned, but you'll also focus on things like data preparation, validation, analysis of quantitative and qualitative datasets, initial results, and maybe even some visuals to really get the point across.
    
    - At this point, try not to confuse statement of work with scope of work, which are both abbreviated as SOW. A statement of work is a document that clearly identifies the products and services a vendor or contractor will provide to an organization. It includes objectives, guidelines, deliverables, schedule, and costs. A scope of work is project-based and sets the expectations and boundaries of a project. A scope of work may be included in a statement of work to help define project outcomes.
    - As a junior data analyst, It's more typical to be asked to create a scope of work than a statement of work.
    
    With a solid scope of work, you'll be able to address any confusion, contradictions, or questions about the data up- front and make sure these sneaky setbacks don't stand in your way.
    
    **Pieces:**
    
    - Deliverables: What work is being done, and what things are being created as a result of this project? When the project is complete, what are you expected to deliver to the stakeholders? Be **specific** here. Will you collect data for this project? How much, or for how long?. Use numbers and aim for hard, measurable goals and objectives.
    - Milestones: This is closely related to your timeline. What are the major milestones for progress in your project? How do you know when a given part of the project is considered complete?. It can be identified by you, by stakeholders, or by other team members such as the Project Manager.
    - Timeline: Your timeline will be closely tied to the milestones you create for your project. The timeline is a way of mapping expectations for how long each step of the process should take. The timeline should be specific enough to help all involved decide if a project is on schedule. When will the deliverables be completed? How long do you expect the project will take to complete? If all goes as planned, how long do you expect each component of the project will take? When can we expect to reach each milestone?
    - Reports: Good SOWs also set boundaries for how and when you’ll give status updates to stakeholders. How will you communicate progress with stakeholders and sponsors, and how often? Will progress be reported weekly? Monthly? When milestones are completed? What information will status reports contain?
    
    At a minimum, any SOW should answer all the relevant questions in the above areas. Note that these areas may differ depending on the project. But at their core, the SOW document should always serve the same purpose by containing information that is specific, relevant, and accurate. If something changes in the project, your SOW should reflect those changes.
    
    SOWs should also contain information specific to what is and isn’t considered part of the project. The scope of your project is everything that you are expected to complete or accomplish, defined to a level of detail that doesn’t leave any ambiguity or confusion about whether a given task or item is part of the project or not.
    
    ### Staying objective
    
    In the world of data, numbers don't mean much without context.
    
    We use data at many different levels. Sometimes our data is descriptive, answering questions like, how much did we spend on travel last month? Data becomes more valuable, as we generate diagnostic and predictive insights, like understanding why travel spend increased last month. Data is most valuable, however, when we can generate prescriptive insights. For example, how can we leverage data to incentivize more efficient travel? 
    
    Figuring out what data means, is just as important as collecting it. As a data analyst, a big part of your job, is putting data into context. It's also up to you, to remain objective and recognize all sides of an argument, before drawing conclusions.
    
    The thing about context, is that it's very personal. Conclusions can be influenced by your own conscious and subconscious biases, which are based on cultural, social and market norms. 
    
    To really understand what the data is about and put information into context, you have to think through who, what, where, when, how and why.
    
    It's good to ask yourself questions like:
    
    - Who: The person or organization that created, collected, and/or funded the data collection
    - What: The things in the world that data could have an impact on
    - Where: The origin of the data
    - When: The time when the data was created or collected
    - Why: The motivation behind the creation or collection. The why can have a particularly strong relationship with bias. Why? Because sometimes, data is collected, or even made up, to serve an agenda.
    
    The best thing you can do for the fairness and accuracy of your data, is to make sure you start with an accurate representation of the population, and collect the data in the most appropriate, and objective way. Then, you'll have the facts so you can pass on to your team
    
    ### The importance of context
    
    Understanding the context behind the data can help us make it more meaningful at every stage of the data analysis process.
    
    Understanding and including the context is important during each step of your analysis process, so it is a good idea to get comfortable with it early in your career. For example, when you collect data, you’ll also want to ask questions about the context to make sure that you understand the business and business process. During organization, the context is important for your naming conventions, how you choose to show relationships between variables, and what you choose to keep or leave out. And finally, when you present, it is important to include contextual information so that your stakeholders understand your analysis.
    
    ## BALANCE TEAM AND STAKEHOLDER NEEDS
    
    Communication is key.
    
    The stakeholders hold stakes in what you're doing. Your stakeholders will want to discuss things like the project objective, what you need to reach that goal, and any challenges or concerns you have. These conversations help build trust and confidence in your work.
    
    Focusing on stakeholder expectations will help you understand the goal of a project, communicate more effectively across your team, and build trust in your work.
    
    ### Types of stakeholders
    
    - Executive team: They set goals, develop strategy, and make sure that strategy is executed effectively. The executive team might include vice presidents, the chief marketing officer, and senior-level professionals who help plan and direct the company’s work. Working closely with your project manager can help you pinpoint the needs of the executive stakeholders for your project, so don’t be afraid to ask them for guidance.
    - Customer-facing team: includes anyone in an organization who has some level of interaction with customers and potential customers. Typically they compile information, set expectations, and communicate customer feedback to other parts of the internal organization. You want to be sure that your analysis and presentation focuses on what is actually in the data-- not on what your stakeholders hope to find.
    - Data science team:
        
        ![Untitled](Google_Data_Analytics_images/Untitled%2017.png)
        
    
    ### Working effectively with stakeholders
    
    - Discuss goals: Start a discussion. Ask about the kind of results the stakeholder wants. Sometimes, a quick chat about goals can help set expectations and plan the next steps.
    - Feel empowered to say “no.”: Whatever the case may be, don’t be afraid to push back when you need to. You should feel empowered to say no-- just remember to give context so others understand why.
    - Plan for the unexpected: Before you start a project, make a list of potential roadblocks. Then, when you discuss project expectations and timelines with your stakeholders, give yourself some extra time for problem-solving at each stage of the process.
    - Know your project: Get to know how your project connects to the rest of the company and get involved in providing the most insight possible. If you have a good understanding about why you are doing an analysis, it can help you connect your work with other goals and be more effective at solving larger problems.
    - Start with words and visuals: To help avoid the illusion of agreement, start with a description and a quick visual of what you are trying to convey.
    - Communicate often: use resources like notes or change-log to create a shareable report.
    
    It is important to concentrate on what matters and not get distracted. By asking yourself a few simple questions at the beginning of each task, you can ensure that you're able to stay focused on your objective while still balancing stakeholder needs:
    
    1. who are the primary and secondary stakeholders?
    2. who is managing the data. By understanding who's managing the data, you can spend your time more productively.
    3. where can you go for help?
    
    ## COMMUNICATION IS KEY
    
    You need effective communication. Before you communicate, think about:
    
    - who your audience is
    - what they already know
    - what they need to know
    - how you can communicate that effectively to them.
    
    When you communicate thoughtfully and think about your audience first, you'll build better relationships and trust with your team members and stakeholders.
    
    ### Tips for effective communication
    
    - learn as you go and ask questions when you aren't sure of something.
    - You'll want your emails to be just as professional as your in-person communications.
        - Good writing practices will go a long way to make your emails professional and easy to understand.
        - read important emails out loud before you hit send; that way, I can hear if they make sense and catch any typos.
        - the tone of your emails can change over time, but being professional is always a good place to start.
        - Don’t let your emails to be too long. You'll want to make sure that your emails are clear and concise so they don't get lost in the shuffle. If what you need to say is too long for a meeting, you might want to set up a meeting instead.
        - Answer in a timely manner.
    - set a reasonable and realistic timeline for the project. Setting expectations for a realistic timeline will help you in the long run.
    - Flag problems early for stakeholders.
    - Set realistic expectations at every stage of the project.
    - You have to make sure that the data tells you the stories. Sometimes people think that data can answer everything and sometimes we have to acknowledge that that is simply untrue.
    - Communication is one of the most valuable tools for working with teams. It's important to start with structured thinking and a well-planned scope of work. If you start with a clear understanding of your stakeholders' expectations, you can then develop a realistic scope of work that outlines agreed upon expectations, timelines, milestones, and reports.
    
    ### Speed VS accuracy
    
    At the end of the day, it's your job to balance fast answers with the right answers.
    
    - Take time to address the bigger problem. You could re-frame the question, outline the problem, challenges, potential solutions, and time-frame. You might say, "I can certainly check out the rates of completion, but I sense there may be more to the story here. Could you give me two days to run some reports and learn what's really going on?”
    - With more time, you can gain context.
    - Redirecting the conversation will help you find the real problem which leads to more insightful and accurate solutions.
    - Communicating about problems, potential solutions and different expectations can help you move forward on a project instead of getting stuck.
    - The fastest answer and the most accurate answer aren't usually the same answer. But by making sure that you understand their needs and setting expectations clearly, you can balance speed and accuracy.
    
    ### Limitations of Data
    
    - Incomplete or nonexistent data: You can still use the data, but you will need to make the limits of your analysis clear. But to be safe, you should be up front about the incomplete dataset until that data becomes available.
    - Don’t miss misaligned data: establishing how to measure things early on standardizes the data across the board for greater reliability and accuracy. This will make sure comparisons between teams are meaningful and insightful.
    - Deal with dirty data: When you find and fix the errors - while tracking the changes you made - you can avoid a data disaster.
    - Tell a clear story:
        - compare the same types of data and double check that any segments in your chart definitely display different metrics.
        - Visualize with care. To make sure your audience sees the full story clearly, it is a good idea to set your Y-axis to 0.
        - Leave out needless graphs.
        - Run statistical tests to see how much confidence you can place in that difference.
        - If you find that you have too little data, be careful about using it to form judgments. Look for opportunities to collect more data, then chart those trends over longer periods.
    
    When you know the limitations of your data, you can make judgment calls that help people make better decisions supported by the data.
    
    ### Think about your process and outcome
    
    does your analysis answer the original question? Are there other angles you haven't considered? Can you answer any questions that may get asked about your data and analysis? How detailed should you be when sharing your results? Would a high level analysis be okay?. Above all else, your data analysis should help your team make better, more informed decisions.
    
    ## AMAZING TEAMWORK
    
    ### Meeting best practices: Do and don’t
    
    Meetings are a huge part of how you communicate with team members and stakeholders.
    
    | DO’S | DON’TS |
    | --- | --- |
    | Come prepare. Read the meeting agenda ahead of time and be ready to provide any updates on your work. | Show up unprepared |
    | Be on time. | Arrive late |
    | Pay attention. Also means asking questions when you need clarification | Be distracted. |
    | Ask questions | Dominate the conversation |
    |  | Talk over others |
    |  | Distract people with unfocused discussions |
    
    ### Leading great meetings
    
    Before the meeting you should:
    
    - Identify your objective. Establish the purpose, goals, and desired outcomes of the meeting, including any questions or requests that need to be addressed.
    - Acknowledge participants and keep them involved with different points of view and experiences with the data, the project, or the business.
    - Organize the data to be presented.
    - Prepare and distribute an agenda. An agenda should include some basic parts as:
        - Meeting start and end time
        - Meeting location, including information to participate remotely, if that option is available.
        - Objectives
        - Background material or data the participants should review beforehead.
    
    During the meeting:
    
    - Make introductions (if necessary) and review key messages
    - Present the data
    - Discuss observations, interpretations, and implications of the data
    - Take notes during the meeting
    - Determine and summarize the next steps for the group
    
    After the meeting:
    
    - Distribute any notes or data
    - Confirm next steps and timeline for additional actions
    - Ask for feedback (this is an effective way to figure out if you missed anything in your recap)
    
    ### From conflict to collaboration
    
    A conflict can pop up for a variety of reasons, and there are some ways to resolve it and move foward:
    
    - Try and be objective and stay focused on the team's goals.
    - re-frame the problem. Instead of focusing on what went wrong or who to blame, change the question you're starting with. Try asking, how can I help you reach your goal?
    - If you find yourself in the middle of a conflict, try to communicate, start a conversation or ask things like, are there other important things I should be considering?
    - If you find yourself feeling emotional, give yourself some time to cool off so you can go into the conversation with a clearer head.
    - If you find you don't understand what your team member or stakeholder is asking you to do, try to understand the context of their request. Ask them what their end goal is, what story they're trying to tell with the data or what the big picture is.
    
- 3 - **PREPARE** DATA FOR EXPLORATION
    
    when you prepare the data correctly, understand the different types of data and structure that comes in. Knowing this lets you figure out what type of data is right for the question you're answering. Plus, you'll gain practical skills about how to extract, use, organize, and protect your data.
    
    ### How data is collected
    
    - Interview
    - Observations
    - Forms
    - Questionnaires
    - Surveys
    - Cookies. Cookies are small files stored on computers that contain information about users. Cookies can help inform advertisers about your personal interests and habits based on your online surfing, without personally identifying you (track people's online activities and interests).
    
    ### Factors to consider when you are collecting data
    
    - How the data will be collected. Decide if you will collect the data using your own resources or receive (and possibly purchase it) from another party.
        - First-party data: Data collected by an individual or group using their own resources. It is typically the preferred method because you know exactly where it came from.
    - Choose data sources: If you don’t collect the data using your own resources, you might get data from second-party or third-party data providers.
        - Second-party data: data collected by a group directly from its audience and then sold.
        - Third-party data: Data collected from outside sources who did not collect it directly. This data might have come from a number of different sources before you investigated it. It might not be as reliable, but that doesn't mean it can't be useful. You'll just want to make sure you check it for accuracy, bias, and credibility.
    - Decide what data to use. Be sure to choose data that can actually help solve your problem question.
    - How much data to collect. If you are collecting your own data, make reasonable decisions about sample size, or we may want to choose a sample of the population if it is too challenging. Each project has its own needs.
    - Select the right data type
    - Determine the time frame. If you are collecting your own data, decide how long you will need to collect it, especially if you are tracking trends over a long period of time. If you need an immediate answer, you might not have time to collect new data. In this case, you would need to use historical data that already exists.
    
    ![Untitled](Google_Data_Analytics_images/Untitled%2018.png)
    
    ## DIFFERENTIATE BETWEEN DATA FORMATS AND STRUCTURES
    
    ### Qualitative data
    
    it can't be counted, measured, or easily expressed using numbers. Qualitative data is usually listed as a name, category, or description. 
    
    - **Nominal data:** it’s data categorized without a set order (this data doesn't have a sequence). For example:
        - First time customer, returning customer, regular customer
        - New job applicant, existing applicant, internal applicant
        - New listing, reduced price listing, foreclosure
        - Yes/no questions
    - **Ordinal data:** it is a type of qualitative data with a set order or scale. For example:
        - Movie ratings (number of stars: 1 star, 2 stars, 3 stars)
        - Ranked-choice voting selections (1st, 2nd, 3rd)
        - Income level (low income, middle income, high income)
    
    ### Quantitative data
    
    It can be measured or counted and then expressed as a number. This is data with a certain quantity, amount, or range. We can break it down into discrete or continuous data
    
    - **Discrete data:** This is data that's counted and has a limited number of values. When partial measurements (half-stars or quarter-points) aren't allowed, the data is discrete. If you don't accept anything other than full stars or points, the data is considered discrete.
    - **Continuous data:** data can be measured using a timer, and its value can be shown as a decimal with several places.
    
    ### Internal data
    
    It is data that lives within a company's own systems. It's usually more reliable and easier to collect
    
    ### External data
    
    Data that lives and is generated outside of an organization. External data becomes particularly valuable when your analysis depends on as many sources as possible; and it is structured.
    
    ### Structured data
    
    Data that's organized in a certain format, such as rows and columns. Spreadsheets and relational databases are two examples of software that can store data in a structured way. We will be working with structured data most of the time. Structured data works nicely within a data model. Data models help to keep data consistent and provide a map of how data is organized. Structured data can be applied directly to charts, graphs, heat maps, dashboards and most other visual representations of data.
    
    - Spreadsheets
    - Databases that store datasets
    
    ### Unstructured data
    
    Data that is not organized in any easily identifiable manner (there's no clear way to identify or organize their content). Unstructured data might have an internal structure, but the data doesn't fit neatly in rows and columns like structured data. For example (These can be harder to analyze in their unstructured format.):
    
    - Social media posts
    - Emails
    - Videos and audio files
    - Photos
    
    ![Untitled](Google_Data_Analytics_images/Untitled%2019.png)
    
    | **STRUCTURED DATA** | **UNSTRUCTURED DATA** |
    | --- | --- |
    | Defined data types | Varied data types |
    | Most often quantitative data | Most often qualitative data |
    | Easy to organize | Difficult to search |
    | Easy to search | Provides more freedom for analyisis |
    | Easy to analyze | Stored in data lakes, data warehouses, and NoSQL databases |
    | Stored in relational databases and data warehouses | Can’t be put in rows and columns |
    | Examples: Excel, Google sheets, SQL, customer data, phone records, transaction history | Examples: text messages, social media comments, phone call transcriptions, various log files, images, audio, video |
    
    The lack of structure makes unstructured data difficult to search, manage, and analyze. But recent advancements in artificial intelligence and machine learning algorithms are beginning to change that. Now, the new challenge facing data scientists is making sure these tools are inclusive and unbiased. Otherwise, certain elements of a dataset will be more heavily weighted and/or represented than others. And an unfair dataset does not accurately represent the population, causing skewed outcomes, low accuracy levels, and unreliable analysis.
    
    ## DATA MODELING LEVELS AND TECHNIQUES
    
    Data models help keep data consistent and enable people to map out how data is organized. Data modeling is the process of creating diagrams that visually represent how data is organized and structured. These visual representations are called data models. Different users might have different data needs, but the data model gives them an understanding of the structure as a whole.
    
    ### The three most common types of data modeling
    
    - **Conceptual data modeling:** gives a high-level view of the data structure, such as how data interacts across an organization. A conceptual data model doesn't contain technical details.
    - **Logical data modeling:** focuses on the technical details of a database such as relationships, attributes, and entities.
    - **Physical data modeling:** depicts (describes) how a database operates. A physical data model defines all entities and attributes used
    
    ![Untitled](Google_Data_Analytics_images/Untitled%2020.png)
    
    ### Data-modeling techniques
    
    There are two common methods:
    
    - **Entity Relationship Diagram (ERD):** a visual way to understand the relationship between entities in the data model.
    - **Unified Modeling Language (UML) diagram:** are very detailed diagrams that describe the structure of a system by showing the system's entities, attributes, operations, and their relationships.
    
    There are different data modeling techniques, but in practice, you will probably be using your organization’s existing technique.
    
    ### Data analysis and data modeling
    
    Data modeling can help you explore the high-level details of your data and how it is related across the organization’s information systems. Data modeling sometimes requires data analysis to understand how the data is put together; that way, you know how to map the data. And finally, data models make it easier for everyone in your organization to understand and collaborate with you on your data.
    
    ## DATA TYPES, FIELDS AND VALUES
    
    A data type is a specific kind of data attribute that tells what kind of value the data is. A data type tells you what kind of data you're working with. Data types can be different depending on the query language you're using.
    
    ### Data types in spreadsheets
    
    - Number
    - Text or string: a sequence of characters and punctuation that contains textual information. It can also contain numbers, but wouldn’t be used for calculations, so they are treated like texts, not numbers.
    - Boolean: A data type with only two possible values, such as TRUE or FALSE.
    
    ### Boolean logic
    
    Data analysts use Boolean statements to do a wide range of data analysis tasks, such as creating queries for searches and checking for conditions when writing programming code. The boolean logic is easy to understand with a Venn diagram:
    
    ![Untitled](Google_Data_Analytics_images/Untitled%2021.png)
    
    - AND: is the center of the Venn diagram, where two conditions overlap. The AND operator lets you stack multiple conditions.
    - OR: includes either condition. The OR operator lets you move forward if either one of your two conditions is met.
    - NOT: includes only the part of the Venn diagram that doesn't contain the exception. The NOT operator lets you filter by subtracting specific conditions from the results.
    
    ### Data table components
    
    A data table, or tabular data is arranged in rows and columns. You can call the rows "records" and the columns "fields." They basically mean the same thing, but records and fields can be used for any kind of data table, while rows and columns are usually reserved for spreadsheets. People in data analytics usually go with "records" and "fields”. Sometimes a field can also refer to a single piece of data, like the value in a cell. Each separate field has the same data type, but different fields can have different types.
    
    ### Wide data and long data
    
    - Wide data:
        - Every data subject has a single row with multiple columns to hold the values of various attributes of the subject.
        - each row contains multiple data points for the particular items identified in the columns.
        - It lets you easily identify and quickly compare different columns.
        - Wide data is easier to read and understand. That is why data analysts typically transform long data to wide data more often than they transform wide data to long data.
        - Each column contains a unique data variable.
        
        ![Untitled](Google_Data_Analytics_images/Untitled%2022.png)
        
    - Long data:
        - Each row is one time point per subject, so each subject will have data in multiple rows.
        - each row contains a single data point for a particular item.
        - It is a great format for storing and organizing data when there's multiple variables for each subject at each time point that we want to observe.
        - We can store and analyze all of this data using fewer columns. Plus, if we added a new variable we'd only need one more column.
        - The long data format keeps everything nice and compact.
        - Separate columns contain the values and the context for the values
        
        ![Untitled](Google_Data_Analytics_images/Untitled%2023.png)
        
    
    | **Wide data is preferred when:** | **Long data is preferred when:** |
    | --- | --- |
    | Creating tables and charts with a few variables about each subject | Storing a lot of variables about each subject. For example, 60 years worth of interest rates for each bank |
    | Comparing straightforward line graphs | Performing advanced statistical analysis or graphing |
    
    ### Transforming data
    
    Data transformation is the process of changing the data’s format, structure, or values. Data transformation usually involves:
    
    - Adding, copying, or replicating data
    - Deleting fields or records
    - Standardizing the names of variables
    - Renaming, moving, or combining columns in a database
    - Joining one set of data with another
    - Saving a file in a different format. For example, saving a spreadsheet as a comma-separated values (CSV) file.
    
    The goals for data transformation might be:
    
    - Data **organization**: a better organized data is easier to use
    - Data **compatibility**: different applications or systems can then use the same data
    - Data **migration**: data with matching formats can be moved from one system to another
    - Data **merging**: data with the same organization can be merged together
    - Data **enhancement**: data can be displayed with more detailed fields
    - Data **comparison**: apples-to-apples comparisons of the data can then be made
    
    ## ANALYZE DATA FOR BIAS AND CREDIBILITY
    
    Even the most sound data can be skewed or misinterpreted.
    
    ### Biased and unbiased data
    
    Bias is a preference in favor of or against a person, group of people, or thing. It can be conscious or subconscious. We're biased when we have preferences based on our own preconceived or even subconscious notions.
    
    Data bias is a type of error that systematically skews results in a certain direction.
    
    Bias can also happen if a sample group lacks inclusivity. The way you collect data can also bias a data set.
    
    ### Types of data bias
    
    - **Sampling bias:** A sample that isn't representative of the population as a whole. You can avoid this by making sure the sample is chosen at random, so that all parts of the population have an equal chance of being included. If you don't use random sampling during data collection, you end up favoring one outcome.
        - Unbiased sampling results in a sample that's representative of the population being measured. Another great way to discover if you're working with unbiased data is to bring the results to life with visualizations.
    - **Observer bias** (sometimes referred to as experimenter bias or research bias): it's the tendency for different people to observe things differently.
    - **Interpretation bias:** The tendency to always interpret ambiguous situations in a positive, or negative way. It can lead to two people seeing or hearing the exact same thing, and interpreting it in a variety of different ways, because they have different backgrounds, and experiences.
    - **Confirmation bias:** the tendency to search for, or interpret information in a way that confirms preexisting beliefs. This happens all the time in everyday life.
    
    All these types of data bias are unique, but they have one thing in common: They each affect the way we collect, and make sense of the data.
    
    ## DATA CREDIBILITY
    
    There's some best practices to follow that'll help you measure the reliability of data sets before you use them.
    
    ### How to identify “good” data
    
    We can apply the process “ROCC”:
    
    - **Reliable.** With this data you can trust that you're getting accurate, complete and unbiased information that's been vetted and proven fit for use.
    - **Original.** To make sure you're dealing with good data, be sure to validate it with the original source.
    - **Comprehensive.** The best data sources contain all critical information needed to answer the question or find the solution.
    - **Current.** The best data sources are current and relevant to the task at hand. For example:
        - data.gov, which is home to the U.S. government's open data.
    - **Cited.** Citing makes the information you're providing more credible. When you're choosing a data source, think about three things. Who created the data set? Is it part of a credible organization? When was the data last refreshed?
    
    If you have original data from a reliable organization and it's comprehensive, current, and cited, it ROCCCs! (or more seriously: it's good).
    
    For good data, stick with vetted public data sets, academic papers, financial data and governmental agency data.
    
    ### How to identify “bad” data
    
    They're not reliable, original, comprehensive, current or cited. Even worse, they could be flat-out wrong or filled with human error.
    
    - **NOT Reliable.** Bad data can't be trusted because it's inaccurate, incomplete, or biased. For example:
        - data that has sample selection bias because it doesn't reflect the overall population
        - data visualizations and graphs that are just misleading.
    - **NOT Original.** If you can't locate the original data source and you're just relying on second or third party information, that can signal you may need to be extra careful in understanding your data.
    - **NOT Comprehensive.** Bad data sources are missing important information needed to answer the question or find the solution. What's worse, they may contain human error, too.
    - **NOT Current.** Bad data sources are out of date and irrelevant.
    - **NOT Cited.** If your source hasn't been cited or vetted, it's a no-go.
    
    It's important for data analysts to understand and keep an eye out for bad data because it can have serious and lasting impacts. Whether it's an incorrect conclusion leading to one bad business decision, or inaccurate information causing processes to fail and putting populations at risk, every good solution is found by avoiding bad data.
    
    ## DATA ETHICS AND PRIVACY
    
    One practical view is that ethics refers to well-founded standards of right and wrong that prescribe what humans ought to do, usually in terms of rights, obligations, benefits to society, fairness or specific virtues.
    
    Data ethics refers to well- founded standards of right and wrong that dictate how data is collected, shared, and used.
    
    The GDPR (General Data Protection Regulation of the European Union) of the European Union is a data protection legislation to help protect people and their data.
    
    Data ethics tries to get to the root of the accountability companies have in protecting and responsibly using the data they collect.
    
    ### Aspects of data ethics (data ethics concerns)
    
    - **Ownership:** This answers the question who owns data?: individuals who own the raw data they provide, and they have primary control over its usage, how it's processed and how it's shared.
    - **Transaction transparency:** All data processing activities and algorithms should be completely explainable and understood by the individual who provides their data. This is in response to concerns over data bias.
    - **Consent:** an individual's right to know explicit details about how and why their data will be used before agreeing to provide it. They should know answers to questions like why is the data being collected? How will it be used? How long will it be stored?. The best way to give consent is probably a conversation between the person providing the data and the person requesting it, but it usually looks like a terms and conditions checkbox with links to more details. Consent is important because it prevents all populations from being unfairly targeted which is a very big deal for marginalized groups who are often disproportionately misrepresented by biased data.
    - **Currency:** Individuals should be aware of financial transactions resulting from the use of their personal data and the scale of these transactions.
    - **Privacy:** Privacy is personal. We may all define privacy in our own way, and we're all entitled to it. When talking about data, privacy means preserving a data subject's information and activity any time a data transaction occurs. This is sometimes called information privacy or data protection. It's all about access, use, and collection of data. It also covers a person's legal right to their data; this means people should have:
        - protection from unauthorized access to our private data
        - freedom from inappropriate use of our data
        - the right to inspect, update, or correct our data
        - ability to give consent to use our data
        - legal right to access our data
        
        For companies, it means putting privacy measures in place to protect the individuals' data. The importance of data privacy has been recognized by governments worldwide, and they've started creating data protection legislation to help protect people and their data.
        
    - **Openness:** When referring to data, openness refers to free access, usage and sharing of data. We should still be transparent, respect privacy, and make sure we have consent for data that's owned by others.
    
    ### Data anonymization
    
    Personally identifiable information, or PII, is information that can be used by itself or with other data to track down a person's identity (is data that is reasonably likely to identify a person and make information known about them). Data anonymization is the process of protecting people's private or sensitive data by eliminating that kind of information. Typically, data anonymization involves blanking, hashing, or masking personal information, often by using fixed-length codes to represent data columns, or hiding data with altered values.
    
    As a data analyst, you might be expected to understand what data needs to be anonymized, but you generally wouldn't be responsible for the data anonymization itself. A rare exception might be if you work with a copy of the data for testing or development purposes. In this case, you could be required to anonymize the data before you work with it.
    
    Healthcare and financial data are two of the most sensitive types of data. Data in these two industries usually goes through de-identification, which is a process used to wipe data clean of all personally identifying information. The data that is often anonymized can be:
    
    - Telephone numbers
    - Names
    - License plates and license numbers
    - Social security numbers
    - IP addresses
    - Medical records
    - Email addresses
    - Photographs
    - Account numbers
    
    Data anonymization is one of the ways we can keep data private and secure!
    
    ## OPEN DATA
    
    Open data is part of data ethics, which has to do with using data ethically. Openness refers to free access, usage, and sharing of data.
    
    ### Open-data standards
    
    Open data must meet three standards:
    
    - Be available and accessible to the public as a complete dataset. Be available as a whole, preferably by downloading over the Internet in a convenient and modifiable form. The website [data.gov](http://data.gov/) is a great example.
    - Be provided under terms that allow it to be reused and redistributed. Open data must be provided under terms that allow reuse and redistribution including the ability to use it with other datasets.
    - Universal participation. Everyone must be able to use, reuse, and redistribute the data. There shouldn't be any discrimination against fields, persons, or groups. No one can place restrictions on the data like making it only available for use in a specific industry.
    
    ### Pros or benefits of open data
    
    Just imagine the impact that would have on scientific collaboration, research advances, analytical capacity, and decision-making.
    
    - credible databases can be used more widely.
    - All of that good data can be leveraged, shared, and combined with other data.
    - Interoperability is key to open data's success. But this kind of interoperability requires a lot of cooperation. While there is serious potential in the open, timely, fair, and simple sharing of data, its future will depend on how effectively larger challenges are addressed.
    - It improves public service by giving people ways to be a part of public planning or provide feedback to the government.
    - open data leads to innovation and economic growth by helping people and companies better understand their markets.
    
    ### Sites and resources for open data
    
    - [U.S. government data site]([https://data.gov/](https://data.gov/)): This resource gives users the data and tools that they need to do research, and even helps them develop web and mobile applications and design data visualizations.
    - [U.S. Census Bureau]([https://www.census.gov/data.html](https://www.census.gov/data.html)): It offers demographic information from federal, state, and local governments, and commercial entities in the U.S. too.
    - [Open Data Network]([https://www.opendatanetwork.com/](https://www.opendatanetwork.com/)): It has a really powerful search engine and advanced filters. Here, you can find data on topics like finance, public safety, infrastructure, and housing and development.
    - [Google Cloud Public Datasets]([https://cloud.google.com/datasets?hl=es-419](https://cloud.google.com/datasets?hl=es-419)): There are a selection of public datasets available through the Google Cloud Public Dataset Program that you can find already loaded into BigQuery.
    - [Dataset Search]([https://datasetsearch.research.google.com/](https://datasetsearch.research.google.com/)): It is a search engine designed specifically for data sets; you can use this to search for specific data sets.
    - [Kaggle datasets]([https://www.kaggle.com/datasets](https://www.kaggle.com/datasets)): you can create a new dataset or search for datasets created by other Kagglers.
    
    ## WORKING WITH DATABASES
    
    Databases store and organize data, making it much easier for data analysts to manage and access information. They help us get insights faster, make data-driven decisions, and solve problems.
    
    ### Database features
    
    - **Relational database:** is a database that contains a series of related tables that can be connected via their relationships. They allow data analysts to organize and link data based on what the data has in common. For two tables to have a relationship, one or more of the same fields must exist inside both tables.
        
        In a non-relational table, you will find all of the possible variables you might be interested in analyzing all grouped together. This is one reason why relational databases are so common in data analysis: they simplify a lot of analysis processes and make data easier to find and use across an entire database.
        
    - **Primary key:** is an identifier that references a column in which each value is unique, meaning no two rows can have the same primary key, and it cannot be null or blank. In other words, it's a column of a table that is used to uniquely identify each record within that table.
        - A primary key is used to ensure data in a specific column is unique.
        - It uniquely identifies a record in a relational database table.
        - Only one primary key is allowed in a table
        - Cannot contain null or blank values.
        - Some tables don't require a primary key.
    - **Foreign keys:** is a column or group of columns in a relational database table that provides a link between the data and two tables. These keys are what create the relationships between tables in a relational database, which helps organize and connect data across multiple tables in the database.
        - It is a field within a table that's a primary key in another table.
        - A foreign key is how one table can be connected to another.
        - More than one foreign key is allowed to exist in a table.
    - **Composite key:** is a candidate key that consists of two or more attributes (table columns) that together uniquely identify an entity occurrence (table row). A primary key may also be constructed using multiple columns of a table. This type of primary key is called a composite key.
    - **SQL:** Databases use a special language to communicate called a query language. Structured Query Language (SQL) is a type of query language that lets data analysts communicate with a database.
    
    ### Database normalization
    
    Normalization is a process of organizing data in a relational database. For example, creating tables and establishing relationships between those tables. It is applied to eliminate data redundancy, increase data integrity, and reduce complexity in a database.
    
    ### Inspecting a dataset
    
    Inspecting your dataset will help you pinpoint what questions are answerable and what data is still missing. You may be able to recover this data from an external source or at least recommend to your stakeholders that another data source be used.
    
    ## METADATA
    
    Metadata is not the data itself, instead is data about data. Metadata is extremely important when working with databases. Think of it like a reference guide. Metadata tells you where the data comes from, when and how it was created, and what it's all about.
    
    Metadata is information that's used to describe the data that's contained in something, like a photo or an email.
    
    In database management, it provides information about other data and helps data analysts interpret the contents of the data within a database.
    
    In essence, metadata tells the who, what, when, where, which, how, and why of data.
    
    ### Elements of metadata
    
    - **Title and description:** What is the name of the file or website you are examining? What type of content does it contain?
    - **Tags and categories:** What is the general overview of the data that you have? Is the data indexed or described in a specific way?
    - **Who created it and when:** Where did the data come from, and when was it created? Is it recent, or has it existed for a long time?
    - **Who last modified it and when:** Were any changes made to the data? If yes, were the modifications recent?
    - **Who can access or update it:** Is this dataset public? Are special permissions needed to customize or modify the dataset?
    
    ### Types of metadata
    
    As a data analyst, there are three common types of metadata that you’ll come across:
    
    - **Descriptive:** metadata that describes a piece of data and can be used to identify it at a later point in time.
    - **Structural:** metadata that indicates how a piece of data is organized and whether it's part of one or more than one data collection. Structural metadata also keeps track of the relationship between two things.
    - **Administrative:** metadata that indicates the technical source of a digital asset.
    
    ### Examples of metadata
    
    - Photos. Whenever a photo is captured with a camera, metadata such as camera filename, date, time, and geolocation are gathered and saved with it.
    - Emails. When an email is sent or received, there is lots of visible metadata such as subject line, the sender, the recipient and date and time sent. There is also hidden metadata that includes server names, IP addresses, HTML format, and software details.
    - Spreadsheets and documents. Titles, author, creation date, number of pages, user comments as well as names of tabs, tables, and columns are all metadata that one can find in spreadsheets and documents.
    - Websites. Every web page has a number of standard metadata fields, such as tags and categories, site creator’s name, web page title and description, time of creation and any iconography.
    - Digital files. Usually, if you right click on any computer file, you will see its metadata. This could consist of file name, file size, date of creation and modification, and type of file.
    - Books. Metadata is not only digital. Every book has a number of standard metadata on the covers and inside that will inform you of its title, author’s name, a table of contents, publisher information, copyright description, index, and a brief description of the book’s contents.
    
    ### Benefits of using metadata
    
    - Putting data into context is probably the most valuable thing that metadata does.
    - Metadata creates a single source of truth by keeping things consistent and uniform. Plus, when a database is consistent, it's so much easier to discover relationships between the data inside it and the data elsewhere.
    - Metadata makes data more reliable by making sure it's accurate, precise, relevant, and timely. This also makes it easier for data analysts to identify the root causes of any problems that might pop up.
    
    One of the ways data analysts make sure their data is consistent and reliable is by using something called a metadata repository. 
    
    ### Important aspects of metadata
    
    metadata is stored in a single, central location and it gives the company standardized information about all of its data. This is done in 2 ways:
    
    1. Metadata includes information about where each system is located and where the data sets are located within those systems.
    2. The metadata describes how all of the data is connected between the various systems.
    
    Another important aspect of metadata is something called data **governance**. Data governance is a process to ensure the formal management of a company’s data assets. This gives an organization better control of their data and helps a company manage issues related to data security and privacy, integrity, usability, and internal and external data flows.
    
    Data governance is about more than just standardizing terminology and procedures. It's about the roles and responsibilities of the people who work with the metadata every day. These are metadata specialists, and they organize and maintain company data, ensuring that it's of the highest possible quality. These people create basic metadata identification and discovery information, describe the way different data sets work together, and explain the many different types of data resources. Metadata specialists also create very important standards that everyone follows and the models used to organize the data. Metadata analysts are great team players. They're passionate about making data accessible by sharing with colleagues and other stakeholders.
    
    ### Metadata repository
    
    A metadata repository is a database specifically created to store metadata. These repositories describe where metadata came from, keep it in an accessible form so it can be used quickly and easily, and keep it in a common structure for everyone who may need to use it. Metadata repositories make it easier and faster to bring together multiple sources for data analysis. They do this by:
    
    - describing the state and location of the metadata
    - describing the structure of the tables inside
    - describing how data flows through the repository
    - Keeping track of who accesses the metadata and when
    
    They help ensure that my team is pulling the right content for the particular project and using it appropriately. We can confirm this because the metadata clearly describes how and when the data was collected, how it's organized, and much more.
    
    Using a metadata repository, a data analyst can find it easier to bring together multiple sources of data, confirm how or when data was collected, and verify that data from an outside source is being used appropriately.
    
    ## ACCESSING DIFFERENT DATA SOURCES
    
    There are two basic types of data used by data analysts: internal (also described as primary data) and external (sometimes called secondary data).
    
    ### Benefits of internal and external data
    
    | INTERNAL | EXTERNAL |
    | --- | --- |
    | It provides information that's relevant to problems you're trying to solve | Its is used when internal data doesn’t give you the full picture |
    | it's free to access because the company already owns it | use their data to create deeper analyses and add some more industry- level perspective. |
    | analysts can work on all data projects without ever looking beyond their own walls. |  |
    
    ### Importing data from other spreadsheets
    
    - Google Sheets: You can use the **IMPORTRANGE** function. It enables you to specify a range of cells in the other spreadsheet to duplicate in the spreadsheet you are working in. You must allow access to the spreadsheet containing the data the first time you import the data.
    - Microsoft excel:
        1. Select **Data** from the main menu.
        2. Click **Get Data**, and then select **From File** within the toolbar. In the drop down, choose **From Excel Workbook**
        3. Browse for and select the spreadsheet file and then click **Import**.
        4. In the Navigator, select which worksheet to import.
        5. Click **Load** to import all the data in the worksheet; or click **Transform Data** to open the Power Query Editor to adjust the columns and rows of data you want to import.
        6. If you clicked Transform Data, click **Close & Load** and then select one of the two options:
            - **Close & Load** - import the data to a new worksheet
            - **Close & Load to...** - import the data to an existing worksheet
    
    ### Importing data from CSV files
    
    CSV files use plain text and they're delineated by characters. So each column or field is clearly distinct from another when importing. CSVs are comma-separated, and usually the spreadsheet app will auto-detect those separations. But sometimes, you might need to indicate that the separator is another character or a space by selecting the different options. A CSV file also Examine a small subset of a large dataset
    
    - Google Sheets:
        1. Open the **File** menu in your spreadsheet and select **Import** to open the Import file window.
        2. Select **Upload** and ****then ****select the CSV file you want to import.
        3. You will have a few options. For Import location, you can choose to replace the current spreadsheet, create a new spreadsheet, insert the CSV data as a new sheet, add the data to the current spreadsheet, or replace the data in a specific cell. The data will be inserted as plain text only if you uncheck the Convert text to numbers, dates, and formulas checkbox, which is the default setting. Sometimes a CSV file uses a separator like a semi-colon or even a blank space instead of a comma. For Separator type, you can select Tab or Comma, or select Custom to enter another character that is being used as the separator.
        4. Select **Import data**. The data in the CSV file will be loaded into your sheet, and you can begin using it!
        
        You can also use the **IMPORTDATA** function in a spreadsheet cell to import data using the URL to a CSV file.
        
    - Microsoft excel:
        1. Open a new or existing spreadsheet
        2. Click **Data** in the main menu and select the **From Text/CSV** option.
        3. Browse for and select the CSV file and then click **Import**.
        4. From here, you will have a few options. You can change the delimiter from a comma to another character such as a semicolon. You can also turn automatic data type detection on or off. And, finally, you can transform your data by clicking **Transform Data** to open the Power Query Editor.
        5. In most cases, accept the default settings in the previous step and click **Load** to load the data in the CSV file to the spreadsheet. The data in the CSV file will be loaded into the spreadsheet, and you can begin working with the data.
    
    ### Importing HTML tables from web pages
    
    Importing HTML tables is a very basic method to extract or "scrape" data from public web pages.
    
    - Google Sheets: you can use the **IMPORTHTML** function. It enables you to import the data from an HTML table (or list) on a web page.
    - Microsfot excel: You can import data from web pages using the **From Web** option:
        1. Open a new or existing spreadsheet.
        2. Click Data in the main menu and select the **From Web** option.
        3. Enter the URL and click OK.
        4. In the Navigator, select which table to import.
        5. Click **Load** to load the data from the table into your spreadsheet.
    
    ## SORTING AND FILTERING
    
    Having lots of data can make it difficult to quickly find and analyze the information you need. No two analytics projects are the same. Often data analysts process, view, and use data very differently, even if it comes from the exact same source. 
    
    Sorting and filtering the data in a spreadsheet helps us:
    
    - customize the way data is presented.
    - organize data so analysts can zoom in on the pieces that matter.
    
    ### Sorting
    
    Sorting involves arranging data into a meaningful order to make it easier to understand, analyze, and visualize.
    
    - Data can be sorted in ascending or descending order, and alphabetically or numerically (choosing the column > right click > sort).
        - A → Z to sort in ascending order.
        - Z → A to sort in descending order.
    - Sorting can be done across all of a spreadsheet or just in a single column or table.
    - You can also sort by multiple variables (multiple criteria sorting).
    - The details across each row are automatically kept together when sorting a particular section
    
    Anytime you're sorting data, it's always a good idea to freeze the header row first. To do this, we'll highlight the row. Then from the view menu, choose freeze and one row.
    
    ![Untitled](Google_Data_Analytics_images/Untitled%2024.png)
    
    To do multiple criteria sorting:
    
    1. Select the entire dataset
    2. Select Data and Sort range, and then choose Advanced range sorting options to view the dialog box
    3. In the dialog box, make sure that "Data has header row" is highlighted.
    4. Now add another all the sort columns you need.
    5. Finally, select Sort.
    
    ### Filter
    
    Filtering means showing only the data that meets a specific criteria while hiding the rest.
    
    - simplifies a spreadsheet by only showing us the information we need.
    - The filter temporarily hides anything that doesn't meet the condition. But note that, even though they aren't visible, they're still there. When it's time to view the entire area spreadsheet again, simply turn off the filter.
    
    ### Ways to clean your data
    
    - Sorting
    - Removing incorrect data (using filters)
    - Filling in missing data (using filters)
    - Converting data. it's sometimes necessary to change text data (words) to numeric data (numbers). To do this, you can match specific number values to the text data in each column (using find and replace is easier).
        
        ![Untitled](Google_Data_Analytics_images/Untitled%2025.png)
        
    
    ## SPREADSHEETS AND DATASETS: COMPARE AND CONTRAST
    
    | QUESTION | SPREADSHEETS | DATABASES |
    | --- | --- | --- |
    | Can be relational? | are better-suited to self-contained data, where the data exists in one place. | you can use databases to store data from external tables, allowing you to change data in several places by editing in only one place. |
    | How do they store data? | Stores data in cells | Stores data in tables. |
    | How are they used to interact with data? | You can input functions, or math equations, that allow you to manipulate this data. | The primary way to work with a relational database is to use SQL |
    | How powerful is each? | They’re a powerful tool for tasks like crunching numbers, storing lists, and budget tracking. |  |
    | What are their pros and cons when sorting? |  |  |
    | What are their pros and cons when filtering? |  |  |
    
    ## WORKING WITH LARGE DATASETS IN SQL
    
    [BigQuery]([https://cloud.google.com/bigquery/docs?hl=es-419](https://cloud.google.com/bigquery/docs?hl=es-419)) is a data warehouse on Google Cloud that data analysts can use to query, filter large datasets, aggregate results, and perform complex operations.
    
    `https://console.cloud.google.com/bigquery`
    
    ### Setting up BigQuery
    
    There are two account types without charges:
    
    - Sandbox: A Sandbox account is available at no charge and anyone with a Google account can log in and use it. There are a couple of limitations to this account type. There are a couple of limitations:
        - you get a maximum of 12 projects at a time.
        - Cannot insert new records to a database
        - Cannot update the field values of existing records.
        
        These last Data Manipulation Language or DML operations aren't supported
        
    - Free Trial: The free trial gives you access to more of what BigQuery has to offer with fewer overall limitations.
        - $300 in credit for use in Google Cloud during the first 90 days.
        - select to upgrade to a paid account to keep working in Google Cloud.
        - You will never be automatically charged
    
    ### How to: BigQuery
    
    Link to the BigQuery landing page: [BigQuery console](https://www.notion.so/console.cloud.google.com/bigquery)
    
    - Go to the SQL workspace (from the BigQuery landing page):
        
        ![Untitled](Google_Data_Analytics_images/Untitled%2026.png)
        
    - Search for public datasets.
        
        ![Untitled](Google_Data_Analytics_images/Untitled%2027.png)
        
    - Follow these steps to find and pin the bigquery-public-data.
        1. Navigate to the Explorer menu in BigQuery.
        2. Type the word public in the search box and enter.
        3. Click "Broaden search to all projects"
        4. Find the bigquery-public-data and pin it.
    - Create a new editor window with the template for a query already populated (the * does not display automatically).
        
        ![Untitled](Google_Data_Analytics_images/Untitled%2028.png)
        
        ![Untitled](Google_Data_Analytics_images/Untitled%2029.png)
        
    - Add your own data to BigQuery
        
        ![Untitled](Google_Data_Analytics_images/Untitled%2030.png)
        
        ![Untitled](Google_Data_Analytics_images/Untitled%2031.png)
        
        ![Untitled](Google_Data_Analytics_images/Untitled%2032.png)
        
        ![Untitled](Google_Data_Analytics_images/Untitled%2033.png)
        
        ![Untitled](Google_Data_Analytics_images/Untitled%2034.png)
        
    
    ### Queries
    
    ```sql
    SELECT * 
    FROM '*DatabaseName.TableName'*
    WHERE *ColumnName (condition)*
    ```
    
    - Most Queries begin with the word SELECT
    - “*” means we want to include all columns.
    - SQL queries can be written in a lot of different ways, but still provide the same results. The additional lines and spaces don't impact the query's outcome, but they keep your query organized and easier to read for yourself and others.
    - **SELECT** is the section of a query that indicates what data you want SQL to return to you
    - **FROM** is the section of a query that indicates which table the desired data comes from.
    - **WHERE** is the section of a query ****that indicates any filters you’d like to apply to your dataset
    
    ### SQL best practices
    
    Check Google drive for a SQL best practices document; here is a small resume:
    
    - use **all caps** for clause starters (e.g., SELECT, FROM, WHERE, etc.), functions (e.g., SUM()).
    - Column names should be all **lowercase**
    - Table names should be in **CamelCase**
    - Vendors of SQL databases may use slightly different variations of SQL. These variations are called **SQL dialects**. Some SQL dialects are case sensitive. BigQuery is one of them. Vertica is another. But most, like MySQL, PostgreSQL, and SQL Server, aren’t case sensitive. This means if you searched for country_code = ‘us’, it will return all entries that have 'us', 'uS', 'Us', and 'US'. This isn’t the case with BigQuery. BigQuery is case sensitive, so that same search would only return entries where the country_code is exactly 'us'. If the country_code is 'US', BigQuery wouldn’t return those entries as part of your result.
    - For the most part, it also doesn’t matter if you use single **quotes ' '** or **double quotes " "** when referring to strings. But there are two situations where it does matter what kind of quotes you use:
        1. When you want strings to be identifiable in *any* SQL dialect. A general rule across almost all SQL dialects is to use single quotes for strings. This helps get rid of a lot of confusion.
        2. When your string contains an apostrophe (this should be the only time you would use double quotes instead of single quotes) or quotation marks
    - For **comments as reminders**, it is best to use -- and be consistent with it. Keep in mind that # isn’t recognized in all SQL dialects (MySQL doesn’t recognize #). When you add a comment to a query using --, the database query engine will ignore everything in the same line after --. It will continue to process the query starting on the next line.
    - Give your columns useful names, especially when using functions, and those names should never have spaces in them (spaces are bad in SQL names. Never use spaces.) The best practice is to use **snake_case**.
    - You can also use **CamelCase** capitalization when naming your table. CamelCase capitalization means that you capitalize the start of each word, like a two-humped (Bactrian) camel.
    - **indentation** doesn’t matter in SQL. But, as a general rule, you want to keep the length of each line in a query <= 100 characters. This makes your queries easy to read.
    - If you make **comments that take up multiple lines**, you can use -- for each line. Or, if you have more than two lines of comments, it might be cleaner and easier is to use /* to start the comment and */ to close the comment. The /* and */ method for multi-line comments usually looks cleaner and helps separate the comments from the query.
    - Keep in mind that not all SQL platforms provide native script editors to write SQL code. SQL text editors give you an interface where you can write your SQL queries in an easier and color-coded way. We can use text editors like [Sublime Text](https://www.sublimetext.com/) or [Atom](https://atom.io/).
    
    ## EFFECTIVELY ORGANIZE DATA
    
    ### Benefits of organizing data
    
    - makes it easier to find and use
    - helps you avoid making mistakes during your analysis
    - helps to protect it
    
    ### Best practices when organizing data
    
    It makes all my data more accessible and useful:
    
    - **Naming conventions (file naming):** consistent guidelines that describe the content, date, or version of a file in its name. You want to use logical and descriptive names for your files to make them easier to find and use. These file naming conventions help us organize, access, process, and analyze our data.
        - Work out and agree on file naming conventions early on in a project to avoid renaming files again and again.
        - Align your file naming with your team's or company's existing file-naming conventions.
        - Ensure that your file names are meaningful; consider including information like project name and anything else that will help you quickly identify (and use) the file for the right purpose.
        - keep your file name short and sweet. They're supposed to be quick reference points that tell you what's in a file.
        - Include the date and version number in file names; common formats are YYYYMMDD for dates and v## for versions (or revisions).
        - Lead revision numbers with 0, so that if you run into double digits of revisions, it's already built into your conventions.
        - Create a text file as a sample file with content that describes (breaks down) the file naming convention and a file name that applies it.
        - Avoid spaces and special characters in file names. Instead, use dashes, underscores, or capital letters. Spaces and special characters might not be recognized by your software. Plus avoiding spaces definitely makes it easier to work in SQL.
    - **Foldering:** Organizing your files into folders helps keep project-related files together in one place. You can also break folders down into subfolders
        - Create folders and subfolders in a logical hierarchy so related files are stored together.
        - Separate ongoing from completed work so your current project files are easier to find. Archive older files in a separate folder, or in an external storage location.
        - If your files aren't automatically backed up, manually back them up often to avoid losing important work.
    - Archiving older files: move old projects to a separate location to create an archive and cut down on clutter.
    
    there are two more things you'll want to consider when organizing data for work use (in addition to the previous three):
    
    - Align your naming and storage practices with your team to avoid any confusion.
    - develop metadata practices like creating a file that outlines project naming conventions for easy reference.
    - think about how often you're making copies of data and storing it in different places. If data is stored in lots of different databases or spreadsheets, it can contradict itself and lead to mistakes later on. Relational databases can help you avoid data duplication and store your data more efficiently.
    
    ## SECURING DATA
    
    Spreadsheets come with security features already built in. Security features can be designed to keep unauthorized users from viewing certain files, or just lock your worksheets so that you don't accidentally break your formulas. This is called data security.
    
    Data security means protecting data from unauthorized access or corruption by adopting safety measures. Usually the purpose of data security is to keep unauthorized users from accessing or viewing sensitive data.
    
    As a data analyst, you'll run into Google Sheets and Excel a lot, and both have some security features in common:
    
    - Both programs have features that let you protect your spreadsheets or parts of your spreadsheets from being edited, from the entire worksheet down to single cells in a table.
    - Both have access control features like password protection and user permissions.
    
    Because these programs are located in different places, these features are slightly different:
    
    | EXCEL | GOOGLE SHEET |
    | --- | --- |
    | you can encrypt files and worksheets with passwords before emailing them to other users. | these settings are found under the sharing menu, which allows you to control who can see or edit the sheet online. |
    
    ### Data security options
    
    - **Encryption:** Encryption uses a unique algorithm to alter data and make it unusable by users and applications that don’t know the algorithm. This algorithm is saved as a “key” which can be used to reverse the encryption; so if you have the key, you can still use the data in its original form.
    - **Tokenization:** Tokenization replaces the data elements you want to protect with randomly generated data referred to as a “token.” The original data is stored in a separate location and mapped to the tokens. To access the complete original data, the user or application needs to have permission to use the tokenized data and the token mapping. This means that even if the tokenized data is hacked, the original data is still safe and secure in a separate location.
    
    There are a lot of others, like using authentication devices for AI technology
    
    ## CREATE OR ENHANCE YOUR ONLINE PRESENCE
    
    A proffesional online presence can:
    
    - help potential employers find you.
    - make connections with other data analysts in your field
    - learn and share data findings
    - participate in community events.
    
    ### LinkedIn y Github
    
    | **LINKEDIN** | **GITHUB** |
    | --- | --- |
    | Make connections | Share insights and resources |
    | Follow industry trends | Read forums and wikis |
    | Find job opportunities | Manage team projects |
    | lets you connect with people and build a network. | Hosts community events |
    
    LinkedIn has become one of the standard professional social media sites, so it's a good starting place for building your online presence. GitHub offers a lot of really great tools for data analysts in the community.
    
    ## BUILD A DATA ANALYTICS NETWORK
    
    Once you've learned the skills and developed a strong portfolio, the next step is to connect with people in your profession or industry who can help you use those strengths to build a career.
    
    Networking can be called professional relationship building. It's all about meeting people both on and offline and building relationships with them. Networking is the most effective way to connect with fellow data analysts. When you’re networking, you can meet other professionals and participate in industry-related groups. How?:
    
    - Search for public meetups in your area. Just google data analytics meetups near you or search on [meetup.com](http://meetup.com/). Then you can learn more about different types of data analytics or share your interest with other people in the field.
    - Follow interesting companies or thought leaders on LinkedIn, Twitter, Facebook, and Instagram, interact with them, and share their content.
    - There's also a ton of blogs and online communities like O'Reilly, Kaggle, KDnuggets, GitHub and Medium, that can help you connect with peers and experts.
    
    | ONLINE CONNECTIONS | IN-PERSON GATHERINGS |
    | --- | --- |
    | Subscriptions to newsletters like Data Elixir. | Conferences usually present innovative ideas and topics. |
    | Hackathons (competitions) like those sponsored by Kaggle | Associations or societies gather members to promote a field like data science. |
    | Meetups | User communities and summits offer events for users of data analysis tools |
    | Platforms like LinkedIn and Twitter. | Non-profit organizations that promote the ethical use of data science and might offer events for the professional advancement of their members. |
    | Webinars may showcase a panel of speakers and are usually recorded for convenient access and playback. |  |
    
    ### Benefits of mentorship
    
    A mentor is a professional who shares their knowledge, skills, and experience to help you develop and grow. Mentors come in many forms. They can be trusted advisors, sounding boards, critics, resources or all of the above.
    
    For instance, websites like [Score.org](http://score.org/) and [MicroMentor.org](http://micromentor.org/) and an app called Mentorship allow you to look for specific credentials that match your needs.
    
    Now, while a mentor will help you gain critical skills and navigate challenges at work, a lot of people find that having a sponsor can take their career even further. A sponsor is a professional advocate who's committed to moving a sponsee's career forward with an organization. A mentor helps you skill up, a sponsor helps you move up.
    
    First, build and nurture your LinkedIn presence. Next, look at your current social media presence and make sure it's helping you put your best foot forward. Finally, always be open to connecting with peers and colleagues. You never know what great things a conversation will bring.
    
- 4 - **PROCESS** DATA FROM DIRTY TO CLEAN
    
    Clean data is the key to making sure your data has integrity before you analyze it. So the first step in processing data is learning about data integrity. Testing data is another important step to take when processing data.
    
    ## DATA INTEGRITY AND ANALYTICS OBJECTIVES
    
    A strong analysis depends on the integrity of the data. If the data you're using is compromised in any way, your analysis won't be as strong as it should be. Data integrity is the accuracy, completeness, consistency, and trustworthiness of data throughout its lifecycle. When data integrity is low, it can cause anything from the loss of a single pixel in an image to an incorrect medical decision.
    
    Data integrity can be compromised or threatened in lots of different ways:
    
    - **Data** **Replication:** Data replication is the process of storing data in multiple locations. If you're replicating data at different times in different places, there's a chance your data will be out of sync.
    - **Data transfer:** the process of copying data from a storage device to memory, or from one computer to another. If your data transfer is interrupted, you might end up with an incomplete data set, which might not be useful for your needs.
    - **Data manipulation:** the process of changing the data to make it more organized and easier to read. Data manipulation is meant to make the data analysis process more efficient, but an error during the process can compromise efficiency.
    - Human error
    - Viruses
    - Malware
    - Hacking
    - System failures
    
    In a lot of companies, the data warehouse or data engineering team takes care of ensuring data integrity. After you've found out what data you're working with, it's important to double-check that your data is complete and valid before analysis. This will help ensure that your analysis and eventual conclusions are accurate.
    
    ### Data constraints and examples
    
    | **Data constraint** | **Definition** | **Examples** |
    | --- | --- | --- |
    | **Data type** | Values must be of a certain type: date, number, percentage, Boolean, etc. | If the data type is a date, a single number like 30 would fail the constraint and be invalid |
    | **Data range** | Values must fall between predefined maximum and minimum values | If the data range is 10-20, a value of 30 would fail the constraint and be invalid |
    | **Mandatory** | Values can’t be left blank or empty | If age is mandatory, that value must be filled in |
    | **Unique** | Values can’t have a duplicate | Two people can’t have the same mobile phone number within the same service area |
    | **Regular expression (regex) patterns** | Values must match a prescribed pattern | A phone number must match ###-###-#### (no other characters allowed) |
    | **Cross-field validation** | Certain conditions for multiple fields must be satisfied | Values are percentages and values from multiple fields must add up to 100% |
    | **Primary-key** | (Databases only) value must be unique per column | A database table can’t have two rows with the same primary key value. A primary key is an identifier in a database that references a column in which each value is unique. More information about primary and foreign keys is provided later in the program. |
    | **Set-membership** | (Databases only) values for a column must come from a set of discrete values | Value for a column must be set to Yes, No, or Not Applicable |
    | **Foreign-key** | (Databases only) values for a column must be unique values coming from a column in another table | In a U.S. taxpayer database, the State column must be a valid state or territory with the set of acceptable values defined in a separate States table |
    | **Accuracy** | The degree to which the data conforms to the actual entity being measured or described | If values for zip codes are validated by street location, the accuracy of the data goes up. |
    | **Completeness** | The degree to which the data contains all desired components or measures | If data for personal profiles required hair and eye color, and both are collected, the data is complete. |
    | **Consistency** | The degree to which the data is repeatable from different points of entry or collection | If a customer has the same address in the sales and repair databases, the data is consistent. |
    
    ![Untitled](Google_Data_Analytics_images/Untitled%2035.png)
    
    ### Balancing objectives with data integrity
    
    It's important to check that the data you use aligns with the business objective. This adds another layer to the maintenance of data integrity because the data you're using might have limitations that you'll need to deal with. Before digging into any analysis, you need to consider a few limitations that might affect it
    
    - duplicate data
    - Not enough data
    
    ### Well-aligned objectives and data
    
    You can gain powerful insights and make accurate conclusions when data is well-aligned to business objectives. Allignment is something you will need to judge. Good alignment means that the data is relevant and can help you solve a business problem or determine a course of action to achieve a given business objective.
    
    **Clean data + alignment to business objective = accurate conclusions.** When there is clean data and good alignment, you can get accurate insights and make conclusions the data supports.
    
    **Alignment to business objective + additional data cleaning = accurate conclusions.** If there is good alignment but the data needs to be cleaned, clean the data before you perform your analysis.
    
    **Alignment to business objective + newly discovered variables + constraints = accurate conclusions.** If the data only partially aligns with an objective, think about how you could modify the objective, or use data constraints to make sure that the subset of data better aligns with the business objective.
    
    ## DEALING WITH INSUFFICIENT DATA
    
    Once you know your business objective, you'll be able to recognize whether you have enough data. And if you don't, you'll be able to deal with it before you start your analysis.
    
    ### Types of insufficient data
    
    - Data from only one source. If a limitation like this impacts your analysis, you can stop and go back to your stakeholders to figure out a plan.
    - Data that keeps udating. You might want to wait a month to gather data. Or you can check in with the stakeholders and ask about adjusting the objective. For example, you might analyze trends from week to week instead of month to month. You could also base your analysis on trends over the past three months and say, "Here's what attendance at the attraction for month four could look like.”
    - Outdated data. In this case, your best bet might be to find a new data set to work with.
    - Geographically-limited data.
    
    ### Ways to address insuficient data
    
    - Identify trends with tha available data
    - Wait for more data if time allows
    - Talk with stakeholders and adjust your objective
    - Look for a new dataset
    
    ### What to do when you find an issue with your data
    
    When you are getting ready for data analysis, you might realize you don’t have the data you need or you don’t have enough of it. In some cases, you can use what is known as proxy data in place of the real data. Think of it like substituting oil for butter in a recipe when you don’t have butter. In other cases, there is no reasonable substitute and your only option is to collect more data.
    
    - no data:
    
    | **Possible Solutions** | **Examples of solutions in real life** |
    | --- | --- |
    | Gather the data on a small scale to perform a preliminary analysis and then request additional time to complete the analysis after you have collected more data. | If you are surveying employees about what they think about a new performance and bonus plan, use a sample for a preliminary analysis. Then, ask for another 3 weeks to collect the data from all employees. |
    | If there isn’t time to collect data, perform the analysis using proxy data from other datasets. 
    *This is the most common workaround.* | If you are analyzing peak travel times for commuters but don’t have the data for a particular city, use the data from another city with a similar size and demographic. |
    - Too little data
    
    | **Possible Solutions** | **Examples of solutions in real life** |
    | --- | --- |
    | Do the analysis using proxy data along with actual data. | If you are analyzing trends for owners of golden retrievers, make your dataset larger by including the data from owners of labradors. |
    | Adjust your analysis to align with the data you already have. | If you are missing data for 18- to 24-year-olds, do the analysis but note the following limitation in your report: *this conclusion applies to adults 25 years and older* *only*. |
    - wrong data, including data with errors (sometimes data with errors can be a warning sign that the data isn’t reliable. Use your best judgment.)
    
    | **Possible Solutions** | **Examples of solutions in real life** |
    | --- | --- |
    | If you have the wrong data because requirements were misunderstood, communicate the requirements again. | If you need the data for female voters and received the data for male voters, restate your needs. |
    | Identify errors in the data and, if possible, correct them at the source by looking for a pattern in the errors. | If your data is in a spreadsheet and there is a conditional statement or boolean causing calculations to be wrong, change the conditional statement instead of just fixing the calculated values. |
    | If you can’t correct data errors yourself, you can ignore the 
    wrong data and go ahead with the analysis if your sample size is still large enough and ignoring the data won’t cause systematic bias. | If your dataset was translated from a different language and some of the translations don’t make sense, ignore the data with bad translation and go ahead with the analysis of the other data. |
    
    ![Untitled](Google_Data_Analytics_images/Untitled%2036.png)
    
    ### What to do when there is no data
    
    Sometimes the data to support a business objective isn’t readily available. This is when proxy data is useful. 
    
    Null most often means that a data field was unassigned (left empty), but sometimes Null can be interpreted as the value, 0. It is important to understand how Null was used before you start analyzing a dataset with Null data.
    
    ### The importance of sample size
    
    The goal is to get enough information from a small group within a population to make predictions or conclusions about the whole population. The sample size helps ensure the degree to which you can be confident that your conclusions accurately represent the population. Using a sample for analysis is more cost-effective and takes less time.
    
    But when you only use a small sample of a population, it can lead to uncertainty. You can't really be 100 percent sure that your statistics are a complete and accurate representation of the population. This leads to sampling bias, but using random sampling can help address some of those issues with sampling bias.
    
    Random sampling is a way of selecting a sample from a population so that every possible type of the sample has an equal chance of being chosen.
    
    Companies usually create sample sizes before data analysis so analysts know that the resulting dataset is representative of a population.
    
    ### Things to remember when determining the size of your sample
    
    - Don’t use a sample size less than 30. It has been statistically proven that 30 is the smallest sample size where an average result of a sample starts to represent the average result of a population. This recommendation is based on the Central Limit Theorem (CLT) in the field of probability and statistics. As sample size increases, the results more closely resemble the normal (bell-shaped) distribution from a large number of samples.
    - The confidence level most commonly used is 95%, but 90% can work in some cases.
    - For a **higher** confidence level, use a larger sample size
    - To **decrease** the margin of error, use a larger sample size
    - For **greater** statistical significance, use a larger sample size
    - Sample size will vary based on the type of business problem you are trying to solve, and It depends on the stakes.
    - Larger sample sizes have a higher cost
    - Knowing the basics will help you make the right choices when it comes to sample size. Sample size calculators let you enter a desired confidence level and margin of error for a given population size. They then calculate the sample size needed to statistically achieve those results.
    
    ## TESTING YOUR DATA
    
    Statistical power is the probability of getting meaningful results from a test. For data analysts, your projects might begin with the test or study. Hypothesis testing is a way to see if a survey or experiment has meaningful results. Usually, the larger the sample size, the greater the chance you'll have statistically significant results with your test. And that's statistical power.
    
    Statistical power is usually shown as a value out of one. So if your statistical power is 0.6, that's the same thing as saying 60%.
    
    If a test is statistically significant, it means the results of the test are real and not an error caused by random chance. Usually, you need a statistical power of at least 0.8 or 80% to consider your results statistically significant.
    
    [A Gentle Introduction to Statistical Power and Power Analysis in Python]([https://machinelearningmastery.com/statistical-power-and-power-analysis-in-python/](https://machinelearningmastery.com/statistical-power-and-power-analysis-in-python/))
    
    "**Statistical power** can be calculated and reported for a completed experiment to comment on the confidence one might have in the conclusions drawn from the results of the study. It can also be used as a tool to estimate the number of observations or sample size required in order to detect an effect in an experiment."
    
    ### Determine the best sample size
    
    It can be both expensive and time-consuming to analyze an entire population of data. Using sample size usually makes the most sense and can still lead to valid and useful findings. There are handy calculators online that can help you find sample size. You need to input the:
    
    - **confidence level:** the probability that your sample accurately reflects the greater population. It's how strongly you feel that you can rely on something or someone. Having a 99 percent confidence level is ideal. But most industries hope for at least a 90 or 95 percent confidence level. Industries like pharmaceuticals usually want a confidence level that's as high as possible when they are using a sample size.
    - **population size.**
    - **margin of error:** tells you how close your sample size results are to what your results would be if you use the entire population that your sample size represents.
    
    Check Sample size calculator excel at google drive.
    
    The confidence level and margin of error don't have to add up to 100 percent. They're independent of each other.
    
    ### Sample size calculator
    
    A sample size calculator tells you how many people you need to interview (or things you need to test) to get results that represent the target population. When you use a sample size calculator you will come across with some terms:
    
    - **Confidence level**: The probability that your sample size accurately reflects the greater population.
    - **Margin of error**: The maximum amount that the sample results are expected to differ from those of the actual population.
    - **Population**: This is the total number you hope to pull your sample from.
    - **Sample**: A part of a population that is representative of the population.
    - **Estimated response rate**: If you are running a survey of individuals, this is the percentage of people you expect will complete your survey out of those who received the survey.
    
    In order to use a sample size calculator, you need to have the population size, confidence level, and the acceptable margin of error already decided so you can input them into the tool.
    
    You can use the next sample size calculators:
    
    - [surveymonkey]([https://www.surveymonkey.com/mp/sample-size-calculator/](https://www.surveymonkey.com/mp/sample-size-calculator/))
    - [Raosoft]([http://www.raosoft.com/samplesize.html](http://www.raosoft.com/samplesize.html))
    
    The calculated sample size is the minimum number to achieve what you input for confidence level and margin of error.
    
    ## MARGIN OF ERROR
    
    Margin of error is the maximum that the sample results are expected to differ from those of the actual population. It defines a range of values below and above the average result for the sample. The average result for the entire population is expected to be within that range.
    
    Margin of error helps you understand how reliable the data from your hypothesis testing is. The closer to zero the margin of error, the closer your results from your sample would match results from the overall population.
    
    If you've already been given the sample size, you can calculate the margin of error yourself. Then you can decide yourself how much of a chance your results have of being statistically significant based on your margin of error. In general, the more people you include in your survey, the more likely your sample is representative of the entire population.
    
    to calculate margin of error, you need three things: 
    
    - population size
    - sample size
    - confidence level
    
    Check Margin of error calculator at google Drive
    
    Calculators are just one of the many tools you can use to ensure data integrity. And it's also good to remember that checking for data integrity and aligning the data with your objectives will put you in good shape to complete your analysis.
    
    In most cases, a 90% or 95% confidence level is used. But, depending on your industry, you might want to set a stricter confidence level. A 99% confidence level is reasonable in some industries, such as the pharmaceutical industry.
    
    ### Marging of error in marketing
    
    The margin of error is also important in marketing. Let’s use A/B testing as an example. **A/B testing** (or split testing) tests two variations of the same web page to determine which page is more successful in attracting user traffic and generating revenue. User traffic that gets monetized is known as the **conversion rate**. A/B testing allows marketers to test emails, ads, and landing pages to find the data behind what is working and what isn’t working. Marketers use the **confidence interval** (determined by the conversion rate and the margin of error) to understand the results. Examining the margin of error is important when making conclusions based on your test results.
    
    ## DATA CLEANING IS A MUST
    
    According to IBM, the yearly cost of poor-quality data is $3.1 trillion in the US alone. The #1 cause of poor-quality data is human error.
    
    Dirty data can be the result of someone typing in a piece of data incorrectly, inconsistent formatting, blank fields; or the same piece of data being entered more than once, which creates duplicates.
    
    Dirty data is data that's incomplete, incorrect, or irrelevant to the problem you're trying to solve.
    
    Clean data is data that's complete, correct, and relevant to the problem you're trying to solve.
    
    ### Why data cleaning is important
    
    Clean data is incredibly important for effective analysis. If a piece of data is entered into a spreadsheet or database incorrectly, or if it's repeated, or if a field is left blank, or if data formats are inconsistent, the result is dirty data. Small mistakes can lead to big consequences in the long run.
    
    Let's talk about some people you'll work with as a data analyst:
    
    - Data engineers: transform data into a useful format for analysis and give it a reliable infrastructure. This means they develop, maintain, and test databases, data processors and related systems.
    - Data warehousing specialist: develop processes and procedures to effectively store and organize data. They make sure that data is available, secure, and backed up to prevent loss.
    
    If data passes through the hands of a data engineer or a data warehousing specialist first, you know you're off to a good start on your project.
    
    But data cleaning becomes even more important when working with external data, especially if it comes from multiple sources.
    
    ### Types of dirty data
    
    - Duplicate data: Any data record that shows up more than once. The possible causes can be:
        - Manual data entry
        - batch data imports
        - Data migration
    - Outdated data: Any data that is old which should be replaced with newer and more accurate information. The possible causes can be:
        - People changing roles or companies
        - software and systems becoming obsolete
    - Incomplete data: Any data that is missing important fields. The possible causes can be:
        - Improper data collection
        - incorrect data entry
    - Incorrect/inaccurate data: Any data that is complete but inaccurate. The possible causes can be:
        - Human error inserted during data input
        - fake information
        - mock data
    - Inconsistent data: Any data that uses different formats to represent the same thing. The possible causes can be:
        - Data stored incorrectly
        - errors inserted during data transfer
    - Labeling.
    - having an inconsistent field length. Field length is a tool for determining how many characters can be keyed into a field. Assigning a certain length to the fields in your spreadsheet is a great way to avoid errors.
    
    | TYPE | POTENTIAL HARM TO BUSINESSES |
    | --- | --- |
    | Duplicated data | Skewed metrics or analyses, inflated or inaccurate counts or predictions, or confusion during data retrieval |
    | Outdated data | Inaccurate insights, decision-making, and analytics |
    | Incomplete data | Decreased productivity, inaccurate insights, or inability to complete essential services |
    | Incorrect/inaccurate data | Inaccurate insights or decision-making based on bad information resulting in revenue loss |
    | Inconsistent data | Contradictory data points leading to confusion or inability to classify or segment customers |
    
    ## CLEANING DATA
    
    Clean data depends largely on the data integrity rules that an organization follows, such as spelling and punctuation guidelines.
    
    The techniques for data cleaning will be different depending on the specific data set you're working with. We can clean the data by:
    
    1. removing unwanted data:
        1. It's always a good practice to make a copy of the data set before.
        2. Once that's done, then you can move on to getting rid of the duplicates or data that isn't relevant to the problem you're trying to solve. Typically, duplicates appear when you're combining data sets from more than one source or using data from multiple departments within the same business.
        3. Irrelevant data, which is data that doesn't fit the specific problem that you're trying to solve, also needs to be removed.
        4. Removing irrelevant data takes a little more time and effort because you have to figure out the difference between the data you need and the data you don't.
    2. cleaning up text to remove extra spaces and blanks. Extra spaces can cause unexpected results when you sort, filter, or search through your data.
    3. fix typos. You can use spreadsheet tools, such as spellcheck, autocorrect, and conditional formatting to fix those problems.
        1. Fixing misspellings
        2. Inconsistent capitalization
        3. incorrect punctuation and other typos.
    4. make formatting consistent. This is particularly important when you get data from lots of different sources. Every database has its own formatting, which can cause the data to seem inconsistent.
    
    ### Cleaning data from multiple sources
    
    Cleaning data that comes from two or more sources is very common for data analysts, but it does come with some interesting challenges like:
    
    - Merge. All the data from each organization would need to be combined using data merging. Data merging is the process of combining two or more datasets into a single dataset. When merging datasets, We should begin by asking myself some key questions to help me avoid redundancy and to confirm that the datasets are compatible:
        - do I have all the data I need?
        - does the data I need exist within these datasets?
        - Do the datasets need to be cleaned, or are they ready for me to use?
        - are the datasets cleaned to the same standard? For example, what fields are regularly repeated? How are missing values handled? How recently was the data updated?
        
    
    ### Common data-cleaning pitfalls
    
    Some of the errors you might come across while cleaning your data could include:
    
    - Not checking for spelling errors. if you don’t double-check for spelling errors and catch this, your analysis will have mistakes in it.
    - Forgetting to document errors. Documenting your errors can be a big time saver, as it helps you avoid those errors in the future by showing you how you resolved them. It also helps you keep track of changes in your work, so that you can backtrack if a fix didn’t work.
    - Not checking for misfielded values. A misfielded value happens when the values are entered into the wrong field. These values might still be formatted correctly, which makes them harder to catch if you aren’t careful.
    - Overlooking missing values. Missing values in your dataset can create errors and give you inaccurate conclusions. As a best practice, try to keep your data as clean as possible by maintaining completeness and consistency.
    - Looking at a subset of data and not the whole picture. It is important to think about all of the relevant data when you are cleaning. This helps make sure you understand the whole story the data is telling, and that you are paying attention to all possible errors. If you want to avoid common errors like duplicates, each field of your data requires equal attention.
    - Losing track of the business objectives. Being curious is great! But try not to let it distract you from the task at hand.
    - Not fixing the source of the error. Addressing the source of the errors in your data will save you a lot of time in the long run.
    - Not analyzing the system prior to data cleaning. First, you figure out where the errors come from. Then, once you understand where bad data comes from, you can control it and keep your data clean.
    - Not backing up your data prior to data cleansing. The simple procedure of backing up your data can save you hours of work-- and most importantly, a headache.
    - Not accounting for data cleaning in your deadlines/process. When you set aside time for data cleaning, it helps you get a more accurate estimate for ETAs for stakeholders and can help you know when to request an adjusted ETA.
    
    ### Techniques to clean the data
    
    - Select and remove blank cells using filters
    - Transpose the data. It will help you convert the data from the current long format (more rows than columns) to the wide format (more columns than rows). This action is called transposing. You can do it by selecting the data, copying it and Paste Special from the right-click menu. Select the Transposed option.
    - Get rid of extra spaces in cells with string data. In Excel, you can use the TRIM command to get rid of white spaces
    - Change Text Lower/Uppercase/Proper Case. Just Click on the Add-Ons tab and select ChangeCase. Select the option All uppercase. Notice the other options that you could have chosen if needed.
    - Delete all formatting. Select the data and click the Format tab and select the Clear Formatting option.
    
    ### Data-cleaning features in spreadsheets
    
    There's a lot of great efficiency tools that data analysts use all the time, such as:
    
    - conditional formatting: a spreadsheet tool that changes how cells appear when values meet specific conditions.
    - removing duplicates. "Remove duplicates" is a tool that automatically searches for and eliminates duplicate entries from a spreadsheet. Go to data > Data deletion > remove duplicates. Choose "Data has header row”. This removes duplicated rows
    - formatting dates. Use a standard date format to make all of our dates consistent
    - fixing text strings and substrings. A text string is a group of characters within a cell, most often composed of letters. An important characteristic of a text string is its length. A substring is a smaller subset of a text string.
    - splitting text to columns. Split is a tool that divides a text string around the specified character and puts each fragment into a new and separate cell. Split is helpful when you have more than one piece of data in a cell and you want to separate them out. To do it Highlight the column, then select "Data," and "Split text to columns.” Split text to columns is also helpful for fixing instances of numbers stored as text.
        - Split text to columns is also helpful for fixing instances of numbers stored as text. Sometimes values in your spreadsheet will seem like numbers, but they're formatted as text. This can happen when copying and pasting from one place to another or if the formatting's wrong.
    
    ### Optimize the data-cleaning process (with functions)
    
    Every function has a certain syntax that needs to be followed for it to work.
    
    - COUNTIF: a function that returns the number of cells that match a specified value. Basically, it counts the number of times a value appears in a range of cells. We'll use COUNTIF to check for some common problems, like negative numbers or a value that's much less or much greater than expected.
    - LEN: a function that tells you the length of the text string by counting the number of characters it contains. This is useful when cleaning data if you have a certain piece of information in your spreadsheet that you know must contain a certain length.
    - LEFT: a function that gives you a set number of characters from the left side of a text string.
    - RIGHT: a function that gives you a set number of characters from the right side of a text string.
    - MID: a function that gives you a segment from the middle of a text string.
    - CONCATENATE: a function that joins together two or more text strings.
    - TRIM: a function that removes leading, trailing, and repeated spaces in data. Sometimes when you import data, your cells have extra spaces, which can get in the way of your analysis. TRIM fixed the extra spaces.
    - SPLIT: The SPLIT function divides text around a specified character or string, and puts each fragment of text into a separate cell in the row.
    
    ### Workflow automation
    
    Workflow automation is the process of automating parts of your work. That could mean creating an event trigger that sends a notification when a system is updated. Or it could mean automating parts of the data cleaning process.
    
    Automating different parts of your work can save you tons of time, increase productivity, and give you more bandwidth to focus on other important aspects of the job.
    
    | **Task** | **Can it be automated?** | **Why?** |
    | --- | --- | --- |
    | Communicating with your team and stakeholders | No | Communication is key to understanding the needs of your team and stakeholders as you complete the tasks you are working on. There is no replacement for person-to-person communications. |
    | Presenting your findings | No | Presenting your data is a big part of your job as a data analyst. Making data accessible and understandable to stakeholders and creating data visualizations can’t be automated for the same reasons that communications can’t be automated. |
    | Preparing and cleaning data | Partially | Some tasks in data preparation and cleaning can be automated by setting up specific processes, like using a programming script to automatically detect missing values. |
    | Data exploration | Partially | Sometimes the best way to understand data is to see it. Luckily, there are plenty of tools available that can help automate the process of visualizing data. These tools can speed up the process of visualizing and understanding the data, but the exploration itself still needs to be done by a data analyst. |
    | Modeling the data | Yes | Data modeling is a difficult process that involves lots of different factors; luckily there are tools that can completely automate the different stages. |
    
    One of the most important ways you can streamline your data cleaning is to clean data where it lives. This will benefit your whole team, and it also means you don’t have to repeat the process over and over.
    
    ### Different data perspectives
    
    - sorting and filtering: sorting and filtering data helps data analysts customize and organize the information the way they need for a particular project. But these tools are also very useful for data cleaning. For data cleaning you can use sorting to:
        - put things in alphabetical or numerical order, so you can easily find a piece of data.
        - bring duplicate entries closer together for faster identification.
        
        Filters are very useful in data cleaning when you want to find a particular piece of information. When cleaning data, you might use a filter to:
        
        - find values above a certain number
        - Find even or odd values.
    - Pivot table: In data cleaning, pivot tables are used to give you a quick, clutter- free view of your data. You can choose to look at the specific parts of the data set that you need to get a visual in the form of a pivot table.
    - VLOOKUP stands for vertical lookup. It's a function that searches for a certain value in a column to return a corresponding piece of information. When data analysts look up information for a project, it's rare for all of the data they need to be in the same place. Usually, you'll have to search across multiple sheets or even different databases.
        
        ![Untitled](Google_Data_Analytics_images/Untitled%2037.png)
        
        - To put it simply, VLOOKUP searches for the value in the first argument in the leftmost column of the specified location. Then the value of the third argument tells VLOOKUP to return the value in the same row from the specified column.
        - False at the end of the syntax means that an exact match is what we're looking for.
    - Plotting: When you plot data, you put it in a graph chart, table, or other visual to help you quickly find what it looks like. Plotting is very useful when trying to identify any skewed data or outliers.
    
    ### Even more data-cleaning techniques
    
    Knowing how to fix specific problems, either manually with spreadsheet tools, or with functions, is extremely valuable. But it's also important to think about how your data has moved between systems and how it's evolved along it's journey to your data analysis project. To do this, data analysts use something called data mapping.
    
    Data mapping is the process of matching fields from one database to another. This is very important to the success of data migration, data integration, and lots of other data management activities.
    
    The first step to data mapping is identifying what data needs to be moved. This includes the tables and the fields within them. We also need to define the desired format for the data once it reaches its destination.
    
    Depending on the schema and number of primary and foreign keys in a data source, data mapping can be simple or very complex.
    
    When selecting a software program to map your data, you want to be sure that it supports the file types you're working with, such as Excel, SQL, Tableau, and others.
    
    - Mapping data (manually):
        1. we need to determine the content of each section to make sure the data ends up in the right place. This step makes sure that each piece of information ends up in the most appropriate place in the merged data source.
        2. transforming the data into a consistent format. This is a great time to use concatenate.
        3. Now that everything's compatible, it's time to transfer the data to its destination. There's a lot of different ways to move data from one place to another, including querying, import wizards, and even simple drag and drop.
        4. testing phase of data mapping. For this, you inspect a sample piece of data to confirm that it's clean and properly formatted. It's also a smart practice to do spot checks on things such as the number of nulls. For the test, you can use a lot of the data cleaning tools we discussed previously, such as data validation, conditional formatting, COUNTIF, sorting, and filtering. Here you can ask yourself:
            1. Was all the data merged?
            2. Was data migrated correctly?
            3. Are the data formats in the merged spreadsheet consistent?
        
        Data mapping is so important because even one mistake when merging data can ripple throughout an organization, causing the same error to appear again and again. This leads to poor results.
        
    
    ## USING SQL TO CLEAN DATA
    
    SQL is a core skill and highly sought after by everybody. SQL is the primary way data analysts extract data from databases.
    
    SQL is a structured query language that analysts use to work with databases. Data analysts usually use SQL to deal with large datasets because it can handle huge amounts of data. And I mean trillions of rows. 
    
    Development on SQL actually began in the early 70s. In 1970, Edgar F.Codd developed the theory about relational databases. In 1979, after extensive testing SQL, now just spelled S-Q-L, was released publicly. By 1986, SQL had become the standard language for relational database communication, and it still is. This is another reason why data analysts choose SQL. It's a well-known standard within the community.
    
    ### Spreadsheets functions and formulas or SQL queries?
    
    When it comes down to it, where the data lives will decide which tool you use. If you are working with data that is already in a spreadsheet, that is most likely where you will perform your analysis. And if you are working with data stored in a database, SQL will be the best tool for you to use for your analysis.
    
    | **Features of Spreadsheets** | **Features of SQL Databases** |
    | --- | --- |
    | Generated with a program like Excel or google sheets | A language used to interact with database programs like Oracle MySQL or Microsoft SQL Server |
    | Smaller data sets; or when you are working independently | Larger datasets, like more than a million rows. Tracks changes across team |
    | Enter data manually. Access to the data you input | Access tables across a database. Can pull information from different sources in the database |
    | Stored locally | Stored across a database |
    | Create graphs and visualizations in the same program | Prepare data for further analysis in another software |
    | Built-in spell check and other useful functions | Fast and powerful functionality |
    | Best when working solo on a project | Great for collaborative work and tracking queries run by all users |
    
    Spreadsheets and SQL have some things in common, like there're tools you can use in both spreadsheets and SQL to achieve similar results. For example, you can still perform arithmetic, use formulas and join data when you're using SQL
    
    As a junior data analyst, it is important to know that there are slight differences between dialects. But by mastering Standard SQL, which is the dialect you will be working with in this program, you will be prepared to use SQL in any database.
    
    ### Processing time with SQL
    
    Data is measured by the number of bits it takes to represent it. All information in a computer can be represented as a binary number consisting solely of 0’s and 1’s. Each 0 or 1 in a number is a bit. A bit is the smallest unit of storage in computers. Since computers work in binary (Base 2), this means that all the important numbers that differentiate between different data sizes will be powers of 2. A byte is a collection of 8 bits.
    
    BigQuery caches the query results to avoid extra work if the query needs to be rerun.
    
    ### Widely used SQL queries
    
    Queries can help you do a lot of things, but there are some common ones that data analysts use all the time:
    
    - SELECT: We can use SELECT to specify exactly what data we want to interact with in a table. If we combine SELECT with FROM, we can pull data from any table in this database as long as they know what the columns and rows are named.
        
        ```sql
        SELECT *column(s)_name*
        FROM *personal_project_name*.*database_name.table_name*
        ```
        
    - INSERT INTO: used to insert information to the table
        
        ```sql
        INSERT INTO *database_name.table_name*
        (*column1,...,columnN*)
        VALUES
        (*value1,...,valueN*)
        ```
        
    - UPDATE: update information to a row
        
        ```sql
        UPDATE *database_name.table_name*
        SET *columnA* = '*value*'
        WHERE *columnB = 'condition'*
        ```
        
    - CREATE TABLE IF NOT EXISTS: Create a new table for the database
        - Keep in mind, just running a SQL query doesn't actually create a table for the data we extract. It just stores it in our local memory. To save it, we'll need to download it as a spreadsheet or save the result into a new table.
    - if you're creating lots of tables within a database, you'll want to use the DROP TABLE IF EXISTS statement to clean up after yourself.
    - DISTINCT: we can remove duplicates including DISTINCT in our SELECT statement.
    
    ### Cleaning string variables using SQL
    
    - LENGTH: we can use LENGTH to double-check that our string variables are consistent. For some databases, this query is written as LEN, but it does the same thing.
        
        ```sql
        SELECT 
        	LENGTH (*column_name*) AS *new_column_name*
        FROM *personal_project_name*.*database_name.table_name*
        ```
        
    - SUBSTRING(): function extracts some characters from a string.
        
        ```sql
        SUBSTRING(*string_or_column, start, length*)
        ```
        
    - TRIM: This is really useful if you find entries with extra spaces and need to eliminate those extra spaces for consistency. To use the TRIM function, we tell SQL the column we want to remove spaces from
        
        ```sql
        TRIM(*column_we_want_to_remove_spaces_from*) = '*text*'
        ```
        
    
    ### Advanced data cleaning functions
    
    - CAST: When you import data that doesn't already exist in your SQL tables, the datatypes from the new dataset might not have been imported correctly. This is where the CAST function comes in handy. CAST can be used to convert anything from one data type to another.
        
        ```sql
        SELECT
        	CAST(*column_name* AS *datatype*)
        FROM *personal_project_name*.*database_name.table_name*
        ORDER BY
        	**CAST(*column_name* AS *datatype*)
        ```
        
    - CONCAT(): lets you add strings together to create new text strings that can be used as unique keys.
    - COALESCE(): Can be used to return non-null values in a list
        
        ```sql
        SELECT
        	COALESCE(*column_name,column_replacement_if_value_is_null*) AS *new_column_name*
        FROM *personal_project_name*.*database_name.table_name
        ...*
        ```
        
        COALESCE can save you time when you're making calculations too by skipping any null values and keeping your math correct.
        
    
    ## MANUALLY CLEANING DATA
    
    ### Verifying and reporting results
    
    Verification is a process to confirm that a data cleaning effort was well- executed and the resulting data is accurate and reliable. It involves rechecking your clean dataset, doing some manual clean ups if needed, and taking a moment to sit back and really think about the original purpose of the project. That way, you can be confident that the data you collected is credible and appropriate for your purposes.
    
    Making sure your data is properly verified is important because:
    
    - it allows you to double-check that the work you did to clean up your data was thorough and accurate.
    - lets you catch mistakes before you begin analysis.
    - Without it, any insights you gain from analysis can't be trusted for decision-making.
    
    The other big part of the verification process is reporting on your efforts. Reports are a super effective way to show your team that you're being 100 percent transparent about your data cleaning. Reporting is also a great opportunity to show stakeholders that you're accountable, build trust with your team, and make sure you're all on the same page of important project details.
    
    A changelog is a file containing a chronologically ordered list of modifications made to a project. It's usually organized by version and includes the date followed by a list of added, improved, and removed features. Changelogs are very useful for keeping track of how a dataset evolved over the course of a project.
    
    ### Steps in the verification process
    
    1. Going back to your original unclean data set and comparing it to what you have now. Review the dirty data and try to identify any common problems.
    2. clean up the problems manually. For example, by eliminating extra spaces or removing an unwanted quotation mark. But there's also some great tools for fixing common errors automatically, such as TRIM and remove duplicates. Now sometimes you had an error that shows up repeatedly, and it can't be resolved with a quick manual edit or a tool that fixes the problem automatically. In these cases, it's helpful to create a pivot table. Pivot tables sort, reorganize, group, count, total or average data stored in a database.
    3. Taking a big-picture view of your project. This is an opportunity to confirm you're actually focusing on the business problem that you need to solve and the overall project goals and to make sure that your data is actually capable of solving that problem and achieving those goals. Taking a big picture view of your project involves doing three things:
        1. consider the business problem you're trying to solve with the data.
        2. consider the goal of the project.
        3. consider whether your data is capable of solving the problem and meeting the project objectives. That means thinking about where the data came from and testing your data collection and cleaning processes.
        
        ask yourself, do the numbers make sense?
        
    
    ### Data-cleaning verification: A checklist
    
    - Correct the most common problems:
        - **Sources of errors**: Did you use the right tools and functions to find the source of the errors in your dataset?
        - **Null data**: Did you search for NULLs using conditional formatting and filters?
        - **Misspelled words**: Did you locate all misspellings?
        - **Mistyped numbers**: Did you double-check that your numeric data has been entered correctly?
        - **Extra spaces and characters**: Did you remove any extra spaces or characters using the **TRIM** function?
        - **Duplicates**: Did you remove duplicates in spreadsheets using the **Remove Duplicates** function or **DISTINCT** in SQL?
        - **Mismatched data types**: Did you check that numeric, date, and string data are typecast correctly?
        - **Messy (inconsistent) strings**: Did you make sure that all of your strings are consistent and meaningful?
        - **Messy (inconsistent) date formats**: Did you format the dates consistently throughout your dataset?
        - **Misleading variable labels (columns)**: Did you name your columns meaningfully?
        - **Truncated data:** Did you check for truncated or missing data that needs correction?
        - **Business Logic**: Did you check that the data makes sense given your knowledge of the business?
    - Review the goal of your project: Once you have finished these data cleaning tasks, it is a good idea to review the goal of your project and confirm that your data is still aligned with that goal. This is a continuous process that you will do throughout your project-- but here are three steps you can keep in mind while thinking about this:
        - Confirm the business problem
        - Confirm the goal of the project
        - Verify that data can solve the problem and is aligned to the goal
        
        ![Untitled](Google_Data_Analytics_images/Untitled%2038.png)
        
    
    ## DOCUMENTING RESULTS AND THE CLEANING PROCESS
    
    When you clean your data, all the incorrect or outdated information is gone, leaving you with the highest-quality content. But all those changes you made to the data are valuable too. Documentation is the process of tracking changes, additions, deletions, and errors involved in your data cleaning effort.
    
    Having a record of how a data set evolved does three very important things:
    
    1. it lets us recover data-cleaning errors. It's also a good idea to create a clean table rather than overriding your existing table. This way, you still have the original data in case you need to redo the cleaning.
    2. documentation gives you a way to inform other users of changes you've made.
    3. documentation helps you to determine the quality of the data to be used in analysis.
    
    The first two benefits assume the errors aren't fixable. But if they are, a record gives the data engineer more information to refer to.
    
    Data analysts usually use a changelog to access this information. You can use and view a changelog in spreadsheets and SQL to achieve similar results:
    
    - Spreadsheet: We can use Sheet's version history, which provides a real-time tracker of all the changes and who made them from individual cells to the entire worksheet. To find this feature:
        1. click the File tab >  Version history > see version history
        2. In the right panel, choose an earlier version.
        3. We can find who edited the file and the changes they made in the column next to their name.
        4. To return to the current version, go to the top left and click "Back." If you want to check out changes in a specific cell, we can right-click and select Show Edit History.
        5. if you want others to be able to browse a sheet's version history, you'll need to assign permission.
    - SQL: The way you create and view a changelog with SQL depends on the software program you're using. Essentially, all you have to do is specify exactly what you did and why when you commit a query to the repository as a new and improved query.
        
        Another option is to just add comments as you go while you're cleaning data in SQL. This will help you construct your changelog after the fact.
        
        There is an option called “Query history” which tracks all the queries you’ve run. You can click on any of them to revert back to a previous version of your query or to bring up an older version to find what you've changed.
        
    
    But there's another way to keep the communication flowing, and that's reporting.
    
    ### Embrace changelogs
    
    Data analysts use changelogs to keep track of data transformation and cleaning. It is common to keep changelogs as a readme file in a code repository. Here are some examples of these:
    
    ![Untitled](Google_Data_Analytics_images/Untitled%2039.png)
    
    ![Untitled](Google_Data_Analytics_images/Untitled%2040.png)
    
    Most software applications have a kind of history tracking built in. For example, in Google sheets, you can check the version history of an entire sheet or an individual cell and go back to an earlier version. In Microsoft Excel, you can use a feature called Track Changes. And in BigQuery, you can view the history to check what has changed.
    
    | Google Sheets | 1. Right-click the cell and select **Show edit history**.
    2. Click the left-arrow < or right arrow > to move backward and forward in the history as needed. |
    | --- | --- |
    | Microsoft Excel | 1. If Track Changes has been enabled for the spreadsheet: click **Review**. ****
    2. Under **Track Changes**, click the **Accept/Reject Changes** option to accept or reject any change made. |
    | BigQuery | Bring up a previous version (without reverting to it) and figure out what changed by comparing it to the current version. |
    
    Beneffits of changelogs:
    
    - are super useful for helping us understand the reasons changes have been made
    - have no set format and you can even make your entries in a blank document. But if you are using a shared changelog, it is best to agree with other data analysts on the format of all your log entries.
    - By following up, you would ensure data integrity outside your project. You would also be showing personal integrity as someone who can be trusted with data.
    - a changelog is important for when lots of changes to a spreadsheet or query have been made.
    
    Typically, a changelog records this type of information:
    
    - Data, file, formula, query, or any other component that changed
    - Description of what changed
    - Date of the change
    - The person who made the change
    - The person who approved the change
    - Version number
    - Reason for the change
    
    ### Best practices for changelogs
    
    - Changelogs are for humans, not machines, so write legibly.
    - Every version should have its own entry.
    - Each change should have its own line.
    - Group the same types of changes. Types of changes usually fall into one of the following categories:
        - Added: new features introduced
        - Changed: changes in existing functionality
        - Deprecated: features about to be removed
        - Removed: features that have been removed
        - Fixed: bug fixes
        - Security: lowering vulnerabilities
    - Versions should be ordered chronologically starting with the latest.
    - The release date of each version should be noted.
    
    Consider what changes you need to record in a changelog. To start, you record the various changes, additions, and fixes that were discussed above. Arrange them using bullets or numbering with one change per line. Group similar changes together with a label describing the change immediately above them.
    
    Use different version numbers for each milestone reached in your project. Within each version, place the logged changes that were made since the previous version (milestone). Dates are not generally necessary for each change, but they are recommended for each version.
    
    ### Why documentation is important
    
    Since changelog is staged chronologically, it provides a real-time account of every modification.
    
    There're plenty of ways we could go about documenting what we did. One common way is to just create a doc listing out the steps we took and the impact they had.
    
    This helps build our credibility as witnesses who can be trusted to present all the evidence accurately during testimony.
    
    ### Feedback and cleaning
    
    By now it's safe to say that verifying, documenting and reporting are valuable steps in the data-cleaning process. The next step is getting feedback about the evidence and using it for good.
    
    The feedback we get when we report on our cleaning can transform data collection processes, and ultimately business development.
    
    One of the biggest challenges of working with data is dealing with errors. Some of the most common errors involve:
    
    - human mistakes like mistyping or misspelling
    - flawed processes like poor design of a survey form
    - system issues where older systems integrate data incorrectly.
    
    With consistent documentation and reporting, we can uncover error patterns in data collection and entry procedures and use the feedback we get to make sure common errors aren't repeated.
    
    In more extreme cases, the feedback we get can even send us back to the drawing board to rethink expectations and possibly update quality control procedures.
    
    ### Advanced functions for speedy data cleaning
    
    ![Untitled](Google_Data_Analytics_images/Untitled%2041.png)
    
    ## THE DATA ANALYST HIRING PROCESS
    
    The most common way to start is by checking out available jobs. There's a lot of job sites that are built specifically for people seeking employment. You can also go to company websites where they usually post job listings too.
    
    Once you find a few that you like, do some research to learn more about the companies and the details about the specific positions you'll be applying for.
    
    Then you can update your resume or create a new one. You'll want it to be specific and reflect what each company is looking for. But you can definitely have a master resume that you tweak for each position. It can also help to create a spreadsheet with all of your experiences and accomplishments to help you decide what to include in your resume for each possition.
    
    One of the most challenging part of a job search ir hearing the word "no.” The key is to stay focused. Don't get discouraged, and above all else believe in yourself.
    
    A recruiter might also reach out to you based on their own research. So when you talk with the recruiter, whether on the phone, online or in person, be professional and personable. it can help to refer back to your resume to wow them with your knowledge of the data analytics industry. And remember, recruiters are also looking for someone and they're hoping it'll be you.
    
    - Using technical terms like "SQL" and "clean data" will show recruiters that you know what you're doing.
    - they want to see that you know what you're talking about. They might also give you prep materials or other recommendations.
    
    Next up is usually the hiring manager. This is the most important step. The hiring manager's job is to evaluate whether you have the ability to do the work and whether you'd be a good fit for their team. Your job is to convince them that yes, you do, and yes, you would be. A good thing you can do here is use LinkedIn or other professional sites to research the hiring managers or even other analysts who have a similar role to the one you're applying for. The more information you have about the job, the better your chances of actually getting it. You should also use this opportunity to ask lots of questions to help you figure out if the company's a good fit for you. You can do this when you talk to recruiters too.
    
    Now if the hiring manager sees you as a fit, it's very possible you'll have at least one more interview. The point of these interviews is to give your future stakeholders and teammates a chance to decide if you're the best candidate for the position.
    
    If all goes well, you'll get an official offer. Usually by phone first and maybe followed by an official letter. Make sure it's a competitive offer before you sign. Remember, if they reach out to you with an offer, that means they want you as much as you want them.
    
    You should also research salaries, benefits, vacation time, and any other factors that are important to you for similar jobs. Keep in mind, you'll need to find a balance between what you want, what they want to give you, and what's fair. So know your own worth but also understand that the company hiring you has already placed a certain value on your role.
    
    If you're already employed somewhere else during your job search, it's customary and polite to give at least a two-week notice at your old job before starting at the new one. Plus, it's good to give yourself a break before starting your exciting new adventure. You've earned it.
    
    ### Creating a resume
    
    A strong resume is essential to moving forward as a data analytics professional. You want your resume to be a snapshot of all that you've done both in school and professionally.
    
    - The key is to be breif. Try to keep everything in one page and each description to just a few bullet points. Two to four bullet points is enough but remember to keep your bullet points concise.
    - Sticking to one page will help you stay focused on the details that best reflect who you are or who you want to be professionally. One page might also be all that hiring managers and recruiters have time to look at. They're busy people, so you want to get their attention with your resume as quickly as possible.
    - A template is a great way to build a brand new resume or reformat one you already have. Programs like Microsoft Word or Google Docs and even some job search websites all have templates you can use. A template has placeholders for the information you'll need to enter and its own design elements to make your resume look inviting.
    - There's more than one way to build a resume, but most have contact information at the top of the document. This includes your:
        - name
        - address
        - phone number
        - email address. If you have multiple email addresses or phone numbers, use the ones that are most reliable and sound professional. It's also great if you can use your first and last name in your email address, like [janedoe17@email.com](mailto:janedoe17@email.com).
    - Make sure that your contact information matches the details that you've included on professional websites.
    - While most resumes have contact information in the same place, it's up to you how you organize that info.
        - A format that focuses more on skills and qualifications and less on work history is great for people who have gaps in their work history. It's also good for those who are just starting out their career or making a career change.
        - If you do want to highlight your work history, feel free to include details of your work experience starting with your most recent job. If you've had lots of jobs that are related to a new position you're applying for, this format makes sense.
    - Once you've decided on your format, you can start adding your details.
        - Some resumes begin with the summary, but this is optional. A summary can be helpful if you have an experience that is not traditional for a data analyst or if you're making a career transition. If you decide to include a summary, keep it to one or two sentences that highlight your strengths and how you can help the company you're applying to. You'll also want to make sure your summary includes positive words about yourself, like dedicated and proactive. You can support those words with data, like the number of years you've worked or the tools you're experienced in like SQL and spreadsheets.
        - A summary might start off with something like hardworking customer service representative with over five years of experience.
        - Once you've completed this program and have your certificate, you'll be able to include that too, which could sound like this, "entry-level data analytics professional recently completed the Google Data Analytics Professional Certificate.”
        - Another option is leaving a placeholder for your summary while you build the rest of your resume and then writing it after you finish the other sections. This way, you can review the skills and experience you've mentioned and grab two or three of the highlights to use in your summary.
        - The summary might change a little as you apply for different jobs.
    - If you're including a work experience section, there's lots of different types of experience you could add. Outside of jobs with other companies, you could also include volunteer positions you've had and any freelance or side work you've done. The key here is the way in which you describe these experiences.
        - Try to describe the work you did in a way that relates to the position you're applying for. It's important to clearly state the minimum qualifications or requirements shown in the job description in your resume.
    - check out preferred qualifications, which lots of job descriptions also include. These aren't required, but every additional qualification you match makes you a more competitive candidate for the role. Including any part of your skills and experience that matches a job description will help your resume rise above the competition.
    - It's helpful to describe your skills and qualifications in the same way. For example, if a listing talks about organization and partnering with others, try to think about relevant experiences you've had. In your descriptions, you want to highlight the impact you've had in your role, as well as the impact the role had on you.
    - After you've added work experience and skills, you should include a section for any education you've completed. You can add this course as part of your education, and you can also refer to it in your summary and skill sections.
        
        ![Untitled](Google_Data_Analytics_images/Untitled%2042.png)
        
    - Depending on the format of your resume, you might want to add a section for technical skills you've acquired both in this course and elsewhere. Besides technical skills like SQL, you could also include language proficiencies in this section. Having some ability in a language other than English can only help your job search.
    
    ### Elements of a data analyst resume
    
    For data analytics, one of the most important things your resume should do is show that you are a clear communicator. Companies looking for analysts want to know that the people they hire can do the analysis, but also can explain it to any audience in a clear and direct way.
    
    - Summary section:
        - it's a good spot to point out if you're transitioning into a new career role.
        - P-A-R, or PAR statements. PAR stands for Problem, Action, Result. This is a great way to help you write clearly and concisely. Adding PAR statements to your job descriptions or skill section can help with the organization and consistency in your resume.
    - Skill section:
        - make sure you include any skills and qualifications you've acquired through this course and on your own. You don't need to be super technical. But talking about your experience with spreadsheets, SQL, Tableau, and R will enhance your resume and your chances of getting a job.
        - You might want to prioritize technical skills over soft skills, such as:
            - Strong analytical skills
            - Pattern recognition
            - Relational databases and SQL
            - Strong data visualization skills
            - Proficiency with spreadsheets, SQL, R and Tableau
        - Some common professional skills for entry-level data analysts are:
            - SQL: SQL is considered a basic skill that is pivotal to any entry-level data analyst position.
            - Spreadsheets: Although SQL is popular, 62% of companies still prefer to use spreadsheets for their data insights.
            - Data visualization tools: Data visualization tools help to simplify complex data and enable the data to be visually understood. Tableau is best known for its ease of use, so it is a must-have for beginner data analysts. Also, studies show that data analysis jobs requiring Tableau are expected to grow about 34.9% over the next decade.
            - R or Python programming.
    
    ### Highlighting experiences on resumes
    
    What matters for your resume is how you present the work you've done. 
    
    Transferable skills are skills and qualities that can transfer from one job or industry to another. All of there are known as soft skills. Some of those are:
    
    - Communication: When job descriptions say they want strong communication skills for a data analyst, it usually means they want someone who can speak about what they do to people who aren't as technical or analytical. In your work history section, you can highlight how your effective communication skills have helped you. You can also refer to specific presentations you've made and the outcomes of those presentations, and you can even include the audience for your presentations, especially if you present it to large groups or people in senior positions.
        - "effectively implemented and communicated daily workflow to fellow team members, resulting in a 15% increase in productivity.”
    - Problem-solving: When problems arise in a database or lines of code, data analysts need to be able to find and troubleshoot the problem.
        
        ![Untitled](Google_Data_Analytics_images/Untitled%2043.png)
        
    - Teamwork: While you might have plenty of work to do on your own, it'll always be for the benefit of the team. Team means not only the data team you're part of, but the whole company as well.
    - detail-oriented
    - perseverance
    - Presentation skills
    - Collaboration: your ability to share ideas, insights, and criticisms will be crucial.
    - Research: To analyze the data and draw conclusions, you will need to conduct research to stay in-line with industry trends.
    - Adaptability
    - Attention to detail
    
    Using PAR statements and focusing on your transferable soft skills can really add to the power of your resume.
    
    ### Exploring areas of interest
    
    Finding a job you love is even better. Always keep in mind that data analytics is constantly evolving within lots of different industries. Job titles and hiring needs might also change. But the opportunities, no matter what they are when you're searching, will be there.
    
    The certificate you earn here will be most applicable to junior or associate data analyst positions. But that doesn't mean you have to limit your job search to only postings for junior or associate analysts.
    
    - Health care analysts gather and interpret data from sources like electronic health records and patient surveys. Their work helps organizations improve the quality of their care.
    - Data analysts in marketing complete quantitative and qualitative market analysis. They identify important statistics and interpret and present their findings to help stakeholders understand the data behind their marketing strategies.
    - Business intelligence analysts help companies use data they've collected to increase their efficiency and maximize their profits. These analysts usually work with large amounts of data to identify trends and generate business insights
    - Financial analysts use the data to identify and potentially recommend business and investment opportunities.
    
- 5 - **ANALYZE** DATA TO ANSWER QUESTIONS
    
    Organizing your data is one of the most important steps for analysis! Once you get organized, you can perform calculations to find clear and objective answers to any data question.
    
    ## DATA ANALYSIS BASICS
    
    The analysis is the process used to make sense of the data collected. It means taking the right steps to proceed and think about your data in different ways. The goal of analysis is to identify **trends** and **relationships** within the data so that you can accurately answer the question you're asking. To do this, you should stick to the 4 phases of analysis:
    
    1. **Organize data**
    2. **Format and adjust data.** Formatting data streamlines things and saves you time. You can adjust the data in a way that makes it easy to digest by filtering and sorting your data. Sorting involves arranging data into a meaningful order to make it easier to understand, analyze, and visualize.
    3. **Get input from others.** Input from other people can also be really helpful when analyzing information and making decisions. Gaining input from others is important because it gives you a viewpoint you might not understand or have access to. It's also important to seek out others' perspectives early. That way, if they predict any obstacles or challenges, you'll know beforehand.
    4. **Transform data** by observing relationships between data points and making calculations based on the data you have.
    
    ## ORGANIZE DATA
    
    Organization isn't just about making things look orderly. It's also about making it easier to search and locate the data you need in a quick and easy way.
    
    Most of the data you'll use in your analysis will be organized in tables. Tables help you organize similar kinds of data into categories and subject areas that you can focus on as you analyze.
    
    Tables allow you to make decisions about data types. They help you to figure out what variables you need and the data type those variables should have. Tables are helpful because they let you manipulate your data and categorize it. Having distinct categories and classifications lets you focus on, and differentiate between, your data quickly and easily.
    
    Once you have the data organized and formatted, you'll be ready to sort and filter it to find the data you need. Filters and sorts are affected by the type of data we're working with.
    
    The bottom line is that it's important to have your data in the right format. So always be prepared to adjust, no matter how far into your analysis you are.
    
    ### Sorting and filters
    
    Sorting and filtering are two ways you can keep things organized when you format and adjust data to work with it.
    
    - A filter can help you find errors or outliers so you can fix or flag them before your analysis.
    - After you fix errors or identify outliers, you can remove the filter and return the data to its original organization.
    - It is important to point out that, after you filter data, you can **sort the filtered data**, too.
    - Items in the row and column areas of a pivot table are sorted in ascending order by any custom list first.
    - If the items aren’t in a custom list, they will be sorted in ascending order by default. But, if you sort in descending order, you are setting up a rule that controls how the field is sorted even after new data fields are added.
    
    | SORTING | FILTERING |
    | --- | --- |
    | Is when you arrange data into a meaningful order to make it easier to understand, analyze, and visualize. | Is used to show only the data that meets a specific criteria, and hiding the rest. Filtering is really useful when you have lots of data. When it comes to sifting through large, disorganized piles of data |
    | It ranks your data based on a specific metric you choose. Like ascending or descending order, alphabetic | You can save time by zeroing in on the data that is really important or the data that has bugs or errors. |
    | It helps you to group similar data together by a classification. | It gives you the ability to find what you are looking for without too much effort. It reduces the amount of data that is displayed. |
    | You can sort data in spreadsheets, SQL databases (when your dataset is too large for spreadsheets), and tables in documents. | Most spreadsheets and SQL databases allow you to filter your data in a variety of ways. |
    | You can use sort to quickly order the data | You can use filter to display only the data that meets the criteria that you have chosen. |
    - You can also **filter** data in SQL using the WHERE clause. The WHERE clause works similarly to filtering in a spreadsheet because it returns rows based on a condition you name. Keep in mind that capitalization matters here, so we have to make sure that the letter casing matches the column name exactly. The AND clause allows you to write a query with more than one condition.
    - you can filter data and then sort the filtered results. Using the FILTER and SORT functions together in a range of cells can programmatically and automatically achieve these results for you.
    - while sorting puts data in a specific order, filters narrow down data, so you only see data that fits the filter.
    
    ### Sorting data in spreadsheets
    
    When you sort data based on a specific metric, you can uncover new patterns and relationships within datasets you might not have otherwise noticed.
    
    - In spreadsheets, you can sort data by ascending or descending order, or using numbers or letters.
    - If cells are labeled with color, you can sort them by color, too.
    - When sorting data in a spreadsheet, you can choose to:
        - "Sort sheet": all of the data in a spreadsheet is sorted by the conditions of a single column, but the related information across each row stays together.
        - "Sort range.” (the values in the column): doesn't keep the information across rows together. When you sort a range, you're selecting a specific collection of cells or the range that you want the sorting limited to. Nothing else on the spreadsheet gets rearranged but the specified cells.
    
    There are 2 methods for sorting spreadsheet data: one involves using the menu; the other involves writing out the sort function.
    
    - Using the menu is: Select the column > Data > Short sheet By.. or Short range By…
    - Using de SORT function:  =SORT(1,2,3)
        
        Where:
        
        1. range in which data is collected from
        2. the range from what we're sorting by. this part of the function doesn't recognize column letters. So we use the corresponding number instead (2 for column B, 4 for column D, etc)
        3. You'll need to decide whether you want the data in this column to be in ascending or descending order. A TRUE statement is in ascending order, and FALSE is descending. 
    
    After you've tackled writing SORT functions, you'll want to customize sort orders, too. A customized sort order is when you sort data in a spreadsheet using multiple conditions. This means that sorting will be based on the order of the conditions you select:
    
    1. Click Data, then Sort range, then Advanced range sorting options.
    2. Check the option: Data has a header row
    3. Click on “Sort by” > choose the column you want to use a condition.
    4. If you want to add another sorting condition, click on “Add another sort column”
    
    ### Sorting data in SQL
    
    When a spreadsheet has too much data, you can get error messages, or it can cause your program to crash. SQL shortens processes that would otherwise take a very long time or be impossible to complete in a spreadsheet. It's much quicker than a spreadsheet.
    
    - You can use the ORDER BY clause to sort results returned in a query.
        
        ```sql
        SELECT ...
        FROM ...
        ORDER BY *Column_name* DESC #ASC for ascending order, DESC for descending order
        ```
        
    - You can combine sorts and filters to display information differently.
        
        ```sql
        SELECT ...
        FROM ...
        WHERE *Column_name* = "*Condition*"
        ORDER BY *Column_name* DESC #ASC for ascending order, DESC for descending order
        
        ```
        
    - We can filter for two conditions at once using the AND filter.
        
        ```sql
        SELECT ...
        FROM ...
        WHERE *Column_name1* = "*Condition1*" AND *Column_nameN* = "*Condition2*"
        ORDER BY *Column_name* DESC #ASC for ascending order, DESC for descending order
        ```
        
    
    ## CONVERT AND FORMAT DATA
    
    Sometimes, you need to convert data when you're working with spreadsheets. That might mean changing numbers into dates, strings, percentages, or even currency.
    
    Until you change the data type, you won't be able to sort them the way you want. It's also possible that your datasets contain inconsistent units of measurement that you'll need to convert.
    
    Incorrectly formatted data can:
    
    - Lead to time-consuming mistakes in your analysis
    - Take time to fix
    - Affect stakeholder’s decision-making.
    
    But taking the time early on to convert and format your data can help you avoid that.
    
    ### How to convert and format your data in Spreadsheet
    
    - On the toolbar at the top of the sheet, you'll find a menu that can help you convert these numbers into specific data types. It gives you a lot of choices just from the drop-down menu, such as number, currency, date, percentage.... And if you click to open the full menu, there's even more options, including one for a custom number format.
    - You can go even further and convert the unit of measurement you're using: use the CONVERT function to change the unit of measurement. We'll input the CONVERT function in a new column: CONVERT(A,”B”,”C”) where:
        - A: Cell we want to convert
        - B: original measurement
        - C: New measurement
        
        When adding data to tables using a formula, go back and paste the data in as values afterwards. That way they're locked in. There's an option for "Paste special." And there's an option to "Paste values only.”
        
        - For details on the correct syntax, check [HERE]([https://support.google.com/docs/answer/6055540?hl=en](https://support.google.com/docs/answer/6055540?hl=en))
    
    ### Data validation
    
    Data validation allows you to control what can and can’t be entered in you worksheet. Usually, data validation is used to add drop-down lists to cells with predetermined options for users to choose from. Since you control what's being entered into the worksheet, it cuts down on how much data cleaning you have to do later on.
    
    With data validation, you can:
    
    - Add dropdown lists with predetermined options:
        1. select the column
        2. click on Data > Data validation > + Add rule
        3. Click +Add rule to callup a  new Criteria window that will allow you to define the validation values
        4. Choose Dropdown menu as criteria
    - Create custom checkboxes:
        1. select the cell
        2. click on Data > Data validation > + Add rule
        3. Click +Add rule to callup a  new Criteria window that will allow you to define the validation values
        4. choose Checkbox as a criteria
        5. you can use custom cell values also
    - Protect structured data and formulas
        1. The data validation menu has the option to Reject inputs, which helps make sure our custom tools will continue to run correctly, even if someone puts the wrong data in by mistake.
    
    Data validation can help your team track progress, protect your tables from breaking when working in big teams, and help you customize tables to your needs.
    
    ### Conditional formatting
    
    Conditional formatting is a spreadsheet tool that changes how cells appear when values meet specific conditions. It can be used to change a cell’s color in order to highlight it.
    
    We click on format > Conditional formatting. This brings up a sidebar where we can select our range rule in formatting style.
    
    1. Range: which rows to apply our formatting to when the condition we set is met. We can click this button in the range options to select all of the rows we're applying the formatting to instead of typing it in.
    2. we can choose the rule that we want to apply to these cells.
    3. Then we'll choose a color to apply to those cells
    
    ### Transforming data in SQL (BigQuery dialect)
    
    - CAST function: CAST is an American National Standards Institute (ANSI) function used in lots of programming languages, including BigQuery. CAST indicates that you will be converting the data you select to a different data type
        
        ```sql
        CAST(*expression* AS *typename*)
        ```
        
        Where *expression* is the data to be converted and *typename* is the data type to be returned.
        
        You can do different types of convertions, like:
        
        - Number to a string
        - String to a number
        - Date to a string
        - Date to a datetime
    - [Conversion Rules in Standard SQL]([https://cloud.google.com/bigquery/docs/reference/standard-sql/conversion_rules](https://cloud.google.com/bigquery/docs/reference/standard-sql/conversion_rules))
    - To avoid errors in the event of a failed query, use the SAFE_CAST function instead. The SAFE_CAST function returns a value of Null instead of an error when a query fails. The syntax for **SAFE_CAST** is the same as for **CAST**. Simply substitute the function directly in your queries.
    - Browse these resources for more information about data conversion using other SQL dialects (instead of BigQuery):
        - [SQL Server reference documentation]([https://learn.microsoft.com/en-us/sql/t-sql/functions/cast-and-convert-transact-sql?view=sql-server-ver15](https://learn.microsoft.com/en-us/sql/t-sql/functions/cast-and-convert-transact-sql?view=sql-server-ver15))
        - [MySQL reference documentation]([https://dev.mysql.com/doc/refman/8.0/en/cast-functions.html](https://dev.mysql.com/doc/refman/8.0/en/cast-functions.html))
        - [Blog about type casting that has links to other SQL short guides]([https://www.rudderstack.com/guides/how-to-sql-type-casting/](https://www.rudderstack.com/guides/how-to-sql-type-casting/))
    
    ## COMBINE MULTIPLE DATASETS
    
    We can use CONCAT to combine strings from multiple tables to create new strings (in SQL).
    
    ![Untitled](Google_Data_Analytics_images/Untitled%2044.png)
    
    ### Strings in spreadsheets
    
    - LEN: Return the length of a string of text by counting the number of characters it contains
    - LEFT: Return a set number of characters from the left side of a text string
    - RIGHT: Return a set number of characters from the right side of a text string.
    - FIND: Locate specific characters in a string
    
    | **Function** | **Usage** | **Example** |
    | --- | --- | --- |
    | CONCAT | A function that adds strings together to create new text strings that can be used as unique keys | CONCAT (‘Google’, ‘.com’); |
    | CONCAT_WS | A function that adds two or more strings together with a separator | CONCAT_WS (‘ . ’, ‘www’, ‘google’, ‘com’)
    
    *The separator (being the period) gets input before and after Google when you run the SQL function |
    | CONCAT with + | Adds two or more strings together using the + operator | ‘Google’ + ‘.com’ |
    - JOIN: Combine rows from two or more tables based on a related column
    - LIMIT: Return a certain number of records
    - CONCAT: Add strings together to create new text strings that can be used as unique keys
    - CONVERT: Change the unit of measurement of a value in data
    - ROUND: Limit records to a certain number of decimal places
    
    ### Best practices for searching online
    
    - Thinking skills: it starts with how you approach a problem mentally. This helps you build your mental model. Data analysts use these thinking skills to approach a problem logically and break it into smaller parts. Building this into your own problem-solving process can help you pinpoint specific questions, which you can use to find resources more easily.
    - Data analytics terms: Knowing how to frame data analytics questions with the same language other analysts are using will help you get more search results, and it'll help you understand what other analysts are saying.
    - Basic knowledge of tools. That way, when an online resource is walking you through a new function and a tool that you've used before, you'll know how those tools work.
    - Being able to modify example code is a must. Understanding the syntax of formulas and functions for different tools will allow you to take what you learned online and make it work for you, and maybe even build on it to create a whole new solution.
    
    ### Seeking help on Stack Overflow
    
    Having a variety of tools in your tool kit is important as a data analyst, but just as important is knowing when to use them. If you find yourself stuck on a problem, it can be a good idea to take a step back and reconsider how you're approaching a task. Do you have too much data for a single spreadsheet? Switch to SQL. Are you spending more time debugging queries than actually analyzing data? Maybe you should consider R.
    
    - If you would like to view only questions that have a certain tag, include the tag name in brackets with your search. For example, if you want to only find questions that have the tag “SQL”, then type[SQL] in the search field, along with your keywords or question.
    - When asking a question on Stack Overflow, keep it specific. Don’t use it to ask questions with opinion-based answers.
    - The form for asking a question has three sections:
        - Tittle: this is where you ask your question
        - Body: summarize your problem and include expected and actual results.
        - Tags: tags include specific keywords, like program names.
    
    ### When to use which tool
    
    Having a variety of tools in your tool kit is important as a data analyst, but just as important is knowing when to use them. If you find yourself stuck on a problem, it can be a good idea to take a step back and reconsider how you're approaching a task. Do you have too much data for a single spreadsheet? Switch to SQL. Are you spending more time debugging queries than actually analyzing data? Maybe you should consider R.
    
    R is another programming language, but it's not a database language like SQL. It's a programming language frequently used for statistical analysis, visualization, and other data analysis.
    
    ## VLOOKUP FOR DATA AGGREGATION
    
    Aggregation means collecting or gathering many separate pieces into a whole. 
    
    Data aggregation is the process of gathering data from multiple sources in order to combine it into a single summarized collection. In data analytics, a summarized collection, or summary, describes identifying the data you need and gathering it all together in one place.
    
    So in data, the puzzle pieces represent the data that lives in different, separate datasets. Getting them organized is the aggregation process. Then the piles of pieces that complete a single puzzle become your summary. And finally, putting those pieces back together is like analyzing them to gain important insights.
    
    Data aggregation helps data analyst:
    
    - Identify trends
    - Make comparisons
    - Gain insights that wouldn't be possible if each of the data elements were analyzed on its own.
    
    Data can also be aggregated over a given time period to provide statistics, such as averages, minimums, maximums, and sums.
    
    Functions are a big help in making data aggregation possible. VLOOKUP is a data aggregation tool.
    
    ### Using VLOOKUP
    
    Before using VLOOKUP, we have to clean the data and fix some common errors, so we will do some common data-cleaning tasks:
    
    - different data types: we can change the format manually, or we can use functions like:
        - VALUE: a function that converts a text string that represents a number to a numerical value
    - Extra spaces: We can use TRIM and delete any extra spaces added to the cell
    - Duplicates: If there are duplicate rows in the search, it will return only the first match it finds. Using Remove duplicates is a great way to get rid of duplicates and help make sure you find the right record during the lookup.
    
    Here is the syntax:
    
    **VLOOKUP( *search_key, range, index, [is_sorted]*)**
    
    Lets explain VLOOKUP with an example:
    
    **VLOOKUP(103, A2:B26, 2, FALSE)**
    
    - **103:** a value to search for.
    - **A2:B26:**  the range that will be searched.
    - **2:** VLOOKUP will not recognize column names such as A, B, or C, so we use a number to indicate the column (The column index of the value to be returned).
        - • If index is not between 1 and the number of columns in range, #VALUE! is returned.
    - **FALSE:** tells VLOOKUP to find an exact match. If this said true, the function will return only a close match, which might not be what we want.
    
    One of the most common things data analysts do with VLOOKUP is populating data in one spreadsheet from another. VLOOKUP can connect two sheets together on a matching column to populate one single sheet.
    
    Two common reasons to use VLOOKUP are:
    
    - Populating data in a spreadsheet
    - Merging data from one spreadsheet with data in another
    
    ### Identifying common VLOOKUP errors
    
    Troubleshooting has to do with asking the right questions, some of them are:
    
    - How should I prioritize these issues?
    - In a single sentence, what’s the issue I’m facing?. This helps to clarify what's really going on, so I don't get bogged down with extra details.
    - What resources can help me solve the problem?
    - How can I stop this problem from happening in the future?
    
    VLOOKUP have some limitations:
    
    - VLOOKUP only returns the first match it finds, even if there are lots of possible matches. VLOOKUP only looks at data to the right after a match is found. In other words, the index for VLOOKUP indicates columns to the right only. This may require you to move columns around before you use VLOOKUP.
    - Let's say the first few rows of a VLOOKUP have returned the correct result. But when you drive the function down the column, problems start popping up. This is probably because the table array part of the function hasn't been locked or made absolute. An absolute reference is a reference that is locked so that rows and columns won't change when copied. You can fix this issue by wrapping the table array in dollar signs.
    - Version control issues. In other words, a function worked perfectly at first, but then something in the spreadsheet it was referencing changed. There are a few actions data analysts can take to ensure this doesn't happen:
        - lock the spreadsheet. This stops other people from making changes. Go to select data > Protected sheets and ranges. Next, choose what you want to protect
        - Use MATCH: A function used to locate the position of a specific lookup value and can help you with version control.
    - Exact and approximate matching. TRUE tells VLOOKUP to look for approximate matches, and FALSE tells VLOOKUP to look for exact matches. It's important to know that VLOOKUP starts at the top of a specified range and searches downward vertically in each cell to find the right value. It stops searching when it finds any value that's greater than or equal to the lookup value. That's why data analysts typically use FALSE, like this. That way VLOOKUP only returns the exact match to what you've entered in the lookup value.
    - #N/A: #N/A indicates that a matching value can't be returned as a result of the VLOOKUP. The error doesn’t mean that anything is actually wrong with the data, You can use the IFNA function to replace the #N/A error with something more descriptive, like “Does not exist”.
    - • After you have populated data with the VLOOKUP formula, you may copy and paste the data as values only to remove the formulas so you can manipulate the data again.
    
    ## UNDERSTANDING JOINS
    
    JOIN is a SQL clause that's used to combine rows from two or more tables based on a related column. Basically, you can think of a JOIN as a SQL version of VLOOKUP.
    
    There are four common JOINs data analysts use:
    
    - INNER JOIN: A function that returns records with matching values in both tables. For the records to appear in the results table, they'll have to be key values in both tables.  INNER is *optional* in this SQL query because it is the default as well as the most commonly used JOIN operation.
    - LEFT JOIN: A function that will return all the records from the left table and only the matching records from the right table. Each row in the left table appears in the results even if there are no matches in the right table. You may see this as LEFT OUTER JOIN, but most users prefer LEFT JOIN.
    - RIGHT JOIN: A function that will return all records from the right table and only the matching records from the left. You may see this as RIGHT OUTER JOIN or RIGHT JOIN. Practically speaking, RIGHT JOIN is rarely used. Most people simply switch the tables and stick with LEFT JOIN.
    - OUTER JOIN: A function that combines RIGHT and LEFT JOIN to return all matching records in both tables. You may sometimes see this as FULL JOIN.
    
    ![Untitled](Google_Data_Analytics_images/Untitled%2045.png)
    
    JOINs help you combine matching or related columns from different tables. We refer to these values as primary and foreign keys. JOIN use these keys to identify relationships and corresponding values. A JOIN combines tables by using a primary or foreign key to align the information coming from both tables in the combination process.
    
    When we input JOIN into SQL, it usually defaults to inner JOIN. 
    
    In English and SQL we read from left to right. The table mentioned first is left and the table mentioned second is right. You can also think of left as a table name to the left of the JOIN statement and right as a table name to the right of the JOIN statement.
    
    ### The general JOIN syntax
    
    This is an example of a SQL clause that brings exactly the same:
    
    ![Untitled](Google_Data_Analytics_images/Untitled%2046.png)
    
    ```sql
    SELECT
    	--table columns from tables are inserted here
    	*table_name1.column_name*,
    	*table_name2.column_name*
    FROM
    	*dataset_name.table_name1*
    JOIN
    	*dataset_name.table_name2*
    ON *table_name1.column_name* = *table_name2.column_name*
    ```
    
    **ON** in SQL identifies how the tables are to be matched for the correct information to be combined from both.
    
    Example of INNER JOIN:
    
    ![Untitled](Google_Data_Analytics_images/Untitled%2047.png)
    
    BUT! if we are using a public data set, the query will look something like this:
    
    ![Untitled](Google_Data_Analytics_images/Untitled%2048.png)
    
    And you can make it easier to read using alias for each table:
    
    ![Untitled](Google_Data_Analytics_images/Untitled%2049.png)
    
    Another examples:
    
    ![Untitled](Google_Data_Analytics_images/Untitled%2050.png)
    
    ### Secret identities: The importance of aliases
    
    Aliasing is the process of using aliases. Aliases are used in SQL queries to create temporary names for a column or table. Aliases make referencing tables and columns in your SQL queries much simpler when you have table or column names that are too long or complex to make use of in queries.
    
    In SQL queries, aliases are implemented by making use of the AS command. The basic syntax for the AS command can be seen in the following query for aliasing a table:
    
    ```sql
    SELECT *column_name(s)*
    FROM *table_name* AS *alias_name*
    ```
    
    Notice that AS is preceded by the table name and followed by the new nickname. It is a similar approach to aliasing a column:
    
    ```sql
    SELECT *column_name* AS *alias_name*
    FROM *table_name*
    ```
    
    If using AS results in an error when running a query because the SQL database you are working with doesn't support it, you can leave it out. In the previous examples, the alternate syntax for aliasing a table or column would be:
    
    - FROM table_name alias_name
    - SELECT column_name alias_name
    
    The key takeaway is that queries can run with or without using AS for aliasing, but using AS has the benefit of making queries more readable. It helps to make aliases stand out more clearly.
    
    ## COUNT AND COUNT DISTINCT
    
    COUNT can be used to count the total number of numerical values within a specific range in spreadsheets. COUNT in SQL does the same thing. COUNT is a query that returns the number of rows in a specified range
    
    COUNT DISTINCT is a query that only returns the distinct values in that range. Basically, this means COUNT DISTINCT doesn't count repeating values. 
    
    COUNT returns the number of rows in a specified range. COUNT DISTINCT only returns the distinct values in a specified range.
    
    You'll use COUNT and COUNT DISTINCT anytime you want to answer questions about how many
    
    ![Untitled](Google_Data_Analytics_images/Untitled%2051.png)
    
    ## WORK WITH SUBQUERIES
    
    A subquery is a SQL query that is nested inside of a larger query.
    
    With subqueries you can combine different pieces of logic together. Because the logic of your outer query relies on the inner query, you can get more done with a single query. This means all of the logic is in one place, which makes it more efficient and easier to read. The statement containing the subquery can also be called the outer query or the outer select. This makes the subquery the inner query or inner select. The inner query executes first so that the results can be passed on to the outer query to use.
    
    Usually you'll find subqueries nested in FROM or WHERE clauses.
    
    ![Untitled](Google_Data_Analytics_images/Untitled%2052.png)
    
    We can also add a subquery inside the FROM clause:
    
    ![Untitled](Google_Data_Analytics_images/Untitled%2053.png)
    
    Or inside of a WHERE clause.
    
    Usually, you will find subqueries nested in the SELECT, FROM, and/or WHERE clauses. There is no general syntax for subqueries, but the syntax for a basic subquery is as follows:
    
    ![Untitled](Google_Data_Analytics_images/Untitled%2054.png)
    
    You will find that, within the first SELECT clause is another SELECT clause. The second SELECT clause marks the start of the subquery in this statement.
    
    ### Subquery rules
    
    - Subqueries must be enclosed within parentheses
    - A subquery can have only one column specified in the SELECT clause. But if you want a subquery to compare multiple columns, those columns must be selected in the main query.
    - Subqueries that return more than one row can only be used with multiple value operators, such as the IN operator which allows you to specify multiple values in a WHERE clause.
    - A subquery can’t be nested in a SET command. The SET command is used with UPDATE to specify which columns (and values) are to be updated in a table.
    - Comparison operators such as >, <, or = help you compare data in subqueries. You can also use multiple row operators including IN, ANY, or ALL.
    
    To learn more about Subqueries, check the next links:
    
    - [**SQL subqueries:**](https://www.w3resource.com/sql/subqueries/understanding-sql-subqueries.php) This detailed introduction includes the definition of a subquery, its purpose in SQL, when and how to use it, and what the results will be
    - [**Writing subqueries in SQL](https://mode.com/sql-tutorial/sql-sub-queries/):** Explore the basics of subqueries in this interactive tutorial, including examples and practice problems that you can work through
    
    ### Using subqueries to aggregate data
    
    The WHERE function can't be used with aggregate functions. For example, you can use WHERE on a statement and follow it with GROUP BY. But when you want to use GROUP BY first and then use WHERE on that output, you'll need a different function. This is where HAVING comes in.
    
    HAVING allows you to add a filter to your query instead of the underlying table when you're working with aggregate functions. That way it only returns records that meet your specific conditions.  The HAVING clause was added to SQL because the WHERE keyword could not be used with aggregate functions.
    
    ```sql
    ##SQL HAVING Syntax:
    SELECT *column_name*, aggregate_function(*column_name*)
    FROM *table_name*
    WHERE *column_name operator* value
    GROUP BY *column_name*
    HAVING aggregate_function(*column_name*) operator value;
    ```
    
    Similarly, CASE returns records with your conditions by allowing you to include if/then statements in your query.
    
    The `CASE` expression goes through conditions and returns a value when the first condition is met (like an if-then-else statement). So, once a condition is true, it will stop reading and return the result. If no conditions are true, it returns the value in the `ELSE` clause. If there is no `ELSE` part and no conditions are true, it returns NULL.
    
    ```sql
    ##SQL CASE Syntax:
    CASE
        WHEN *condition1* THEN *result1*
        WHEN *condition2* THEN *result2*
        WHEN *conditionN* THEN *resultN*
        ELSE *result*
    END;
    ```
    
    ![Untitled](Google_Data_Analytics_images/Untitled%2055.png)
    
    ## DATA CALCULATIONS
    
    Formulas are one of the many shortcuts that data analysts use. 
    
    ### Functions
    
    - COUNTIF: returns the number of cells that match a specified value. The basic syntax for COUNTIF is: **=COUNTIF(range, criterion)**
    - SUMIF. The basic syntax of a SUMIF function is: **=SUMIF(range, criterion, sum_range).** The first range is where the function will search for the condition that you have set. The criterion is the condition you are applying and the sum_range is the range of cells that will be included in the calculation.
    - AVERAGEIF
    - SUMIFS. SUMIFS can include multiple conditions. The basic syntax is: **=SUMIFS(sum_range, criteria_range1, criterion1, [criteria_range2, criterion2, ...])**
    - COUNTIFS. COUNTIFS has the same basic syntax as SUMIFS: **=COUNTIFS(criteria_range1, criterion1, [criteria_range2, criterion2, ...])**
    - IFS
    - VLOOKUP
    - INDEX
    - MATCH
    - IF with AND, OR, and NOT.
    - MAXIFS. The basic syntax is: **=MAXIFS(max_range, range1, criteria1, [range2], [criteria2],…)**
    - SUMPRODUCT: multiplies arrays and returns the sum of those products (multiplies each of the values in two or more arrays together.).  The basic syntax is: **=SUMPRODUCT(array1, [array2],…)**. An array is kind of like a range in a spreadsheet. It can be used to find the total revenue, for example.
    
    ### Pivot table
    
    Pivot tables let you view data in multiple ways to find insights and trends. But pivot tables can also help with calculations, organize, and filter data. It is a tool used to sort, reorganize, group, count, total, or average data in spreadsheets. They can help you quickly make sense of larger data sets by comparing metrics, performing calculations, and generating reports.
    
    A pivot table has four **basic parts**:
    
    - Rows: organize and group data you select horizontally.
    - Columns: organize and display values from your data vertically. Similar to rows, columns can be pulled directly from the data set or created using **values**.
    - Values: are used to calculate and count data. This is where you input the variables you want to measure. This is also how you create calculated fields in your pivot table.
    - Filters: enables you to apply filters based on specific criteria — just like filters in regular spreadsheets
    
    When you're in your job, you want to answer the questions that your manager and stakeholders ask. But you also want to answer the ones that come up while you're doing your analysis.
    
    To **create a pivot table**:
    
    1. Insert menu > Pivot table
    2. Adding it a new sheet is especially helpful when working on a large dataset. It helps keep our calculations together in one place and separate from the rest of the data.
    3. It’s a good practice to rename the sheet according to what you are trying to do there.
    4. Once you have created your pivot table, there will be a pivot table editor that you can access to the right of your data. This is where you will be able to customize your pivot table, including what variables you want to include for your analysis. ****We can build our pivot table starting with the rows.
        
        ![Untitled](Google_Data_Analytics_images/Untitled%2056.png)
        
        1. We can create a pivot date group for date rows to see only by year or by month. Right-click on any value of the column > Create pivot date group
    5. Next, we can work with the values.
        1. The Summarize By drop-down menu changes the function applied to the values. The SUM function is the default function, but there are other options, such as COUNT.
    6. After that, we can start adding columns but with the option “values”.
    7. You can create some basic visualizations based on your custom tables to share your findings with stakeholders. Select any cell in your pivot table and then navigate to the Insert menu. Select Insert Chart.
    - We can copy and paste the pivot table to create more
    - The filters will be applied to the entire table.
    - We can create calculated fields as Value columns. In excel we can create it using the field settings and the create formula menu. This is useful to check the accuracy of some of our data before analyzing it.
    
    ### More SQL calculations
    
    The operators are embedded in the queries when pulling data from a database. Just like spreadsheet formulas, there are a few different ways to perform calculations using queries. 
    
    The syntax of a query is its structure. It should include all the specific details of the data you want to pull into a new table where those details should be placed.
    
    For arithmetic calculation, you just need to use the symbols inside the SELECT statement.
    
    A lot of times, you can use functions instead of operators to complete calculations. For example, the SUM function can complete addition problems in spreadsheets and SQL. The AVERAGE function in a spreadsheet is the same as the AVG function in SQL. They both return the average value of a set of numbers. In SQL, these functions are considered aggregate functions because they perform a calculation on one or more values and return a single value.
    
    Sometimes you will need to group data before completing calculations, and the **GROUP BY** and **ORDER BY** commands are great for this. These commands are usually paired with aggregate functions like SUM or COUNT.
    
    - GROUP BY: A command that groups rows that have the same values from a table into summary rows. It comes at the end of the query
    - ORDER BY: It orders the data. By default, it is in ascending order. We can add DESC at the end of the query to order it in descending order.
    
    ## THE DATA-VALIDATION PROCESS
    
    Using data validation lets you control what can and can't be entered into your worksheet. One of its uses is protecting structured data and formulas in your spreadsheets. The data validation function is just one part of a larger data validation process. The data validation function is just one part of a larger data validation process.
    
    While the data validation process is a form of data cleaning, you should use it throughout your analysis. This will help you understand your data, check that it's clean, and make sure you're aligning with your business objectives. In other words, it's what you do to make sure your data makes sense.
    
    Keep in mind, you'll build your business knowledge with time and experience. And here's a pro tip. Asking as many questions as possible whenever you need to will make this much easier.
    
    You should always do data validation no matter what analysis tool you're using.
    
    By building in these types of checks as part of your data validation process, you can avoid errors in your analysis and complete your business objectives to make everyone happy.
    
    ### Types of data validation
    
    1. Data type: Validating a data type means checking that the data matches the data type defined for the field.
        - **Purpose**: Check that the data matches the data type defined for a field.
        - **Example**: Data values for school grades 1-12 must be a numeric data type.
        - **Limitations**: The data value 13 would pass the data type validation but would be an unacceptable value. For this case, data range validation is also needed.
    2. Data range: Validating a data range means checking that the data falls within an acceptable range of values defined for the field.
        - **Purpose**: Check that the data falls within an acceptable range of values defined for the field.
        - **Example**: Data values for school grades should be values between 1 and 12.
        - **Limitations**: The data value 11.5 would be in the data range and would also pass as a numeric data type. But, it would be unacceptable because there aren't half grades. For this case, data constraint validation is also needed.
    3. Data constraints: Validating data constraints means checking that data meets certain conditions or criteria, such as type of characters.
        - **Purpose**: Check that the data meets certain conditions or criteria for a field. This includes the type of data entered as well as other attributes of the field, such as number of characters.
        - **Example**: Content constraint: Data values for school grades 1-12 must be whole numbers.
        - **Limitations**: The data value 13 is a whole number and would pass the content constraint validation. But, it would be unacceptable since 13 isn’t a recognized school grade. For this case, data range validation is also needed.
    4. Data consistency: Validating data consistency means checking that the data makes sense in the context of other related data.
        - **Purpose**: Check that the data makes sense in the context of other related data.
        - **Example**: Data values for product shipping dates can’t be earlier than product production dates.
        - **Limitations**: Data might be consistent but still incorrect or inaccurate. A shipping date could be later than a production date and still be wrong.
    5. Data Structure: Validating data structure means checking that the data follows or conforms to a set structure, such as MP3 files or HTML code.
        - **Purpose**: Check that the data follows or conforms to a set structure.
        - **Example**: Web pages must follow a prescribed structure to be displayed properly.
        - **Limitations**: A data structure might be correct with the data still incorrect or inaccurate. Content on a web page could be displayed properly and still contain the wrong information.
    6. Code validation: Code validation means checking that the application code systematically performs any of the previously mentioned validations during user data input.
        - **Purpose:** Check that the application code systematically performs any of the previously mentioned validations during user data input.
        - **Example:** Common problems discovered during code validation include: more than one data type allowed, data range checking not done, or ending of text strings not well defined.
        - **Limitations:** Code validation might not validate all possible variations with data input.
    
    ## USING SQL WITH TEMPORARY TABLES
    
    ### Temporary tables
    
    Temporary tables, or temp tables, store subsets of data from standard data tables for a certain period. They are automatically deleted when you end your SQL session. They are useful when you only need a table for a short time to complete analysis tasks, like calculations.
    
    Instead of filtering the data over and over to return a subset, you can filter the data once and store it in a temporary table. This will let you run several queries about your data without having to keep filtering it.
    
    Naming and using temp tables can help you deal with a lot of data in a more streamlined way, so you don’t get lost repeating query after query with the same code that you could just include in a temporary table.
    
    They can be used as a holding area for storing values if you are making a series of calculations. This is sometimes referred to as **pre-processing** of the data.
    
    They can collect the results of multiple, separate queries. This is sometimes referred to as data **staging**. Staging is useful if you need to perform a query on the collected data or merge the collected data.
    
    ### Create a temporary table in SQL (BigQuery)
    
    We will use the **WITH** clause. The WITH clause is a type of temporary table that you can query from multiple times. It creates something that does the same thing as a temporary table. Even if it doesn’t add a table to the database you are working in for others to see, you can still see your results and anyone who needs to review your work can see the code that leads to your results.
    
    The general syntax for this method is:
    
    ```sql
    WITH *new_temp_table_name* AS(
    	SELECT *
    	FROM
    		*existing_table*
    	WHERE
    		tripduration >=60
    )
    ```
    
    • The opening parenthesis after the **AS** clause creates the subquery that filters the data from an existing table. The subquery is a regular **SELECT** statement along with a **WHERE** clause to specify the data to be filtered.
    
    • The closing parenthesis ends the subquery created by the **AS** clause.
    
    When the database executes this query, it will first complete the subquery and assign the values that result from that subquery to “*new_temp_table_name*,” which is the temporary table. You can then run multiple queries on this filtered data without having to filter the data every time.
    
    ![Untitled](Google_Data_Analytics_images/Untitled%2057.png)
    
    This will give us an error, but it will create a temporary table. Now, it is time to write a query.
    
    We just need to name the temporary query in another query. It is important to use the ## to describe the purpose of your query. This will help you remember the purpose of your query as you’re writing it. It can also help you share your work with others.
    
    ![Untitled](Google_Data_Analytics_images/Untitled%2058.png)
    
    If you need to end your session and start a new runtime later more servers store the code (Query history) using temp tables. You just need to recreate the table by running the code.
    
    ### Example
    
    ![Untitled](Google_Data_Analytics_images/Untitled%2059.png)
    
    ### Other types of temp tables.
    
    Instead of using the WITH clause, you can use the SELECT INTO or the CREATE TABLE clauses.
    
    The **SELECT INTO** clause copies data from one table into a new table, but doesn’t add the new table to the database. It’s useful if you want to make a copy of a table with a specific condition, like a query with a WHERE clause. BigQuery doesn't currently recognize the SELECT INTO command. Instead, here's an example of how a SELECT INTO statement might look in another RDBMS. Using SELECT and INTO, you can create a temporary table based on conditions defined by a WHERE clause to locate the information you need for the temporary table.
    
    ![Untitled](Google_Data_Analytics_images/Untitled%2060.png)
    
    - The INTO clause tells the database to store the data that is being requested in a new temporary table named, in this case, “AfricaSales.”
    
    Using **SELECT INTO** is a good practice when you want to keep the database uncluttered and you don't need other people using the table.
    
    The **CREATE TABLE** clause is a good option when several people need to access the same temp table. This statement adds the table into the database. In most relational database management systems or RDBMSs, you can add metadata to describe the data that's contained in the table you've created. This can help make the table easier to understand for anyone using it.
    
    ```sql
    CREATE TABLE *table_name*(
    	*column1 datatype*,
    	*column2 datatype*,
    	*column3 datatype*,
    ...
    )
    ```
    
    ![Untitled](Google_Data_Analytics_images/Untitled%2061.png)
    
    The **CREATE TABLE** statement is also useful for more complex tables. For example, if the code's difficult to replicate, then making a temp table in this way means it'll be safe for you to access later.
    
    After you have finished working with the table, you would then delete or drop it from the database at the end of your session.
    
    ```sql
    DROP TABLE *table_name*
    ```
    
    **Note:** BigQuery uses **CREATE TEMP TABLE** instead of **CREATE TABLE**, but the general syntax is the same.
    
    Which clause you use depends on your preference and the project’s demands. Different clauses have their own strengths, so understanding how each of them work is helpful for using them effectively.
    
    You may also find that you're working in an RDBMS that uses a different syntax. For example, you might need to use a **CREATE TEMP TABLE** statement instead of CREATE TABLE.
    
    You can repeat your code over and over instead of making a temp table but that usually leaves your queries less readable and more vulnerable to typos.
    
    ### Best practices when working with temporary tables
    
    - Global vs. local temporary tables: Global temporary tables are made available to all database users and are deleted when all connections that use them have closed. Local temporary tables are made available only to the user whose query or connection established the temporary table. You will most likely be working with local temporary tables. If you have created a local temporary table and are the only person using it, you can drop the temporary table after you are done using it.
    - Dropping temporary tables after use: Dropping a temporary table is a little different from deleting a temporary table. Dropping a temporary table not only removes the information contained in the rows of the table, but removes the table variable definitions (columns) themselves. Deleting a temporary table removes the rows of the table but leaves the table definition and columns ready to be used again. Although local temporary tables are dropped after you end your SQL session, it may not happen immediately. If a lot of processing is happening in the database, dropping your temporary tables after using them is a good practice to keep the database running smoothly.
    
    ## USING CONNECTED SHEETS WITH BIGQUERY
    
    Connected Sheets is a tool that allows data professionals to use basic spreadsheet functions to analyze large datasets housed in BigQuery. With Connected Sheets users don’t need to know SQL. Instead, anyone, not just data professionals, can generate insights with basic spreadsheet operations such as formulas, charts, and pivot tables.
    
    Recall that BigQuery allows users to analyze petabytes (a million gigabytes) of data using complex queries. A benefit of BigQuery is that it reduces the time needed to develop insights from large datasets.
    
    Google Sheets, on the other hand, is a spreadsheet tool that is easy to use and shareable with a familiar interface. It also allows simple and flexible analysis with tools like pivot tables, charts, and formulas.
    
    Connected Sheets integrates both BigQuery and Google Sheets, allowing the user to analyze billions of rows of data in Sheets without any need for specialized knowledge, such as SQL.
    
    Additionally, Connected Sheets is built to handle big data. Users won’t experience the same limitations or performance issues they’ve had in the past (such as data loss) when working with large data sets in spreadsheets.
    
    ### Why would a data analytics professional use Connected Sheets?
    
    As a data analytics professional, Connected Sheets can help with several tasks, such as:
    
    - Collaborating with partners, analysts, or other stakeholders in a familiar spreadsheet interface;
    - Ensuring a single source of truth for data analysis without additional .csv exports;
    - Defining variables so that all users are working with the same data;
    - Sharing insights with your team in a secure environment; and
    - Streamlining your reporting and dashboard workflows.
    
    Many teams and industries benefit from Connected Sheets such as finance, marketing, and operations teams.
    
    A few example use cases of Connected Sheets include:
    
    - **Business planning:** A user can build and prepare datasets, and then find insights from the data. For example, a data analyst can analyze sales data to determine which products sell better in different locations.
    - **Customer service:** A user can find out which stores have the most complaints per 10,000 customers.
    - **Sales:** A user can create internal finance and sales reports. After completing, they can share revenue reports with sales reps.
    - **Logistics, fulfillment, and delivery:** A user can run real-time inventory management and intelligent analytics tools.
- 6 - **SHARE** DATA THROUGH THE ART OF VISUALIZATION
    
    One question to ask yourself is: “what is the best way to tell the story within my data?”
    
    Tableau helps us create visualizations from our analysis so that we can share our findings more effectively.
    
    Data visualization is putting information into an image to make it easier for other people to understand.
    
    Scientists and mathematicians began to truly embrace the idea of arranging data visually in the 1700s and 1800s.
    
    As we keep learning how to more efficiently communicate with visuals, the quality of our insights continues to grow too.
    
    As an analyst in today’s world, we split your time with data visuals in two ways:
    
    1. Looking at visuals to understand and draw conclusions about data.
    2. Creating visuals using raw data to tell a story.
    
    A well-made data visualization has the power to change people’s minds. Plus, it can help someone who doesn’t have the same technical background or experience as you from their own opinions.
    
    ## UNDERSTAND DATA VISUALIZATION
    
    A quick rule for creating visualizations is: that your audience should know exactly what they’re looking at **within the first five seconds of seeing it**. In the five seconds after that, your audience should understand the conclusion your visualization is making, even if they aren’t familiar with the research you’ve been doing.
    
    When creating data visualizations, you must strike a balance between presenting enough information for your audience to understand the meaning of the visualization and not overwhelming them with too much detail.
    
    If a visualization looks confusing, then it probably is confusing.
    
    Part of why data visualization is so effective is because people’s eyes are drawn to colors, shapes, and patterns, which makes those visual elements perfect for telling a story that goes beyond just numbers.
    
    ### Rules about what makes a helpful data visualization
    
    - **Five-second rule:** A data visualization should be **clear, effective, and convincing** enough to be absorbed in five seconds or less.
    - **Color contrast:** Graphs and charts should use a **diverging color palette** to show contrast between elements.
    - **Conventions and expectations:** Visuals and their organization should align with **audience expectations** and **cultural conventions**. For example, if the majority of your audience associates green with a positive concept and red with a negative one, your visualization should reflect this.
    - **Minimal labels:** Titles, axes, and annotations should use as **few labels** as it takes to make sense. Having too many labels makes your graph or chart too busy. It takes up too much space and prevents the labels from being shown clearly.
    
    ### Frameworks for organizing your thoughts about visualization
    
    Frameworks help organize your thoughts about data visualization and give you a useful checklist to reference as you plan and evaluate your data visualization. Here are two frameworks that employ slightly different techniques. Both are intended to improve the quality of your visuals.
    
    - The McCandless method: set of guidelines for presentations. It suggests that you start with broad, general ideas and then work your way into the details.. This method lists four elements of good data visualization:
        1. **Information (data):** the data with which you’re working. Without information or data, you cannot communicate your findings successfully.
        2. **Story (concept):** a clear and compelling narrative or concept. The story allows you to share your data in meaningful and interesting ways. Without a story, your visualization is informative, but not really inspiring.
        3. **Goal (function):** a specific objective or function for the visual. The goal of your data visualization makes the data useful and usable. This is what you are trying to achieve with your visualization. Without a goal, your visualization might still be informative, but can’t generate actionable insights.
        4. **Visual form (metaphor):** an effective use of metaphor or visual expression. The visual form element is what gives your data visualization structure and makes it beautiful. Without visual form, your data is not visualized yet.
        
        ![Untitled](Google_Data_Analytics_images/Untitled%2062.png)
        
        And this method has an order, you start with the most basic information:
        
        1. Introduce the graphic by name
        2. Answer obvious questions before they’re asked. Start with the high-level information and work your way into the lowest level of detail that’s useful to your audience (where is this data from? what does it cover?, when?, how?)
        3. State the insight of your graphic.
        4. Call out data to support that insight.
        5. Tell your audience why it matters (the “so what” moment). It’s a good time to present the possible business impact of the solution and clear actions stakeholders can take
    - Kaiser Fung’s Junk Charts trifecta checkup: This approach is a set of questions that can help consumers of data visualization critique what they are consuming and determine how effective it is. You can also use these questions to determine if your data visualization is effective:
        1. What is the practical question?
        2. What does the data say?
        3. What does the visual say?
        
        A well-designed visual effectively answers all three of those questions at once. Moreover, this framework helps you think about your data viz from the perspective of your audience.
        
    
    ### Pre-attentive attributes
    
    Pre-attentive attributes are the elements of a data visualization that people recognize automatically and without conscious effort. The essential, basic building blocks that make visuals immediately understandable are called marks and channels.
    
    - **Marks:** are basic visual objects such as points, lines, and shapes. Every mark can be broken down into four qualities:
        1. Position: Where is a specific mark in space relative to a scale or other marks?. For example, if you’re looking at two different trends, position allows you to compare the pattern of one element relative to another.
            
            ![Untitled](Google_Data_Analytics_images/Untitled%2063.png)
            
        2. Size: How big, small, long, or tall is a mark? This can be very useful for conveying the relationship between categories or data points. Controlling the scale of a visual is important even when comparative sizes are not intended to offer information.
            
            ![Untitled](Google_Data_Analytics_images/Untitled%2064.png)
            
        3. Shape: Does the shape of a specific object communicate something about it? Rather than using simple dots or lines, a bit of creativity can enhance how quickly people are able to interpret a visual by using shapes that align with a given application.
            
            ![Untitled](Google_Data_Analytics_images/Untitled%2065.png)
            
        4. Color: What color is a mark? Colors can be used both as a simple differentiator of groupings or as a way to communicate other concepts such as profitable versus unprofitable, or hot versus cold.
            
            ![Untitled](Google_Data_Analytics_images/Untitled%2066.png)
            
    - **Channels:** are visual aspects or variables that represent characteristics of the data in a visualization. They are basically specialized marks that have been used to visualize data. It’s important to understand that channels vary in terms of how effective they are at communicating data based on three elements:
        1. Accuracy: Are the channels helpful in accurately estimating the values being represented? For example, color is very accurate when communicating categorical differences, such as apples and oranges. But it is much less effective when distinguishing quantitative data, such as 5 from 5.5.
        2. Popout: How easy is it to distinguish certain values from others? There are many ways of drawing attention to specific parts of a visual, and lots of them leverage pre-attentive attributes including line length, size, line width, shape, enclosure, hue, and intensity.
            
            ![Untitled](Google_Data_Analytics_images/Untitled%2067.png)
            
        3. Grouping: How effective is a channel at communicating groups that exist in the data? Consider the proximity, similarity, enclosure, connectedness, and continuity of the channel.
            
            ![Untitled](Google_Data_Analytics_images/Untitled%2068.png)
            
        
        But, remember: The more you emphasize one single thing, the more that counts. Emphasis diminishes with each item you emphasize because the items begin to compete with one another.
        
    
    ### Types data visualizations
    
    - Bar graphs: Use size contrast to compare two or more values. A bar chart is ideal for comparing similar data side by side.
        - In bar graphs with vertical bars, the x-axis is used to represent categories, time periods, or other variables.
        - The y-axis usually has a scale of values for the variables.
        - By making the y-axis start at zero, we’re changing the visual proportions to be more accurate and more honest.
        - A bar chart should always be ranked by value, unless there is a natural order of the data like age or time, for example.
        - Bar charts with horizontal bars effectively show data that are ranked, with bars arranged in ascending or descending order
    - Histogram: A histogram resembles a bar graph, but it’s a chart that shows how often data values fall into certain ranges. A histogram is ideal for comparing the distribution of two variables by individual grouping.
    - Line graphs: Help your audience understand shifts or changes in your data.
        - They’re usually used to track changes or trends through a period of time, but they can be paired with other factors too.
        - When smaller changes exist, line charts are better to use than bar graphs.
        - Line charts can also be used to compare changes over the same period of time for more than one group.
        - The last line chart example is a combo chart which can include a line chart
        
        ![Untitled](Google_Data_Analytics_images/Untitled%2069.png)
        
    - Pie charts: It’s a circular graph that is divided into segments representing proportions corresponding to the quantity it represents, especially when dealing with parts of a whole.
    - Maps: help organize data geographically.
    - Correlation charts: Show relationships among data.
        - It should be used with caution because they might lead viewers to think that the data shows causation.
    - Column chart:  A column chart allows you to display and compare multiple categories of data by their values. use size to contrast and compare two or more values, using height or lengths to represent the specific values.
    
    ![Untitled](Google_Data_Analytics_images/Untitled%2070.png)
    
    - Area charts: allow you to track changes in value across multiple categories of data.
    - Heatmap: Similar to bar charts, heatmaps also use color to compare categories in a data set. They are mainly used to show relationships between two variables and use a system of color-coding to represent different values.
    
    ![Untitled](Google_Data_Analytics_images/Untitled%2071.png)
    
    - Scatterplot: show relationships between different variables.
        - typically used for showing the relationship between two variables., although additional variables can be displayed.
        - scatterplots are typically used to display trends in numeric data.
        
        ![Untitled](Google_Data_Analytics_images/Untitled%2072.png)
        
    - Distribution graph: displays the spread of various outcomes in a dataset.
    - Combo: combo charts use multiple visual markers like columns and lines to showcase different aspects of the data in one visualization. The example below is a combo chart that has a column and line chart together.
        
        ![Untitled](Google_Data_Analytics_images/Untitled%2073.png)
        
    
    ### Guides - resources:
    
    Below is a list of resources that can inspire your next data-driven decisions, as well as teach you how to make your data more accessible to your audience:
    
    [The data visualization catalogue](https://datavizcatalogue.com/#google_vignette)
    
    [The 25 best data visualizations](https://visme.co/blog/best-data-visualizations/)
    
    [10 data visualization blogs](https://www.tableau.com/learn/articles/best-data-visualization-blogs)
    
    [Information is beautiful](https://informationisbeautiful.net/wdvp/gallery-2019/)
    
    [Data studio gallery](https://datastudio.google.com/gallery?category=visualization)
    
    ### Tips to create a powerful visualization
    
    One of your biggest considerations when creating a data visualization is where you’d like your audience to focus.
    
    - As long as it’s not misleading, you should visually represent only the data that your audience needs in order to understand your findings.
    - with your visualization is important to show:
        - Change over time. The time period relevant to your objective.
        - How your data is distributed
    - Reviewing each of these visual examples, where do you notice that they fit in relation to your type of data? One way to answer this is by evaluating patterns in data. Meaningful patterns can take many forms, such as:
        - **Change:** This is a trend or instance of observations that become different over time. A great way to measure change in data is through a line or column chart.
        - **Clustering:** A collection of data points with similar or different values. This is best represented through a distribution graph.
        - **Relativity:** These are observations considered in relation or in proportion to something else. You have probably seen examples of relativity data in a pie chart.
        - **Ranking:** This is a position in a scale of achievement or status. Data that requires ranking is best represented by a column chart.
        - **Correlation:** This shows a mutual relationship or connection between two or more things. A scatterplot is an excellent way to represent this type of data pattern.
    
    ### Correlation and causation
    
    - Correlation: ****in statistics is the measure of the degree to which two variables move in relationship to each other.
        - Correlation doesn’t mean that one event causes another. But, it does indicate that they have a pattern with or a relationship to each other.
        - It one variable goes up and the other variable goes up, it is a positive correlation.
        - If one variable goes up and the other variable goes down, it is a negative or inverse correlation.
        - If one variable goes up and the other variable stays about the samethere is no correlation.
    - Causation: refers to the idea that an event leads to a specific outcome.
    
    When you make conclusions from data analysis, you need to make sure that you don’t assume a causal relationship between elements of your data when there is only a correlation.
    
    In your data analysis, remember to:
    
    - Critically analyze any correlations that you find
    - Examine the data’s context to determine if a causation makes sense (and can be supported by all of the data)
    - Understand the limitations of the tools that you use for analysis
    
    ### Static and Dynamic visualizations
    
    - Static visualization: do not change over time unless they’re edited.
        - Useful when you want to control your data and your data story.
        - Any visualization printed on paper is automatically static
        - Charts and graphs created on spreadsheets are often static too.
    - Dynamic visualization: visualizations that are interactive or change over time.
        - The interactive nature of these graphics means that users have some control over what they see.
        - Helpful when the stakeholders want to adjust what they’re able to view.
        - Visualizations in Tableau are automatically interactive.
        - Other dynamic visualizations upload new data automatically.
    
    The choice between using a static or dynamic visualization usually depends on:
    
    - the data you’re visualizing.
    - The audience you are presenting to
    - How you’re giving your presentation.
    
    ### Data grows on decision trees
    
    A decision tree is a decision-making tool that allows you to make decisions based on key questions that you can ask yourself. There are many different types of decision trees that vary in complexity, and can provide more in-depth decisions.
    
    ![Untitled](Google_Data_Analytics_images/Untitled%2074.png)
    
    Start off by evaluating the type of data you have and go through a series of questions to determine the best visual source:
    
    - Does your data have only one numeric value? If you have data that has one, continuous, numerical variable, then a histogram or density plot are the best methods of plotting your categorical data. Depending on your type of data, a bar chart can even be appropriate in this case.
    - Are there multiple datasets? For cases dealing with more than one set of data, consider a line or pie chart for an accurate representation of your data. A line chart will connect multiple data sets over a single, continuous line, showing how numbers have changed over time. A pie chart is good for dividing a whole into multiple categories or parts.
    - Are you measuring changes over time? A line chart is usually adequate for plotting trends over time. However, when the changes are larger, a bar chart is the better option.
    - Do relationships between the data need to be shown? When you have two variables for one set of data, it is important to point out how one affects the other. Variables that pair well together are best plotted on a scatterplot. However, if there are too many data points, the relationship between variables can be obscured so a heat map can be a better representation in that case.
    
    More decision trees examples:
    
    - Picture taken from: [Marina R. on LinkedIn: #datavisualization #data #dataanalytics #análisisdedatos #data…](https://www.linkedin.com/posts/activity-7150034395276419072-WXwD/?utm_source=share&utm_medium=member_android)
        
        ![Untitled](Google_Data_Analytics_images/Untitled%2075.png)
        
    - [From data to visualization](https://www.data-to-viz.com/)
    - [Selecting the best chart](https://www.youtube.com/watch?v=C07k0euBpr8)
    
    ## DESIGN DATA VISUALIZATIONS
    
    Communicating data visually is a form of art.
    
    The elements of art are:
    
    - Line: can be curved or straight, thick or thin, vertical, horizontal, or diagonal. They can add visual form to your data, and build a structure for your visualization.
    - Shape: should always be two-dimensional, because three-dimensional objects in a visualization can complicate the visual and confuse the audience. It is a great way to add eye-catching contrast, especially size contrast to your data story.
    - color: colors can be described by their:
        - hue: its name.
        - intensity: how bright or dull a color is
        - value: how light or dark the colors are in a visualization. Varying the color value can be a very effective way to draw our audience’s attention to specific areas.
    - Space: the area between, around, and in the objects. There should always be space in data visualizations, just not too much or too little.
    - Movement: used to create a sense of flow or action in a visualization. This is something that should be used sparingly, there is a fine line between attracting attention and distracting the audience.
    
    These particular ones can add value to your data viz by making them more visually effective and compelling.
    
    ### Principles of design
    
    There are nine basic principles of design that data analysts should think about when building their visualizations.
    
    1. **Balance:** The design of a data visualization is balanced when the key visual elements, like color and shape, are distributed evenly. Your visualization shouldn’t have one side distracting from the other.
    2. **Emphasis:** Your visualizations should emphasize the most important data so that users recognize it first. Using color and value is one effective way to make this happen. By using contrasting colors, you can make certain that graphic elements—and the data shown in those elements—stand out.
    3. **Movement:** Movement can refer to the path the viewer’s eye travels as they look at a data visualization, or literal movement created by animations. Movement in data visualization should mimic the way people usually read. You can use lines and colors to pull the viewer’s attention across the page.
    4. **Pattern:** You can use similar shapes and colors to create patterns in your data visualization. This can be useful in a lot of different ways. For example, you can use patterns to highlight similarities between different data sets, or break up a pattern with a unique shape, color, or line to create more emphasis.
    5. **Repetition:** Repeating chart types, shapes, or colors adds to the effectiveness of your visualization.
    6. **Proportion:** Using various colors and sizes helps demonstrate that you are calling attention to a specific visual over others. If you make one chart in a dashboard larger than the others, then you are calling attention to it. It is important to make sure that each chart accurately reflects and visualizes the relationship among the values in it.
    
    These first six principles of design are key considerations that you can make while you are creating your data visualization. These next three principles are useful checks once your data visualization is finished.
    
    1. **Rhythm:** This refers to creating a sense of movement or flow in your visualization. Rhythm is closely tied to the movement principle. If your finished design doesn’t successfully create a flow, you might want to rearrange some of the elements to improve the rhythm.
    2. **Variety:** Your visualizations should have some variety in the chart types, lines, shapes, colors, and values you use. Variety keeps the audience engaged. But it is good to find balance since too much variety can confuse people. The variety you include should make your dashboards and other visualizations feel interesting and unified.
    3. **Unity:** your final data visualization should be cohesive. If the visual is disjointed or not well organized, it will be confusing and overwhelming.
    
    ### Data visualization impact
    
    Choosing the right visualization for your data findings can often come down to one question: **which one will make it easiest for the user to understand the point you’re trying to make?**
    
    - When comparing over time line graphs, bar graphs, stacked bar graphs, and area charts are good ways to visualize how data changes over time.
    - When you are comparing distinct objects, ordered bar, ordered column charts, and group bar graphs are useful.
    - Some charts show parts of a whole (data composition) like stack bars, donuts, stacked areas, pie charts, and tree maps.
    - To show the relationship in your data, you might want to use a scatterplot, bubble charts, column/line charts, and heat maps.
    
    A successful data visualization results in a happy audience, so it’s important to understand how your audience is viewing your data visualization since they should always be top of mind.
    
    Visual journalist Dana Wong proposes that effective visuals have three essential elements:
    
    1. Clear meaning. Good visualizations clearly communicate their intended insight.
    2. Sophisticated use of contrast. It helps separate the most important data from the rest using visual context that our brains naturally look for.
    3. Refined execution. Visuals with refined execution include deep attention to detail. This is done by using visual elements (elements of art)
    
    ### Design thinking
    
    It is a process used to solve complex problems in a user-centric way.
    
    There are five phases of the design process:
    
    1. Empathize. You think about the emotions and needs of the target audience of your data viz
    2. Define. It helps you to find your audience's needs, their problems, and your insights. You will use what you learned in the empathize face to help you spell out exactly what your audience needs from your visualization
    3. Ideate. Here you start to generate your data viz ideas. You’ll use all of your findings from the empathize and define phases to brainstorm potential data viz solutions.
    4. Prototype. You’ll start putting your charts, dashboards, or other visualizations together
    5. Test. By showing them to team members before presenting them to stakeholders.
    
    ## VISUALIZATION CONSIDERATIONS
    
    ### Pro tips for highlighting key information
    
    - **Headlines that pop:** a line of words printed in large letters at the top of a visualization to communicate what data is being presented. It is the attention grabber that makes your audience want to read more.
    - **Subtitles that clarify:** They support the headline by adding more context and description. Adding a subtitle will help the audience better understand the details associated with your chart. Typically, the text for subtitles has a smaller font size than the headline.
    - **Labels that identify:** it identifies data in relation to other data. Most commonly, labels in a chart identify what the x-axis and y-axis show. Always make sure you label your axes.
        
        ![Untitled](Google_Data_Analytics_images/Untitled%2076.png)
        
        Data can also be labeled directly in a chart instead of through a chart legend. This makes it easier for the audience to understand data points without having to look up symbols or interpret the color coding in a legend.
        
        ![Untitled](Google_Data_Analytics_images/Untitled%2077.png)
        
    - **Annotations that focus:** An annotation briefly explains data or helps focus the audience on a particular aspect of the data in a visualization.
        
        ![Untitled](Google_Data_Analytics_images/Untitled%2078.png)
        
    
    | **Visualization
    components** | **Guidelines** | **Style checks** |
    | --- | --- | --- |
    | Headlines | - **Content**: Briefly describe the data
    - **Length**: Usually the width of the data frame
    - **Position**: Above the data | - Use brief language 
    - Don’t use all caps
    - Don’t use italic
    - Don’t use acronyms
    - Don't use abbreviations
    - Don’t use humor or sarcasm |
    | Subtitles | - **Content**: Clarify context for the data
    - **Length**: Same as or shorter than headline 
    - **Position**: Directly below the headline | - Use smaller font size than headline
    - Don’t use undefined words 
    - Don’t use all caps, bold, or italic
    - Don’t use acronyms 
    - Don't use abbreviations |
    | Labels | - **Content**: Replace the need for legends
    - **Length**: Usually fewer than 30 characters
    - **Position**: Next to data or below or beside axes | - Use a few words only
    - Use thoughtful color-coding
    - Use callouts to point to the data
    - Don’t use all caps, bold, or italic |
    | Annotations | - **Content**: Draw attention to certain data 
    - **Length**: Varies, limited by open space
    - **Position**: Immediately next to data annotated | - Don’t use all caps, bold, or italic
    - Don't use rotated text
    - Don’t distract viewers from the data |
    
    ### Ways to make data visualizations accessible:
    
    - Labeling
    - Text alternatives, so that it can be changed into other forms people need such as large print, braille, or speech.
    - Text-based format
    - Distinguishing
    - Simplify
    
    ### Design a chart in 60 minutes
    
    A chart is a graphical representation of data.
    
    ![Untitled](Google_Data_Analytics_images/Untitled%2079.png)
    
    - **Prep (5 min):** Create the mental and physical space necessary for an environment of comprehensive thinking. This means allowing yourself room to brainstorm *how* you want your data to appear while considering the amount and type of data that you have.
    - **Talk and listen (15 min):** Identify the object of your work by getting to the “ask behind the ask” and establishing expectations. Ask questions and concentrate on feedback from stakeholders regarding your projects to help you hone how to lay out your data.
    - **Sketch and design (20 min):** Draft your approach to the problem. Define the timing and output of your work to get a clear and concise idea of what you are crafting.
    - Prototype and improve (20 min): Generate a visual solution and gauge its effectiveness at accurately communicating your data. Take your time and repeat the process until a final visual is produced. It is alright if you go through several visuals until you find the perfect fit.
    
    ## TABLEAU
    
    Tableau is a Business Intelligence and analytics platform that helps people see, understand, and make decisions with data.
    
    ### Which chart or Graph should I use?
    
    - [Which chart or graph is right for you?](http://www.tableau.com/sites/default/files/media/which_chart_v6_final_0.pdf) This presentation covers 13 of the most popular charts in Tableau.
    - [The Ultimate Cheat Sheet on Tableau Charts](https://towardsdatascience.com/the-ultimate-cheat-sheet-on-tableau-charts-642bca94dde5). This blog describes 24 chart variations in Tableau and guidelines for use.
    
    In addition to more traditional charts, Tableau also offers some more specific visualizations that you can use in your dashboard design:
    
    - **Highlight tables** appear like tables with conditional formatting. Review the [steps to build a highlight table](https://help.tableau.com/current/pro/desktop/en-us/buildexamples_highlight.htm).
    - **Heat maps** show intensity or concentrations in the data. Review the [steps to build a heat map](https://help.tableau.com/current/pro/desktop/en-us/buildexamples_highlight.htm).
    - **Density maps** illustrate concentrations (such as a population density map). Refer to [instructions to create a heat map for density](https://help.tableau.com/current/pro/desktop/en-us/maps_howto_heatmap.htm).
    - **Gantt charts** demonstrate the duration of events or activities on a timeline. Review the [steps to build a Gantt chart](https://help.tableau.com/current/pro/desktop/en-us/buildexamples_gantt.htm).
    - **Symbol maps** display a mark over a given longitude and latitude. Learn more from this [example of a symbol map](https://interworks.com/blog/ccapitula/2014/08/18/tableau-essentials-chart-types-symbol-map/).
    - **Filled maps** are maps with areas colored based on a measurement or dimension. Explore an [example of a filled map](https://interworks.com/blog/ccapitula/2014/09/23/tableau-essentials-chart-types-filled-map/).
    - **Circle views** show comparative strength in data. Learn more from this [example of a circle view](https://interworks.com/blog/ccapitula/2014/10/17/tableau-essentials-chart-types-circle-view/).
    - **Box plots**, also known as **box and whisker charts,** illustrate the distribution of values along a chart axis. Refer to the [steps to build a box plot](https://help.tableau.com/current/pro/desktop/en-us/buildexamples_boxplot.htm).
    - **Bullet graphs** compare a primary measure with another and can be used instead of dial gauge charts. Review the [steps to build a bullet graph](https://help.tableau.com/current/pro/desktop/en-us/qs_bullet_graphs.htm).
    - **Packed bubble charts** display data in clustered circles. Review the [steps to build a packed bubble chart](https://help.tableau.com/current/pro/desktop/en-us/buildexamples_bubbles.htm).
    
    ### Creating a Report-Tips
    
    - Once you upload data to your worksheet, it will populate the Connections pane.
    
    ![Untitled](Google_Data_Analytics_images/Untitled%2080.png)
    
    - You can add more connections to other data sources in order to build visualizations that compare different datasets. Simply drag and drop tables from the Sheets section in order to join tables and generate those connections:
    
    ![Untitled](Google_Data_Analytics_images/Untitled%2081.png)
    
    - Tableau uses dimensions and measures to generate customized charts. For example, check out this chart focusing on CO2 emissions per country. The Country Name dimension can be used to show a map of the countries on the planet with dots indicating which countries are represented in the data.
    
    ![Untitled](Google_Data_Analytics_images/Untitled%2082.png)
    
    - Tableau has a wide variety of options for depicting the measure for a given dimension. Most of these options are contained near the main display and the column with dimensions and measures.
    
    ![Untitled](Google_Data_Analytics_images/Untitled%2083.png)
    
    - One option in Tableau is choosing between a vertical or horizontal layout. A vertical layout adjusts the height, a horizontal layout resizes the width of the views and objects it contains. Evenly distributing the items within your layout helps create a clear and organized data visual.
    - You can select either tiled or floating layouts. Tiled items are part of a single-layer grid that automatically resizes based on the overall dashboard size. Floating items can be layered over other objects.
    
    ## DESIGN VISUALIZATIONS IN TABLEAU
    
    There are some tools and options that will help us design a more optimal data visualization.
    
    ### Essential design principles
    
    | **Principle** | **Description** |
    | --- | --- |
    | Choose the right visual | One of the first things you have to decide is which visual will be the most effective for your audience. Sometimes, a simple table is the best visualization. Other times, you need a more complex visualization to illustrate your point. Be sure that you are not creating misleading or deceptive charts. |
    | Optimize the data-ink ratio | The data-ink entails focusing on the part of the visual that is essential to understanding the point of the chart. Try to minimize non-data ink like boxes around legends or shadows to optimize the data-ink ratio. |
    | Use orientation effectively | Make sure the written components of the visual, like the labels on a bar chart, are easy to read. You can change the orientation of your visual to make it easier to read and understand. |
    | Color | There are a lot of important considerations when thinking about using color in your visuals. These include using color consciously and meaningfully, staying consistent throughout your visuals, being considerate of what colors mean to different people, and using inclusive color scales that make sense for everyone viewing them. |
    | Numbers of elements | Think about how many elements you include in any visual. If your visualization uses lines, try to plot five or fewer. If that isn’t possible, use color or hue to emphasize important lines. Also, when using visuals like pie charts, try to keep the number of segments to less than seven since too many elements can be distracting. |
    
    ### How to avoid misleading visualizations
    
    | **What to avoid** | **Why** |
    | --- | --- |
    | Cutting off the y-axis | Changing the scale on the y-axis can make the differences between different groups in your data seem more dramatic, even if the difference is actually quite small. |
    | Misleading use of a dual y-axis | Using a dual y-axis without clearly labeling it in your data visualization can create extremely misleading charts. |
    | Artificially limiting the scope of the data | If you only consider the part of the data that confirms your analysis, your visualizations will be misleading because they don’t take all of the data into account. |
    | Problematic choices in how data is binned or grouped | It is important to make sure that the way you are grouping data isn’t misleading or misrepresenting your data and disguising important trends and insights. |
    | Using part-to-whole visuals when the totals do not sum up appropriately | If you are using a part-to-whole visual like a pie chart to explain your data, the individual parts should add up to equal 100%. If they don’t, your data visualization will be misleading. |
    | Hiding trends in cumulative charts | Creating a cumulative chart can disguise more insightful trends by making the scale of the visualization too large to track any changes over time. |
    | Artificially smoothing trends | Adding smooth trend lines between points in a scatter plot can make it easier to read that plot, but replacing the points with just the line can actually make it appear that the point is more connected over time than it actually was. |
    
    ### Optimize the color palette in data visualization
    
    A diverging color palette displays two ranges of values using color intensity to show the magnitude of the number and the actual color to show which range the number is from. It’s a good way to show the difference between numbers.
    
    The colors you choose should fit within the scope of the audience’s expectations.
    
    ### How to choose a data visualization - Extra Google PDF
    
    https://d3c33hcgiwev3.cloudfront.net/XsaUfemhQ-qGlH3poXPqMg_4b74b6280a7a4a10a83e6b5ca9138630_How-to-choose-a-data-visualization.pdf?Expires=1719360000&Signature=kHXaDtgXv-4NH4YSAgNPcKkWzG-SwFQuUgt60O11tnqNzckm4trZPKF5eDhet7PAK0luSwGgujFqQkXz3exwANNv43MBzzfwPhiceytvkBHv-~Rar3H9GZUsX-uKBcv4UU4VCxnqscZ0gAglvvN9hvaWoG2Rz5keHQDsp267BbA_&Key-Pair-Id=APKAJLTNE6QMUY6HBC5A
    
    Check the guide for a better understanding of how to choose a data visualization.
    
    ### Live Vs Static insights
    
    Static data involves providing screenshots or snapshots in presentations or building dashboards using snapshots of data.
    
    - Can tightly control a point-in-time narrative of the data and insight
    - Allows for complex analysis to be explained in-depth to a larger audience
    - Insight immediately begins to lose value and continues to do so the longer the data remains in a static state
    - Snapshots can't keep up with the pace of data change
    
    Live data means that you can build dashboards, reports, and views connected to automatically updated data. 
    
    - Dashboards can be built to be more dynamic and scalable
    - Gives the most up-to-date data to the people who need it at the time when they need it
    - Allows for up-to-date curated views into data with the ability to build a scalable “single source of truth” for various use cases
    - Allows for immediate action to be taken on data that changes frequently
    - Alleviates time/resources spent on processes for every analysis
    - Can take engineering resources to keep pipelines live and scalable, which may be outside the scope of some companies' data resource allocation
    - Without the ability to interpret data, you can lose control of the narrative, which can cause data chaos (i.e. teams coming to conflicting conclusions based on the same data)
    - Can potentially cause a lack of trust if the data isn’t handled properly
    
    ## DATA-DRIVEN STORYTELLING. 3 DATA STORYTELLING STEPS
    
    Data storytelling is communicating the meaning of a dataset with visuals and a narrative that are customized for each particular audience. Stories make people care.
    
    ### 1. Engage your audience.
    
    Engagement is capturing and holding someone’s interest and attention.
    
    1. Know your audience. Think about how your data project might affect them. What role does this audience play? What is their take in the project? What do they hope to get from the data insights I deliver?
    2. Choose your primary message. To get the key message you need to take a few steps back and pinpoint only the most useful pieces, because not every piece of data is relevant to the questions you’re trying to answer. To do a spotlight, you write each insight from your analysis in a piece of paper, spread them out, and display them in a white board, then you examine it. Look for broad and universal idea messages.
    
    ### 2. Create compelling visuals.
    
    There are key parts of your data story. The narrative you share with your stakeholders needs:
    
    - Characters. People affected by your story.
    - Setting. Describes what’s going on, how often is happening, what tasks are involved, and other background information about the data project that describes the current situation.
    - Plot. Conflict. It’s what creates tension in the current situation.
    - Big reveal. How the data has shown that you can solve the problem the characters are facing by becoming more competitive, improving a process, or whatever the ultimate goal of your data project may be.
    - Aha moment. You share your recommendation and explain why you think they’ll help your company be successful.
    
    ### 3. Tell the story in an interesting narrative.
    
     It should connect the data you’ve collected to the project objective and clearly explain important insights from your analysis.
    
    The presentation reflects on you. So ask yourself: what’s the single most important thing I want my audience to learn from my analysis?
    
    There are a lot of advantages on visuals:
    
    - Visuals help the audience quickly understand the content of each slide.
    - Great visuals don’t leave room for interpretation, because the meaning is instantly understood.
    - When you include visuals on a slide, try no to share too many details all at once, choose just the data points that support your points.
    - If you have several important things you need to include, create a new visual for each point. Then, add an arrow, a call-out, or another clearly labeled element to direct your audience’s attention toward what you want them to look at.
    - When you get to your big reveal and aha moment, your visuals must communicate these messages with clarity and excitement.
    - To ensure your audience is focused on what is being said, rather than reading slides, keep text to fewer than five lines and 25 words per slide.
    
    ## THE ART AND SCIENCE OF PRESENTATIONS
    
    The framework of your presentation starts with your understanding of the business task. When creating a presentation to share with stakeholders, the purposes of a framework are:
    
    - Give your audience context to better understand your data
    - Help you focus on the most important information
    - Create logical connections that tie back to the business task
    
    Does this data point or chart support the point I want people to walk away with?
    
    ### Effective presentation practices
    
    - Include a title, subtitle, and date. To start you should see the title slide: the title, who is presenting, and when it occurred.
    - Use a logical sequence of slides. Organizing your slides in an order that makes sense guides your audience through your narrative, building understanding step by step.
    - Provide an agenda with a timeline.
        
        ![image.png](Google_Data_Analytics_images/image.png)
        
    - Limit the amount of text on slides. Aim for your audience to scan it within 5 seconds.
    - Start with the business task.
    - Establish the initial hypothesis.
    - Show what business metrics you used.
    - Use visualizations.
    - Introduce the graphic by name.
    - Provide a title for each graph.
    - Go from the general to the specific.
    - Use speaker notes to help you remember talking points.
    - Include key takeaways. Summarize the main points at the end of your presentation.
    - Create an evaluation table. It gives you a checklist for each slide in a presentation so you can identify any changes that need to be made in an organized fashion.
        
        
        | **Slide #** | **What works well** | **What could be improved** |
        | --- | --- | --- |
        | 1 |  |  |
        | 2 |  |  |
        | 3 |  |  |
        | 4 |  |  |
    - As a data analyst, you have 2 key responsibilities: Analyze the data, and present your findings effectively.
    
    ### Presentation tips
    
    - Channel your excitement. Take deep, controlled breaths to calm your body down.
    - Start with the broader ideas.
    - Use the five second rule:
        - Wait five seconds after showing a data visualization
        - Ask if they understand it. If not, take time to explain it.
        - Give your audience another five seconds
        - Tell them the conclusion
    - Preparation is key.
    - You audience will not always see the steps you took to reach a conclusion. Focus on what information they need to reach the same conclusion you did.
    - You audience has a lot on their mind, so try to keep your presentation focused and to the point to keep their minds from wandering.
    - Your audience is easily distracted, so try to avoid including information in your presentations that you don’t think will be productive to discussions with your audience.
    - Pay attention on how you speak. Keep your sentences short, build in intentional pauses to give your audience time to think about what you’ve just said, keep the pitch of your sentences level.
    - Be mindful of nervous habits. Stay still and move with purpose, practice good posture, and make positive eye contact.
    
    Example of a great slide deck presentation: [https://docs.google.com/presentation/d/1jyZeBt2PizsVU4KdODvzAnUbcz7CIOq6Udvp0d5_jKs/template/preview?resourcekey=0-2M-Yk3_73NwAVg-PaLfvVA](https://docs.google.com/presentation/d/1jyZeBt2PizsVU4KdODvzAnUbcz7CIOq6Udvp0d5_jKs/template/preview?resourcekey=0-2M-Yk3_73NwAVg-PaLfvVA)
    
    ### Guide: Share data findings in presentations
    
    Sharing your data findings in presentations: Tips & tricks: https://d3c33hcgiwev3.cloudfront.net/_LwuiIoNSYq8LoiKDUmKxw_e8ff903b66b943ddaea3b8517fe8a3af_Sharing-your-data-findings-in-presentations-_-Tips-and-Tricks.pdf?Expires=1726790400&Signature=HFbk68lQOuZQ~8VZWo7SOap8K2izkmc0cPtmFyebw~AHa446EuilgoXTDcPCAMgH7omQKwGNkcVaHJ~psA1cBcCN3f5zz51BrAT7wPSEjxKsKrHf5os92Tu3ZvfXnEFQaIML6T1Z9md~wdEz4orq1c02LFIHFXo41wVKWT3yFrM_&Key-Pair-Id=APKAJLTNE6QMUY6HBC5A
    
    ## DATA CAVEATS AND LIMITATIONS
    
    The checklist below identifies 10 tasks that you should engage in to be prepared for your Q&A:
    
    Before the presentation
    
    1. Assemble and prepare your questions.
    2. Discuss your presentation with your manager, other analysts, or other friendly contacts in your organization.
    3. Ask a manager or other analysts what sort of questions were normally asked by your specific audience in the past.
    4. Seek comments, feedback, and questions on the deck or the document of your analysis.
    5. At least 24 hours ahead of the presentation, try and brainstorm tricky questions or unclear parts you may come across- this helps avoid surprises.
    6. It never hurts to practice what you will be presenting, to account for any missing information or simply to calm your nerves.
    
    During the presentation
    
    1. Be prepared to respond to the things that you find and effectively and accurately explain your findings.
    2. Address potential questions that may come up.
    3. Avoid having a single question derail a presentation and propose following-up offline.
    4. Put supplementary visualizations and content in the appendix to help answer questions.
    
    Be prepared to consider any limitations of your data by:
    
    - Critically analyzing the correlations.
    - Looking at the context
    - Understanding the strengths and weakness of the tools.
    
    ### Tips to handle objections
    
    Usually, these objections are:
    
    - About the data. You can include this information in the beginning of your presentation to set up the data context.
        - Where you got the data?
        - What systems it came from?
        - What transformations happened to it?
        - How fresh and accurate is the data?
    - Your analysis.
        - Is your analysis reproducible?
        - Who did you get feedback from?
    - Your findings
        - Do these findings exist in previous time periods?
        - Did you control for the differences in your data?
    
    Responding to possible objections:
    
    - Communicate any assumptions.
    - Explain why your analysis might be different than expected.
    - acknowledge that those objections are valid and take steps to investigate further.
    - Take steps to investigate further
    
    ### Q&A best practices
    
    - Listed to  the whole question.
    - Repeat the question (if necessary)
    - Use the Appendix. It’s a great place to keep extra information that might not be necessary for our presentation but could be useful for answering questions afterwards.
    - Understand the context.
    - Involve the whole audience.
    - Keep your responses short and to the point.
    
    ### Important aspects to a presentation
    
    - Define your purpose
    - Keep it concise
    - Have some logical flow to your presentation
    - Make the presentation visually compelling
    - How easy is it to understand?
    
- 7 - DATA ANALYSIS WITH **R** PROGRAMMING
    
    **Account at the RStudio Cloud** [Cloud sign-up page](https://rstudio.cloud/plans/free)
    
    The R programming language is useful for organizing, cleaning, and analyzing data.
    
    Programming helps you to:
    
    - Clarify the steps of your analysis
    - Save time
    - Reproduce and share your work
    
    R is a data-centric programming language frequently used for statistical analysis, visualization, and other data analysis. It’s based on another programming language named S.
    
    You can use R in many specific situations, like:
    
    - Reproducing your analysis
    - Processing lots of data
    - Creating data visualizations
    
    ### The R-versus-Python debate
    
    | Languages | R | Python |
    | --- | --- | --- |
    | **Common 
    features** | Open-source. Data stored in data frames. Formulas and functions readily available. Community for code development and support | Open-source. Data stored in data frames. Formulas and functions readily available. Community for code development and support |
    | **Unique advantages** | Data manipulation, data visualization, and statistics packages. "Scalpel" approach to data: *find packages to do what you want with the data* | Easy syntax for machine learning needs. Integrates with cloud platforms like Google Cloud, Amazon Web Services, and Azure |
    | **Unique challenges** | Inconsistent naming conventions. Make it harder for beginners to select the right functions. Methods for handling variables may be a little complex for beginners to understand | Many more decisions for beginners to make about data input/output, structure, variables, packages, and objects. "Swiss army knife" approach to data: *figure out a way to do what you want with the data* |
    
    R has been used by professionals who have a statistical or research-oriented approach to solving problems; among them are scientists, statisticians, and engineers. Python has been used by professionals looking for solutions in the data itself, those who must heavily mine data for answers; among them are data scientists, machine learning specialists, and software developers.
    
    ### Spreadsheets, SQL, and R: a comparison
    
    | **Key question** | **Spreadsheets** | **SQL** | **R** |
    | --- | --- | --- | --- |
    | **What is it?** | A program that uses rows and columns to organize data and allows for analysis and manipulation through formulas, functions, and built-in features | A database programming language used to communicate with databases to conduct an analysis of data | A general purpose programming language used for statistical analysis, visualization, and other data analysis |
    | **What is a primary advantage?** | Includes a variety of visualization tools and features | Allows users to manipulate and reorganize data as needed to aid analysis | Provides an accessible language to organize, modify, and clean data frames, and create insightful data visualizations |
    | **Which datasets does it work best with?** | Smaller datasets | Larger datasets | Larger datasets |
    | **What is the source of the data?** | Entered manually or imported from an external source | Accessed from an external database | Loaded with R when installed, imported from your computer, or loaded from external sources |
    | **Where is the data from my analysis usually stored?** | In a spreadsheet file on your computer | Inside tables in the accessed database | In an R file on your computer |
    | **Do I use formulas and functions?** | Yes | Yes | Yes |
    | **Can I create visualizations?** | Yes | Yes, by using an additional tool like a database management system (DBMS) or a business intelligence (BI) tool | Yes |
    
    ## R AND RSTUDIO
    
    Most analyst who work with the R language use the RStudio enviroment to interact with R, and not the basic interface. R is an IDE, this means that RStudio brings together all the tools you might want to use in a single place.
    
    Packages are units of reproducible R code. Members of the R community create packages to keep track of the R functions that they write and reuse. Packages offer a helpful combination of code, reusable R functions, descriptive documentation, tests for checking your code, and sample data sets. We will install and load the core *tidyverse* packages.
    
    ```r
    install.packages("tidyverse")
    ```
    
    To load the cores:
    
    ```r
    library(tidyverse)
    library(lubridate) #This is already part of the tidyverse package.
    ```
    
    - The RStudio enviroment has four main windows called panes. One of them is hidden (File > New File > R Script).
        
        ![image.png](Google_Data_Analytics_images/image%201.png)
        
        - Console: the place where you give commands to R.
        - Source editor: when working with R Scripts. If you write code directly in the R source editor, RStudio can save your code when you close your current session.
        - Environment: you’ll find all the data you currently have loaded and can easily organize and save it.
        - A pane that has tabs for Files, Plots, Packages, and Help.
    - R is key sensitive.
    
    ### The basic concepts of R
    
    The assignment operator is
    
    ```r
    <-
    ```
    
    Factors (R) store categorical data in R where the data values are limited and usually based on a finite group like country or year.
    
    - Functions. A body of reusable code used to perform specific tasks in R. Functions are key sensitive.
        - Argument (R): Information that a function in R needs in order to run
    - Comments. Helpful when you want to describe or explain what’s going on in your code. We use a #.
    - Variables. A representation of a value in R that can be stored for use later during programming. They can also be called objects.
    - Data types. Numeric, alphabetic, logical, data, and date time.
    - Vectors. A group of data elements of the same type stored in a sequence in R.
        
        ```r
        #c() function (called the "combine" function).
        vec_1 <- c(10,5,0.26,20,48.5)
        integer_vector <- c(1L, 5L, 15L) #must place the "L" after each number
        ```
        
    - Pipes. A tool in R for expressing a sequence of multiple operations, represented with “%>%”. It’s used to apply the output of one function into another function
    
    A data structure is a format for organizing and storing data. The most common data structures in the R programming language include:
    
    - Vectors
    - Data frames. The most common way of storing and analyzing data in R.
    - Matrices
    - Arrays
    
    ### Vectors in R
    
    Every vector you create will have two key properties: type and length. You can determine what type of vector you are working with by using:
    
    ```r
    typeof()
    typeof(c("a","b"))
    ```
    
    You can determine the length of an existing vector by using:
    
    ```r
    length()
    x <- c(33.5, 57.75, 120.05)
    length(x) #The result is 3
    ```
    
    You can use the next function to assign a different name to each element of the vector
    
    ```r
    names()
    x <- c(1, 3, 5)
    names(x) <- c("a", "b", "c")
    ```
    
    There are 2 types of vectors:
    
    - Atomic vectors. There are six primart types of atomic vectors:
        - Logical. True/False.
        - Integer. Positive and negative whole values. Known as numeric vectors
        - Double. Decimal values. Known as numeric vectors
        - Character (which contains strings). String/Character values
        - Complex. Not that common.
        - Raw. Not that common
        
        ![image.png](Google_Data_Analytics_images/image%202.png)
        
    - Lists
    
    ### Lists in R
    
    Lists are different from atomic vectors because their elements can be of any type.
    
    You can create a list with:
    
    ```r
    list()
    list("a", 1L, 1.5, TRUE)
    ```
    
    You can find out what types of elements a list contains, and the number of elements using:
    
    ```r
    str()
    z <- list(list(list(1 , 3, 5)))
    str(z)
    #output:
    	#> List of 1
    	#>  $ :List of 1
    	#>   ..$ :List of 3
    	#>   .. ..$ : num 1
    	#>   .. ..$ : num 3
    	#>   .. ..$ : num 5
    ```
    
    The indentation of the $ symbols reflect the nested structure of this list. Here, there are three levels (so there is a list within a list within a list).  
    
    Lists, like vectors, can be named. You can name the elements of a list when you first create it with the list() function:
    
    ```r
    list('Chicago' = 1, 'New York' = 2, 'Los Angeles' = 3)
    
    $`Chicago`
    
    [1] 1
    
    $`New York`
    
    [1] 2
    
    $`Los Angeles`
    
    [1] 3
    ```
    
    ### Dates and times in R
    
    Before you get started working with dates and times, you should load both tidyverse and lubridate. Lubridate is part of tidyverse. 
    
    ```r
    install.packages("tidyverse")
    library(tidyverse) #Loading tidyverse package
    library(lubridate) #Loading lubridate package
    ```
    
    In R, there are three types of data that refer to an instant in time:
    
    - A date ("2016-08-16"). R creates dates in the standard yyyy-mm-dd format by default.
    - A time within a day (“20:11:59 UTC")
    - And a date-time. This is a date plus a time ("2018-03-31 18:15:48 UTC")
    
    Some useful functions:
    
    ```r
    today() #To get the current date
    now() #To get the current date-time
    ymd("2021-01-20") #To convert a string into dates. It will return as a yyyy-mm-dd format
    mdy("January 20th, 2021") #To convert a string into dates. It will return as a yyyy-mm-dd format
    dmy("20-Jan-2021") #To convert a string into dates. It will return as a yyyy-mm-dd format
    mdy_hm("01/20/2021 08:01") #To convert a string into datetime
    	#> [1] "2021-01-20 08:01:00 UTC"
    as_date() #To convert a date-time to a date.
    ```
    
    ### Data Frames
    
    It’s a collection of columns containing data, similar to a spreadsheet or SQL table. Each column has a name that represents a variable and includes one observation per row. Data frames summarize data and organize it into a format that is easy to read and use. 
    
    Data Frames and tibbles are the building blocks for analysis in R.
    
    ```r
    data.frame() #To manually create a data frame.  
    data.frame(x = c(1, 2, 3) , y = c(1.5, 5.5, 7.5)) #x and y are the columns
    z <- data.frame(x = c(1, 2, 3) , y = c(1.5, 5.5, 7.5))
    z[2,1] #Extract operator to extract a subset from a data frame.
    	#[1] 2
    ```
    
    - The data.frame() function takes vectors as input.
    - When you use the extract operator [] on a data frame, it takes two arguments: the row(s) and column(s) you’d like to extract, separated by a comma.
    
    There are many ways to create a Date Frame. One of the most common is to create individual vectors of data and then combine them into a data frame using the `data.frame()` function.
    
    ```r
    #Example of Creating a DataFrame manually:
    names <- c("Peter", "Jennifer", "Julie", "Alex") #First, create a vector of names
    age <- c(15, 19, 21, 25) #Then create a vector of ages
    people <- data.frame(names, age) #With these two vectors, you can create a new data frame called `people`.
    ```
    
    There are some important things to know about DataFrames:
    
    - Columns should be named.
    - Data stored can be many different types, like numeric, factor, or character.
    - Each column should contain the same number of data items.
    
    ```r
    str() #To see the information about the DataFrame structure
    colnames() #To see the column names of the DataFrame
    mutate(*DataFrame*, *NewColumnName=Values*) #To make changes to the DataFrame. Part of the dplyr package.
    ```
    
    ### Tibbles
    
    Tibbles are like streamlined data frames that are automatically set to pull up only the first 10 rows of a dataset, and only as many columns as can fit on the screen. In the tidyverse, they:
    
    - Never change the data types of the inputs.
    - Never change the names of your variables.
    - Never create row names
    - Make printing in R easier. They’re automatically set to pull up only the first 10 rows and as many columns as fit on screen.
    
    ```r
    as_tibble() #To create a tibble from existing data
    ```
    
    ### Files
    
    ```r
    file.create() #To create a  blank file
    file.create("new_text_file.txt") 
    file.create("new_word_file.docx") 
    file.create("new_csv_file.csv") 
    file.copy("new_text_file.txt", "destination_folder") #To copy a file
    unlink("some_.file.csv") #To delete R files
    
    bookings_df <- read_csv("hotel_bookings.csv") #Example of the readr package.
    new_df <- select(bookings_df, `adr`, adults) #To create a new DF using specific columns from another DF
    ```
    
    - If the file is successfully created when you run the function, R will return a value of TRUE. Otherwise, R will return a value of FALSE.
    
    ### Matrices
    
    A matrix is a two-dimensional collection of data elements. This means it has both rows and columns. Matrices can only contain a single data type.
    
    ```r
    matrix(c(3:8), nrow = 2) #To create a matrix
    #Output>
         [,1] [,2] [,3]
    [1,]    3    5    7
    [2,]    4    6    8
    matrix(c(3:8), ncol = 2)
    #Output>
         [,1] [,2]
    [1,]    3    6
    [2,]    4    7
    [3,]    5    8
    ```
    
    - Matrix() function has two main arguments:
        - a vector
        - atleast one matrix dimension. You can choose to specify the number of rows or the number of columns by using the code nrow = or ncol =.
    
    ### Operators
    
    An operator is a symbol that identifies the type of operation or calculation to be performed in a formula. There are different types of operators:
    
    - Assignment operators: Used to assign values to variables and vectors.
        - “<-”, “<<-”, “=”. Leftwards assignment
        - “->”, “->>”. Rightwards assignment
    - Arithmetic operators: Used to complete math calculations. “+”, ”-”, “*”, “/”, “%%” (modulus, returns the remainder after division), “%/%” (integer, returns an integer value after division), “^” (Exponent).
    - Relational operators: also known as comparators, allow you to compare values. “<”, “>”, “!=”, “==”,…
    - Logical operators: Return a logical data type such as TRUE or FALSE. There are three primary types of logical operators (Zero is considered FALSE and non-zero numbers are taken as TRUE):
        - AND (sometimes represented as & or && in R)
        - OR (sometimes represented as | or || in R)
        - NOT (!). !TRUE evaluates to FALSE, and !FALSE evaluates to TRUE.
    - Conditional statement. A declaration that if a certain condition holds, then a certain event must take place.
        - if()
        - else if()
        - else()
    
    ### Packages (R)
    
    Units of reproducible R code. By default, R included a set of packages called base R. Packages include:
    
    - Reusable R functions
    - Documentation about the functions
    - Sample datasets
    - Tests for checking your code.
    
    We need to load our package first with a library command.
    
    You can find repositories on **[Bioconductor](http://bioconductor.org/), [R-Forge](https://r-forge.r-project.org/),  [rOpenSci](https://ropensci.org/)** or **[GitHub](https://github.com/),** but the most commonly used repository is the Comprehensive R Archive Network or [**CRAN**](https://cran.r-project.org/). CRAN stores code and documentation so that you can install packages into your own RStudio space. CRAN makes sure any R content open to the public meets the requires quality standards.
    
    The most useful package is tidyverse. Tidyverse is a system of packages in R with a common design philosophy for data manipulation, exploration, and visualization. The core tidyverse packages are (the first four are an essential part of the workflow for data analysts):
    
    - ggplot2. Create a variety of data viz by applying different visual properties to the data variables in R. Specifically plots.
    - tidyr. Used for data cleaning to make tidy data.
    - readr. Used for importing data. To accurately read a dataset you combine the function with a column specification, which describes how each column should be converted to the most appropriate data type. This isn’t usually necessary.
        - The readr package in R is a great tool for reading rectangular data. Rectangular data is data that fits nicely inside a rectangle of rows and columns, with each column referring to a single variable and each row referring to a single observation.
    - dplyr. Offers a consistent set of functions that help you complete some common data manipulations tasks.
        
        ```r
        data %>%
            filter(variable1 == "DS") %>%  
            ggplot(aes(x = weight, y = variable2, colour = variable1)) +  
            geom_point(alpha = 0.3,  position = position_jitter()) + stat_smooth(method = "lm")
        ```
        
    - tibble. Works with DataFrames
    - purrr. Works with functions and vectors, helping make your code easier to write and more expressive
    - stringr. Includes functions that make it easier to work with strings.
    - forcats. Provides tools that solve common problems with factors.
    
    ![image.png](Google_Data_Analytics_images/image%203.png)
    
    Conflicts happen when packages have functions with the same names as other functions. Conflict notifications are just one type of message that can show up in the console.
    
    Another useful packages are:
    
    - “here”. Makes referencing files easier.
    - “Skimr”. Simplify data cleaning tasks. Makes summarizing data really easy and let’s you skim through it more quickly.
    - “Janitor”. Simplify data cleaning tasks. It has functions for cleaning data.
    
    ### Pipes
    
    A tool in R that helps make your code more efficient and easier to read and understand. A tool in R for expressing a sequence of multiple operations, represented with “%>%”. It’s used to apply the output of one function into another function. This is described as nested.
    
    Nested in programming, describes code that performs a particular function and is contained within code that performs a broader function. A nested function is a function that is completely contained within another function.
    
    ![image.png](Google_Data_Analytics_images/image%204.png)
    
    Now we used a pipe:
    
    ![image.png](Google_Data_Analytics_images/image%205.png)
    
    Important things to keep in mind when using pipes:
    
    - Add the pipe operator at the end of each line of the piped operation except the last one.
    - Check your code after you’ve programmed your pipe.
    - Revisit piped operations to check for parts of your code to fix.
    
    ### Exploring and Cleaning Data in R
    
    Cleaning functions help you preview and rename data so that it’s easier to work with.
    
    Consistent data structures like Data Frames make it easier to operate on an entire dataset. Tidy data refers to the principles that make data structures meaningful and easy to understand. The tidy data standards are:
    
    - Variables are organized into columns.
    - Observations are organized into rows.
    - Each value must have its own cell.
    
    ```r
    data() #To load a specific dataset
    ```
    
    Some of the most important packages to clean data are “head”, “skimr” and “Janitor”.
    
    - Functions to get summaries of our DataFrame:
        - skim_without_charts(). Gives us a comprehensive summary of a dataset.
        - glimpse(). Show us a summary of the data.
        - head(). Get a preview of the column names and the first few rows of the dataset.
        - select(). To specify certain columns, or to exclude columns we don’t need right now. It’s useful for pulling just a subset of variables from a large dataset.
    - Functions to modify the dataset/DataFrame:
        
        ```r
        #"penguins" is the name of our Dataset
        penguins %>% rename(island_new=island)
        rename_with(penguins, tolower) #Makes all column name lowercase
        clean_names(penguins)
        ```
        
        - rename(). Makes it easy to change column names.
        - rename_with(). Can change column names to be more consistent. It can reformat column names to be upper or lower case.
        - clean_names(). This ensure there is only characters, numbers, and underscores in the names.
    
    ### Organizing your data in R
    
    Organizational functions help you sort, filter, and summarize your data.
    
    All packages we’ll need here are part of the core tidyverse
    
    ```r
    penguins %>% arrange(bill_length_mm) #Data sorted by bill_length_mm column in ascending order
    penguins %>% arrange(-bill_length_mm) #Data sorted by bill_length_mm column in descending order
    penguins 
    	%>% group_by(island) #groups data
    	%>% drop_na() #Addresses any missing values in our dataset.
    	%>% summarize(mean_bill_length_mm = mean(bill_length_mm)) #provides the mean of the column
    penguins %>% filter(species == "Adelie")
    ```
    
    ![image.png](Google_Data_Analytics_images/image%206.png)
    
    ![image.png](Google_Data_Analytics_images/image%207.png)
    
    - arrange(). To choose which variable we want to sort by.
    - group_by(). It’s usually combined with other functions.
    - filter().
    - summarize(). lets us get high level information about our data.
    
    ```r
    penguins %>% arrange(*DF_name,* bill_length_mm) #Data sorted by bill_length_mm column in ascending order
    penguins %>% arrange(*DF_name,* -bill_length_mm) #Data sorted by bill_length_mm column in descending order
    penguins %>% arrange(desc(bill_length_mm)) #Data sorted by bill_length_mm column in descending order
    penguins 
    	%>% group_by(island) #groups data
    	%>% drop_na() #Addresses any missing values in our dataset.
    	%>% summarize(mean_bill_length_mm = mean(bill_length_mm)) #provides the mean of the column
    penguins %>% filter(species == "Adelie")
    ```
    
    When organizing or tidying your data using R, you might need to convert wide data to long data or long to wide. For that, we use    
    
    - Wide data has observations across several columns. Each column contains data from a different condition of the variable. We use `pivot_wider()` to have more columns and fewer rows.
        
        ![image.png](Google_Data_Analytics_images/image%208.png)
        
    - Long data has all the observations in a single column, and the variable conditions are placed into separate rows. We use `pivot_longer()` to have more rows and fewer columns.
        
        ![image.png](Google_Data_Analytics_images/image%209.png)
        
    
    ### Transforming data in R
    
    Transformational functions help you separate and combine data, as well as create new variables.
    
    ```r
    #"employee" is the name of our Data Frame.
    separate(employee, name, into=c('first_name','last_name'), sep=' ')
    unite(employee,'name', first_name,last_name, sep=' ')
    ```
    
    - separate(). Splits one column into separate columns.
    - unite(). Allows us to merge columns together. It does the opposite of separate().
    - mutate(). Create new columns on our DataFrame.
    
    ### The bias function
    
    In R, we can actually quantify bias by comparing the actual outcome
    of our data with the predicted outcome. There's a pretty complicated statistical explanation behind this. But with the bias function in R, we don't have to perform this calculation by hand. Basically the bias function finds the average amount that the actual outcomeis greater than the predicted outcome. It's included in the sim design package.
    
    If the model is unbiased, the outcome should be pretty close to zero. A high result means that your data might be biased. A good thing to know before you analyze it.
    
    ![image.png](Google_Data_Analytics_images/image%2010.png)
    
    You can also use the “sample()” function to find bias. This function allows you to take a random sample of elements from a data set.
    
    ### Visualizations in R
    
    Some of the most popular packages for visualizations are:
    
    - ggplot2
    - Plotly
    - Lattice
    - RGL
    - Dygraphs
    - Leaflet
    - Highcharter
    - Patchwork
    - gganimate
    - ggridges
    
    We’ll use mainly ggplot2. The benefits of using this package are:
    
    - Create different types of plots
    - Customize the look and feel of plots. You can change the colors, layout, and dimensions of your plots and add text elements.
    - Create high quality visuals with just a little bit of code.
    
    The core concepts in ggplot2 are:
    
    - Aesthetics. A visual property of an object in your plot (position, color, shape, or size).
    - Geoms. The geometric object used to represent your data (lines, points, bars, etc)
    - Facets. Let you display smaller groups, or subsets, of your data.
    - Labels and annotations. Let you customize your plot.
    
    To create a plot, follow the next steps:
    
    1. Start with the ggplot function and choose a dataset to work with
    2. Add a geom_function to display your data
    3. Map the variables you want to plot in the arguments of the aes() function
    
    ![image.png](Google_Data_Analytics_images/image%2011.png)
    
    ```r
    ggplot(data = penguins) + geom_point(mapping = aes(x = flipper_length_mm, y = body_mass_g, shape=species, color=species, size=species))
    ggplot(data = penguins, mapping = aes(x = flipper_length_mm, y = body_mass_g)) +  geom_point()
    ```
    
    - The ggplot() function creates a coordinate system that you can add layers to.
    - Then, you add a “+” symbol to add a new layer to your plot. You complete your plot by adding one or more layers to ggplot().
    - Next, you choose a geom by adding a geom function. The geom_point() function uses points to create scatterplots, the geom_bar function uses bars to create bar charts, and so on.
    - Mapping in R means matching up a specific variable in your dataset with a specific aesthetic. Each geom function in ggplot2 takes a mapping argument. This defines how variables in your dataset are mapped to visual properties.
    - The mapping argument is always paired with the aes() function. This two tells R what aesthetics to use for the plot. You use the “aes” function to define the mapping between your data and your plot.
    - We can map more than one aesthetic to the same variable. Basic aesthetics for points are:
        - X
        - Y
        - Color. Change the color of all of the points on your plot, or the color of each data group.
        - Shape. Change the shape of the points on your plot by data group.
        - Size. Change the size of the points on your plot by data group.
        - Alpha. is a good option whe you’ve got a dense plot with lots of data points. This will make some points more transparent or see-through than others.
    - You can write the same section of code above using a different syntax with the mapping argument inside the ggplot() call
    - To change the appearance of our overall plot without regard to specific variables, we need to write the code outside of the aes function and quotation marks for the value, because all the code inside of the aes function tells R how to map aesthetics to variables.
        
        ```r
        ggplot(data = penguins) + geom_point(mapping = aes(x = flipper_length_mm, y = body_mass_g),color="purple")
        ```
        
    - ggplot [Cheat Sheet](https://ggplot2.tidyverse.org/)
    
    ### Geoms
    
    We use Geom functions for some graphics.
    
    ```r
    ggplot(data=penguins)+geom_smooth(mapping=aes(x=flipper_length_mm,y=body_mass_g))+geom_point(mapping=aes(x=flipper_length_mm,y=body_mass_g))
    ggplot(data=penguins)+geom_smooth(mapping=aes(x=flipper_length_mm,y=body_mass_g))+geom_jitter(mapping=aes(x=flipper_length_mm,y=body_mass_g))
    ggplot(data=diamonds)+geom_bar(mapping=aes(x=cut,fill=cut))
    ```
    
    - geom_point. Uses points to create scatter plots
    - geom_bar. Uses bars to create bar charts. We don’t need a variable for the y-axis. R automatically counts how many times each X value appears in the data and then shows the counts on the y-axis. The default is to count rows.
        - The “color” aesthetic adds color to the outline of each bar.
        - The “fill” aesthetic adds color to the inside of each bar. If we map it to a different variable to X, it will display a stacked bar chart.
    - geom_line. Uses lines to create a line chart.
    - geom_smooth. It’s useful for showing general trends in our data. It adds a smoothing line as another layer to a plot. There are 2 types of smoothing:
        - Loess: best for smoothing plots with less than 1000 points.
        - Gam: o generalized additive model smoothing, is useful for smoothing plots with a large number of points.
    - geom_jitter. It creates a scatter plot and then adds a small amount of random noise to each point in the plot. Jittering helps us deal with over-plotting, which happens when the data points in a plot overlap with each other.
    
    Some characteristics are:
    
    - We can use two geoms in the same plot, adding a + symbol after the first geom function.
    
    ### Facets
    
    A facet is a side or section of the object
    
    - It shows different sides of your data, by placing each subset on its own plot.
    - Faceting can help you discover new patterns in your data, and focus on relationships between different variables.
    - tilde operator (~) helps to define that dependent variable depends on the independent variable(s) that are on the right-hand side of tilde operator.
    
    There are 2 functions for faceting:
    
    ```r
    #the independent variable at the end of the syntax line would be species, and the dependent variable in this case is facet_wrap. 
    ggplot(data=penguins)+geom_point(mapping=aes(x=flipper_length_mm,y=body_mass_g,color=species))+facet_wrap(~species)
    #the independent variable at the end of the syntax line would be species, and the dependent variable in this case is sex.
    ggplot(data=penguins)+geom_point(mapping=aes(x=flipper_length_mm,y=body_mass_g,color=species))+facet_grid(sex~species)
    ```
    
    - facet_wrap(). Facet your plot by a single variable.
    - facet_grid(). Facet your plot with two variables. It’ll split the plot into facets vertically by the values of the first variable, and horizontally bt the facets of the second variable.
    
    ### Annotate and Label
    
    Label function is useful for adding informative labels to a plot such as
    
    - titles
    
    ```r
    ggplot(data=penguins)+
    	geom_point(mapping=aes(x=flipper_length_mm,y=body_mass_g,color=species))+
    		labs(title="Palmer Penguins: Body Mass vs. Flipper Length")
    ```
    
    - Subtitles
    
    ```r
    ggplot(data=penguins)+
    	geom_point(mapping=aes(x=flipper_length_mm,y=body_mass_g,color=species))+
    		labs(title="Palmer Penguins: Body Mass vs. Flipper Length", subtitle="Sample of Three Penguin Species")
    ```
    
    - Captions. Let us show the source of our data
    
    ```r
    ggplot(data=penguins)+
    	geom_point(mapping=aes(x=flipper_length_mm,y=body_mass_g,color=species))+
    		labs(title="Palmer Penguins: Body Mass vs. Flipper Length", subtitle="Sample of Three Penguin Species", caption="Data collected by Dr.Kristen Gorman")
    ```
    
    Annotate is to add notes to a document or diagram to explain or comment upon it. Adding annotation to your plot can help explain the plot’s purpose, or highlight important data. 
    
    ```r
    ggplot(data=penguins)+
    	geom_point(mapping=aes(x=flipper_length_mm,y=body_mass_g,color=species))+
    		labs(title="Palmer Penguins: Body Mass vs. Flipper Length", subtitle="Sample of Three Penguin Species", caption="Data collected by Dr.Kristen Gorman")+
    		annotate("text",x=220,y=3500,label="The Gentoos are the largest")
    ```
    
    - You can change the color, the font, angle or the size of the text
    
    ```r
    ggplot(data=penguins)+
    	geom_point(mapping=aes(x=flipper_length_mm,y=body_mass_g,color=species))+
    		labs(title="Palmer Penguins: Body Mass vs. Flipper Length", subtitle="Sample of Three Penguin Species", caption="Data collected by Dr.Kristen Gorman")+
    		annotate("text",x=220,y=3500,label="The Gentoos are the largest",color="purple",fontface="bold", size=4.5,angle=45)
    ```
    
    ### Saving your visualizations
    
    You will use the Export option on the plots tab, or the ggsave() function, provided by the ggplot2 package. 
    
    When you make a plot in R, it has to be “sent” to a specific graphics device. To save images without using ggsave(), you can open an R graphics device like **png()** or **pdf()**; these will allow you to save your plot as a .png or .pdf file. You can also choose to print the plot and then close the device using **dev.off()**.
    
    ![image.png](Google_Data_Analytics_images/image%2012.png)
    
    ggsave() function defaults to saving the last plot that you displayed, and use the size of the current graphics device.
    
    ```r
    ggsave("Three Penguin Species.png")
    ```
    
    ## DOCUMENTATION AND REPORTS IN R
    
    ### R Makrdown
    
    It’s a file format for making dynamic documents with R. It lets you create a record of your analysis and conclusions in a document.
    
    Besides text, R Markdown also includes an interactive option called an R notebook, that lets users run your code and show the graphs and charts that visualize the code.
    
    Any Markdown document can be used as a notebook. You can create:
    
    - HTML, PDF, and Word documents
    - Slite presentation
    - Dashboard
    
    Other notebook options are:
    
    - Jupyter
    - Kaggle
    - Google Colab
    
    To install the Markdown package:
    
    ```r
    install.packages("rmarkdown")
    ```
    
    - We will use the HTML default mode
    - Click on “Knit” to produce a report containing all text, code and results converted to  HTML, PDF or Word file.
    
    There are two shortcuts to adding code. On your keyboard, you can press **Ctrl** + **Alt** + **I** (PC) or **Cmd** + **Option** + **I** (Mac). Or you can click the **Add Chunk** command in the editor toolbar:
    
    ![image.png](Google_Data_Analytics_images/image%2013.png)
    
    A code chunk is code added in an .Rmd file.
    
    If you need to create a certain type of document over and over, or you want to customize the appearance of your final report, you can create a template.
    
- 8- CAPSTONE PROJECT
    
    What’s important here is to show off your thought process so that hte interviewers can understand how you approach the problem.
    
    Things to keep in mind in your Portfolio:
    
    - Make sure your case study answers the question being asked
    - Make sure that you’re communicating the steps you’ve taken and the assumptions you’ve made.
    - The best portfolios are personal, unique, and simple
    - Make sure that your portfolio is relevant and presentable
    
    ### What to include in your portfolio
    
    - Biography. Write a concise and clear introduction of yourself. The goal is to capture your audience’s interest and compel them to want to meet you to learn more.
    - Contact page.
    - Resume.
    - Accomplishments.
    - An image of you (optional).
    
    ### What to include in a case study
    
    - Introduction. State the purpose of the case study. This includes what the scenario is and an explanation on how it relates to a real-world obstacle.
    - Problems. You need to identify what the major problems are, explain how you have analyzed the problem, and present any facts you are using to support your findings.
    - Solutions. Outline a solution that would alleviate the problem and have a few alternatives in mind to show that you have given the case study considerable thought. Don’t forget to include pros and cons for each solution.
    - Conclusion. End your presentation by summarizing key takeaways of all of the problem-solving you conducted, highlighting what you have learned from this.
    - Next steps. Choose the best solution and propose recommendations for the client or business to take. Explain why you made your choice and how this will affect the scenario in a positive way. Be specific and include what needs to be done, who should enforce it, and when.
    
    ## CASE STUDY 1: HOW DOES A BIKE-SHARE NAVIGATE SPEEDY SUCCESS?
    
    ### Deliverable
    
    - A clear statement of the business task (**ASK**)
    - A description of all data sources used (**PREPARE**)
    - Documentation of any cleaning or manipulation of data (**PROCESS**)
    - A summary of your analysis (**ANALYZE**)
    - Supporting visualizations and key findings (**SHARE**)
    - Your top three recommendations based on your analysis (**ACT**)
    
    ### Roadmap
    
    1. **Ask.** How do annual members and casual riders use Cyclistic bikes defferently? 
    
    | Guiding questions | Answers | Key Tasks |
    | --- | --- | --- |
    | What is the problem you are trying to solve? |  | Identify the business task |
    | How can your insights drive business decisions? |  | Consider key Stakehorlders |
    1. **Prepare.** We’ll download the previous 12 months of Cyclistic trip data [here](https://divvy-tripdata.s3.amazonaws.com/index.html) (From January to December, 2023).
    
    | Guiding questions | Answers | Key Tasks |
    | --- | --- | --- |
    | Where is your data located? |  | Download data and store it appropriately |
    | How is the data organizes? |  | Identify how it’s organized |
    | Are there issues with bias or credibility in this data? |  | Sort and filter the data |
    | How are you addressing licensing, privacy, security, and accessibility? |  | Determine the credibility of the data |
    | How did you verify the data’s integrity? |  |  |
    | How does it help you answer your question? |  |  |
    | Are there any problems with the data? |  |  |
    1. **Process**.
    
    | Guiding questions | Answers | Key Tasks |
    | --- | --- | --- |
    | What tools are you choosing and why? |  | Check the data for errors |
    | Have you ensured your data’s integrity? |  | Choose your tools |
    | What steps have you taken to ensure that your data is clean? |  | Transform the data so you can work with it effectively |
    | How can you verify that your data is clean and ready to analyze? |  | Document the cleaning process |
    | Have you documented your cleaning process so you can review and share those results? |  |  |
    
    *** Threre are some useful steps to follow on the guide for this ***
    
    1. **Analyze**.
    
    | Guiding questions | Answers | Key Tasks |
    | --- | --- | --- |
    | How should you organize your data to perform analysis on it? |  | Aggegate your data so it’s useful and accessible |
    | Has your data been properly formatted? |  | Organize and format your data |
    | What surprises did you discover in the data? |  | Perform calculations |
    | What trends or relationships did you find in the data? |  | Identify trends and relationships |
    | How will these insights help answer your business quesitons? |  |  |
    
    *** Threre are some useful steps to follow on the guide for this ***
    
    1. **Share**. 
    
    | Guiding questions | Answers | Key Tasks |
    | --- | --- | --- |
    | Were you able to answer the question of how anual members and casual riders use Cyclistic bikes differently? |  | Determine the best way to share your findings |
    | What story does your data tell? |  | Create effective data visualizations |
    | How do your findings relate to your original question? |  | Present your findings |
    | Who is your audience? What is the best way to communicate with them? |  | Ensure your work is accessible |
    | Can data visualization help you share your findings? |  |  |
    | Is your presentation accessible to your audience? |  |  |
    
    *** Threre are some useful steps to follow on the guide for this ***
    
    1. **Act**.
    
    | Guiding questions | Answers | Key Tasks |
    | --- | --- | --- |
    | What is your final conclusion based on your analysis? |  | Create your portfolio |
    | How could your team and business apply your insights? |  | Add your case study |
    | What next steps would you or your stakeholders take based on your findings? |  | Practice presenting your case study to a friend or family member |
    | Is there additional data you could use to expand on your finidings? |  |  |
    
    *** Threre are some useful steps to follow on the guide for this ***
    
    ### Shate your portfolio
    
    When you’re thinking about where you want to share your portfolio, there’s two questions that can help you decide:
    
    1. What platforms align with your interests and passions?
    2. Where do you want to spend more time after this program?
    
    There are different options where you can share your portfolio:
    
    - Kaggle. Great option if you enjoy connecting with other data analysts
        - Broad data science community
        - Hosts a lot of competitions for users to join in
        - Offers all kinds of learning opportunities
    - GitHub.
        - Primarily used for programming languages like R or Python.
        - It has a more technical setup than other platforms, but it’s a great place to share your code and the how behind your analysis with other users.
        - Great place if you want to learn from other data analysts’ work
    - Blog platforms like Medium, WordPress and Google Sites
        - Are personalized and ownable.
        - You’ll have to store your code somewhere else
        - Opportunity to practice thought-leadership
        - Show off your expertise and write in your own voice.
    - Tableau.
        - Great option if you’re focused on the data viz side of things
        - You can create interactive dashboards
    
    ### How to add content to your potfolio
    
    | Platform | Information to help you manage your portfolio |
    | --- | --- |
    | GitHub | [8 steps to publishing your portfolio on GitHub](https://medium.com/tunapanda-institute/8-steps-to-publish-your-portfolio-on-github-9d6e6e3d2e84) |
    | Kaggle | [Publishing your first dataset on Kaggle](https://medium.com/analytics-vidhya/publishing-your-first-dataset-on-kaggle-6be8c37e59e8) |
    | Tableau | Any visualization created in Tableau Public is already public by default. A lot more is involved to add a Tableau visualization to another hosted site. For that reason, it is probably best to link to Tableau visualizations when your portfolio is hosted on a personal website or on a different platform, like GitHub. |
    | Mediu | [Getting started with a Medium publication](https://help.medium.com/hc/en-us/articles/115004681607-Getting-started-with-a-Medium-publication) |
    | WordPress | [Publish your site and share it publicly](https://wordpress.com/learn/get-published/) |
    | Google Sites | [Publish & share your site](https://support.google.com/sites/answer/6372880) or [Use a custom domain for your site](https://support.google.com/sites/answer/9068867) |
    
    ### Polish your portfolio
    
    There are some useful questions your should ask yourself to polish your portfolio:
    
    - Is there anything missing? Are you missing steps in your projects, or details in your descriptions?
        - If you have a website, are all the pages you need accounted for?
        - If you are hosting your portfolio on an existing platform, are all your projects uploaded properly?
    - Is there too much info?
        - Could any descriptions be revised for brevity?
        - Are there places where you include more data than you need? Could something be cut without losing the meaning or context of your project?
    - Is there anything you think you shouldn’t include?
        - Have you included references to others’ work that helped you without citing them? Can you remove them and instead include links to external work?
        - Are there any other components that might seem extraneous or unprofessional?
    - Is your portfolio hosted on the most appropriate platform?
        - There are many options for a data analytics platform, such as GitHub, Kaggle, and more. Is the one you’re using (or plan on using) the most appropriate for your needs?
    
    ### Tips for interview
    
    - [Interview Warmup](https://www.cloudskillsboost.google/interview-warmup). It’s a tool that helps you become more confident and comfortable throughout the interview process. Interview Warmup asks you interview questions to practice delivering your responses verbally. Your answers will be transcribed in real time, allowing you to review how you responded. In addition, Interview Warmup's machine learning algorithm can detect insights that can help you learn more about your answers and improve the way you communicate.
    
    There are some possible questions you might be asked in an interview:
    
    - Technical questions:
        - *“What are your preferred tools for analysis?”*
        - *“How do you maintain integrity in your data?”*
        - *“Do you understand different SQL functions and the roles they play?”*
    - Personal experience questions:
        - “*Was there a time when you took initiative during a project and what was the outcome?*” the goal is to understand your leadership abilities and how you have used them in the past.
        - “*What was the most challenging project you have ever been faced with*?” This question is usually meant to assess your problem-solving and interpersonal skills. Come to the interview prepared with several different examples of how you successfully navigated a difficult project or situation in the past.
        - “*How would you explain a complex topic to a stakeholder who was unfamiliar with it*?” This question helps your interviewer get a sense of how skilled you are at communicating effectively in high-pressure or sensitive circumstances.
        - “*How do you cope when things don’t go according to plan?*” This question provides a great opportunity for you to explain how you coped with unexpected changes and adapted quickly to a different course of action.
    - Questions to ask the interviewer:
        - “What are some upcoming projects I’d be working on?”
        - “What current goals is the company focused on?”
        - Can you tell me about the team I’ll be working with?”
    - Focus your experience on data
        
        ![image.png](Google_Data_Analytics_images/image%2014.png)
        
    
    ## AI FOR DATA ANALYTICS
    
    Artificial intelligence (AI) refers to computer systems that can perform cognitive tasks typically associated with human intelligence. You can use AI tools to augment and automate various data analysis tasks, such as:# GOOGLE DATA ANALYTICS

[Notion Version](https://www.notion.so/GOOGLE-DATA-ANALYTICS-e082e3eea7dd4dbaa209e978c51000b5) ![notion](Google_Data_Analytics_images/notion.PNG)

- GLOSSARY
    
    “##”: it tells the server (BigQuery) that this is a description and not part of the code.
    
    A/B testing: The process of testing two variations of the same web page to determine which page is more successful at attracting user traffic and generating revenue
    
    Absolute reference: a reference that is locked so that rows and columns won't change when copied.
    
    Aggregation: collecting or gathering many separate pieces into a whole.
    
    Alternative text: Provides a textual alternative to non-text content.
    
    Analysis: the process used to make sense of the data collected.
    
    Analytical skills: Qualities and characteristics associated with using facts to solve problems
    Analytical thinking: The process of identifying and defining a problem, then solving it by using data in an organized, step-by-step manner
    
    Annotation: Text that briefly explains data or helps focus the audience on a particular aspect of the data in a visualization.
    
    Array: A collection of values in cells
    
    Big data: Large, complex datasets typically involving long periods of time, which enable data
    analysts to address far-reaching business problems
    
    Borders: Lines that can be added around two or more cells on a spreadsheet
    
    Business task: A business task is the question or problem data analysis answers for business.
    
    Calculated field: A new field within a pivot table that carries out certain calculations based on the values of other fields.
    
    CASE statement: The CASE statement goes through one or more conditions and returns a value as soon as a condition is met.
    
    Case study: A common way for employers to assess job skills and gain insight into how you approach common data related challenges.
    
    Causation: Occurs when an action directly leads to an outcome. It is also called a cause-effect relationship.
    
    Cell reference: A cell or a range of cells in a worksheet typically used in formulas and functions.
    
    Changelog: a file containing a chronologically ordered list of modifications made to a project.
    
    Clean data: data that's complete, correct, and relevant to the problem you're trying to solve.
    
    Coding: writing instructions to the compter in the syntax of a specific programming language.
    
    Compatibility: how well two or more datasets are able to work together.
    
    Computer programming: Giving instructions to a computer to perform an action or set of actions.
    
    CONCAT: allows you to join multiple text strings from multiple sources
    
    CONCATENATE: a function that joins multiple text strings into a single string.
    
    Conditional formatting: a spreadsheet tool that changes how cells appear when values meet specific conditions.
    
    Confidence interval: The range of possible values that the population’s result would be at the confidence level of the study. This range is the sample result +/- the margin of error.
    
    Confidence level: How confident you are in the survey results. Confidence level is targeted before you start your study because it will affect how big your margin of error is at the end of your study.
    
    Context: The condition in which something exists or happens
    
    Correlation: in statistics is the measure of the degree to which two variables move in relationship to each other.
    
    COUNTA: counts the total number of values within a specified range.
    
    COUNTIF: a function that returns the number of cells that match a specified value.
    
    CSV: Comma-separated values. A CSV file saves data in a table format
    
    Dashboard: A tool that monitors live, incoming data. A tool that organizes information from multiple datasets into one central location for tracking, analysis, and simple visualization.
    
    Dashboard filter: A tool for showing only the data that meets a specific criteria while hiding the rest.
    
    Data: A collection of facts
    
    Data aggregation: the process of gathering data from multiple sources in order to combine it into a single summarized collection.
    
    Data analysis: The collection, transformation, and organization of data in order to draw conclusions, make predictions, and drive informed decision-making
    
    Data analyst: Someone who collects, transforms, and organizes data in order to drive informed decision-making
    
    Data analytics: The science of data
    
    Data blending: A Tableau method that combines data from multiple data sources
    
    Data composition: combining the individual parts in a visualization and displaying them together as a whole.
    
    Data elements: pieces of information, such as people's names, account numbers, and addresses
    
    Data integrity: is the accuracy, completeness, consistency, and trustworthiness of data throughout its lifecycle.
    
    Data model: A model that is used for organizing data elements and how they relate to one another.
    
    Database: A database is a collection of data stored in a computer system. When you maintain a database of customer information, ensuring data integrity, credibility, and privacy are all important concerns.
    Data-driven decision-making: Using facts to guide business strategy. Data-driven decision-making is when facts that have been discovered through data analysis are used to guide business strategy.
    Data ecosystem: The various elements that interact with one another in order to produce,
    manage, store, organize, analyze, and share data
    
    Data engineers: transform data into a useful format for analysis and give it a reliable infrastructure.
    
    Data mapping: the process of matching fields from one database to another.
    
    Data merging: the process of combining two or more datasets into a single dataset.
    Data science: A field of study that uses raw data to create new ways of modeling and understanding the unknown
    Dataset: A collection of data that can be manipulated or analyzed as one unit
    
    Data validation: a tool for checking the accuracy and quality of data before adding or importing it.
    
    Data validation process: Checking and rechecking the quality of your data so that it is complete, accurate, secure, and consistent.
    
    Data visualization: The graphic representation and presentation of data.
    
    Data warehousing specialist: develop processes and procedures to effectively store and organize data.
    
    Delimiter: a character that indicates the beginning or end of a data item.
    
    Deliverables:  are items or tasks you will complete before you can finish the project.
    
    Design thinking: a process used to solve complex problems in a user-centric way.
    
    Dirty data: data that's incomplete, incorrect, or irrelevant to the problem you're trying to solve.
    
    Documentation: the process of tracking changes, additions, deletions, and errors involved in your data cleaning effort.
    
    Elevator pitch: A short statement describing an idea or concept
    
    Engagement: Capturing and holding someone’s interest and attention during a data presentation
    
    EXTRACT command (SQL): Lets us pull one part of a given data to use
    
    Historical data: Data that already exist. We use this kind of data when you need an answer immediately.
    
    Fairness: Fairness means ensuring that your analysis doesn't create or reinforce bias. In other words, as a data analyst, you want to help create systems that are fair and inclusive to everyone. Fairness also means crafting questions that make sense to everyone.
    
    Field: a single piece of information from a row or column of a spreadsheet.
    
    Field length: a tool for determining how many characters can be keyed into a field.
    
    Filtering: showing only the data that meets a specific criteria while hiding the rest.
    
    Find and replace: a tool that looks for a specified search term in a spreadsheet and allows you to replace it with something else.
    
    First-party data: Data collected by an individual or group using their own resources. It is typically the preferred method because you know exactly where it came from.
    
    Float: a number that contains a decimal
    
    Foreign key: a field within a table that is a primary key in another table.
    
    Formula: A set of instructions used to perform a calculation using the data in a spreadsheet
    
    Framework: The context a presentation needs to create logical connections that tie back to the business task and metrics
    Function: A preset command that automatically performs a specified process or task using the data in a spreadsheet
    
    Function (R): A body of reusable code used to perform specific tasks in R
    
    Gap analysis: A method for examining and evaluating the current state of a process in order to identify opportunities for improvement in the future
    
    HAVING statement: allows you to add a filter to your query instead of the underlying table when you're working with aggregate functions.
    
    Header: The first row in a spreadsheet that labels the type of data in each column
    
    HTML: The set of markups symbols or codes used to create a webpage
    
    Hypothesis: The theory you’re trying to prove or disprove with data
    
    Hypothesis testing: is a way to see if a survey or experiment has meaningful results.
    
    Integrated Development Enviroment (IDE): A software application that brings together all the tools you may want to use in a single place.
    
    Interoperability: the ability of data systems and services to openly connect and share data.
    
    Issue: An issue is a topic or subject to investigate.
    
    JOIN: a SQL clause that's used to combine rows from two or more tables based on a related column.
    
    Leading question: A question that steers people toward a certain response
    
    LEFT: a function that gives you a set number of characters from the left side of a text string.
    
    LEN: a function that tells you the length of the text string by counting the number of characters it contains.
    
    Markdown: A syntax for formatting plain text files.
    
    MATCH: A function used to locate the position of a specific lookup value.
    
    Margin of error: Since a sample is used to represent a population, the sample’s results are expected to differ from what the result would have been if you had surveyed the entire population. This difference is called the margin of error. The smaller the margin of error, the closer the results of the sample are to what the result would have been if you had surveyed the entire population.
    
    Mental model: your thought process and the way you approach a problem
    
    Merge: an agreement that unites two organizations into a single new one.
    
    Metadata: is data about data.
    
    Metric goal: A metric goal is a measurable goal set by a company and evaluated using metrics. And just like there are a lot of possible metrics, there are lots of possible goals too.
    
    MID: a function that gives you a segment from the middle of a text string.
    
    Milestones: Milestones are significant tasks you will confirm along your timeline to help everyone know the project is on track.
    
    Modulo: An operator (%) that returns the remainder when one number is divided by another
    
    Null: an indication that a value does not exist in a data set. It is not the same as a zero.
    
    Open source: Code that is freely available and may be modified and shared by the people who use it.
    
    Openness (or open data): Free access, usage, and sharing of data.
    
    Operator: A symbol that names the type of operation or calculation to be performed in a formula.
    
    Outliers: are data points that are very different from similarly collected data and might not be reliable values.
    
    Pivot Table: A pivot table is a data summarization tool that is used in data processing. Pivot tables are used to summarize, sort, re-organize, group, count, total, or average data stored in a database. It allows its users to transform columns into rows and rows into columns.
    
    Population: all possible data values in a certain data set.
    
    Portfolio: Collection of case studies that can be shared with potential employers.
    
    Primary key: references a column in which each value is unique
    
    Problem: a problem is an obstacle or complication that needs to be worked out.
    
    Problem domain: the specific area of analysis that encompasses every activity affecting or affected by the problem.
    
    Problem types: The various problems that data analysts encounter, including categorizing things, discovering connections, finding patterns, identifying themes, making predictions, and spotting something unusual
    
    Professional relationship building: Building relationships by meeting people both in person and online (Refer to Networking)
    
    Profit margin: a percentage that indicates how many cents of profit have been generated for each dollar of sale.
    
    Programming languages: The words and symbols we use to write instructions for computers to follow
    
    Quartile: A quartile divides data points into four equal parts or quarters.
    
    Query language: A computer programming language used to communicate with a database
    
    Question: A question is designed to discover information
    
    R: A programming language frequently used for statistical anaysis, visualization, and other data analysis.
    
    Random sampling: a way of selecting a sample from a population so that every possible type of the sample has an equal chance of being chosen.
    
    Reframing: Restating a problem or challenge, then redirecting it toward a potential resolution
    
    Relational database: a database that contains a series of tables that can be connected to form relationships.
    
    Remove duplicates: A tool that automatically searches for and eliminates duplicate entries from a spreadsheet.
    
    Reports: Reports notify everyone as you finalize deliverables and meet milestones.
    
    Revenue: The total amount of income generated by the sale of goods or services
    
    RIGHT: a function that gives you a set number of characters from the right side of a text string.
    
    Root cause: The reason why a problem occurs
    
    Sample: a part of a population that is representative of the population.
    
    Sampling bias: is when a sample isn’t representative of the population as a whole. This means some members of the population are being overrepresented or underrepresented.
    
    Schema: a way of describing how something is organized.
    
    Second-party data: data collected by a group directly from its audience and then sold.
    
    Syntax: a predetermined structure that includes all required information and its proper placement.
    
    SMART methodology: A tool for determining a question’s effectiveness based on whether it is
    specific, measurable, action-oriented, relevant, and time-bound
    
    Soft skills: non-technical traits and behaviors that relate to how you work.
    
    Sorting: arranging data into a meaningful order to make it easier to understand, analyze, and visualize.
    
    Sort sheet: All of the data in a spreadsheet is sorted by the ranking of a specific sorted column - data across rows is kept together.
    
    Sort range: Nothing else on the spreadsheet is rearranged besides the specified cells in a column.
    
    Spotlighting: Scanning through data to quickly identify the most important insights.
    
    Split: a tool that divides a text string around the specified character and puts each fragment into a new and separate cell.
    
    Spreadsheet: A digital worksheet
    
    Statistical power: the probability of getting meaningful results from a test.
    
    Statistical significance: The determination of whether your result could be due to random chance or not. The greater the significance, the less due to chance.
    
    Stakeholders: People who have invested time and resources into a project and are interested in the outcome.
    
    Structured Query Language: A computer programming language used to communicate with a
    database
    Structured thinking: The process of recognizing the current problem or situation, organizing
    available information, revealing gaps and opportunities, and identifying options
    
    Subquery: is a SQL query that is nested inside of a larger query.
    
    SUMPRODUCT: a function that multiplies arrays and returns the sum of those products.
    
    Tableau: A business intelligence and analytics platform that helps people see, understand, and make decisions with data.
    
    Technical mindset: The ability to break things down into smaller steps or pieces and work with them in an orderly and logical way
    
    Temporary table: a database table that is created and exists temporarily on a database server
    
    Text string: a group of characters within a cell, most often composed of letters, numbers or both.
    
    Third-party data: Data collected from outside sources who did not collect it directly. This data might have come from a number of different sources before you investigated it. 
    
    Tidy data (R): A way of standardizing the organization of data within R.
    
    Time-bound question: A question that specifies a timeframe to be studied
    
    Transferable skills: skills and qualities that can transfer from one job or industry to another.
    
    TRIM: a function that removes leading, trailing, and repeated spaces in data.
    
    Turnover rate: the rate at which employees leave a company.
    
    Typecasting: Converting data from one type to another.
    
    Underscores: Lines used to underline words and connect text characters.
    
    Unfair question: A question that makes assumptions or is difficult to answer honestly
    
    VALUE: A function that converts a text string that represents a  number to a numerical value.
    
    Verification: a process to confirm that a data cleaning effort was well- executed and the resulting data is accurate and reliable.
    
    VLOOKUP (stands for vertical lookup): a function that searches for a certain value in a column to return a corresponding piece of information.
    
    YAML: A language for data that translates it so it’s readable
    
- 1 - FOUNDATIONS
    
    ## INTRODUCTION
    
    "Data! Data! Data! I can't make bricks without clay." This line was said by Sherlock Holmes, the famous detective created by Sir Arthur Conan Doyle. What Doyle meant was that Holmes couldn't draw any conclusions, which would be the bricks he mentioned without data, or the clay.
    Data is basically a collection of facts or information, and through analysis, you'll learn how to use the data to draw conclusions, and make predictions, and decisions.
    
    Phases of the data analysis process: Ask, prepare, process, analyze, share, and act.
    
    Data analysis is the collection, transformation, and organization of data in order to draw conclusions, make predictions, and drive informed decision-making.
    
    **People analytics:** is the practice of collecting and analyzing data on the people who make up a company’s workforce in order to gain insights to improve how the company operates.
    
    Being a people analyst involves using data analysis to gain insights about employees and how they experience their work lives. The insights are used to define and create a more productive and empowering workplace. This can unlock employee potential, motivate people to perform at their best, and ensure a fair and inclusive company culture.
    
    ### Steps of the data analysis process:
    
    1. **Ask** questions and define the problem.: Ask effective questions to define what the project would look like and what would qualify as a succesful result. The analysts asked questions to define both the issue to be solved and what would equal a successful result.
    2. **Prepare** data by collecting and storing the information.: solid preparation. Build a timeline. Also, we need to identify what data we need to achieve the succesful result we identified in the previous step
    3. **Process** data by cleaning and checking the information.: Great analysts know how to respect both their data and the people who provide it. The data analysts also made sure employees understood how their data would be collected, stored, managed, and protected. Collecting and using data ethically is one of the responsibilities of data analysts. They processed the data by cleaning it to make sure it was complete, correct, relevant, and free of errors and outliers.
    4. **Analyze** data to find patterns, relationships, and trends.: here you will discover and document what the analyst found  in the analysis
    5. **Share** data with your audience.: Just as they made sure the data was carefully protected, the analysts were also careful **sharing the report.** This process gave managers an opportunity to **communicate the results** with the right context. As a result, they could have productive team conversations about next steps to improve employee engagement.
    6. **Act** on the data and use the analysis results.: to work with leaders within their company and decide how best to **implement changes and take actions** based on the findings.
    
    **Decision Intelligence:** is a combination of applied data science and the social and managerial sciences. It is all about harnessing the power and beauty of data.
    
    Data science, the discipline of making data useful, is an umbrella term that encompasses three disciplines: machine learning, statistics, and analytics.
    
    - Statistics: if you want to make a few important decisions under uncertainty. Statisticians are essentially philosophers, epistemologists. They are very, very careful about protecting decision-makers from coming to the wrong conclusion.
    - Machine learning and AI: If you want to automate, in other words, make many, many, many decisions under uncertainty. Performance is the excellence of the machine learning and AI engineer.
    - Analytics: You want to encounter your unknown unknowns. You want to understand your world. The excellence of an analyst is speed. Don't worry about right answers. See how quickly you can unwrap this gift and find out if there is anything fun in there.
    
    ## DATA ECOSYSTEM
    
    An ecosystem is a group of elements that interact with one another. Ecosystems can be large, like the jungle in a tropical rainforest or the Australian outback. Or, tiny, like tadpoles in a puddle, or bacteria on your skin. Data lives inside its own ecosystem too.
    
    Data ecosystems are made up of various elements that interact with one another in order to produce, manage, store, organize, analyze, and share data. These elements include hardware and software tools, and the people who use them. Data can also be found in something called the cloud.
    
    **The cloud:** The cloud is a place to keep data online, rather than on a computer hard drive. So instead of storing data somewhere inside your organization's network, that data is accessed over the internet. So the cloud is just a term we use to describe the virtual location. The cloud plays a big part in the data ecosystem
    
    | Data Scientist | Data Analyst |
    | --- | --- |
    | Data science is defined as creating new ways of modeling and understanding the unknown by using raw data. | When you think about data, data analysis and the data ecosystem, it's important to understand that all of these things fit under the data analytics umbrella. |
    | Data scientists create new questions using data | Analysts find answers to existing questions by creating insights from data sources. |
    
    **Data Analysis:** is the collection, transformation, and organization of data in order to draw conclusions, make predictions, and drive informed decision-making.
    
    **Data analytics:** is the science of data.
    
    ### DATA-DRIVEN-DECISION-MAKING
    
    it is defined as using facts to guide business strategy. Organizations in many different industries are empowered to make better, data-driven decisions by data analysts all the time.
    
    **Steps:**
    
    1. Figuring out the business need: Usually this is a problem that needs to be solved
    2. Whatever the problem is, once it's defined, a data analyst finds data, analyzes it and uses it to uncover trends, patterns and relationships.
    
    Data-driven decision-making can be so powerful, it can make entire business methods obsolete.
    
    By ensuring that data is built into every business strategy, data analysts play a critical role in their companies' success, but it's important to note that no matter how valuable data-driven decision-making is, data alone will never be as powerful as data combined with human experience, observation, and sometimes even intuition.
    
    To get the most out of data-driven decision-making, it's important to include insights from people who are familiar with the business problem. These people are called **subject matter experts**, and they have the ability to look at the results of data analysis and identify any inconsistencies, make sense of gray areas, and eventually validate choices being made.
    
    ## DATA AND GUT INSTINCT
    
    Detectives and data analysts have a lot in common. Your job is all about following steps to collect and understand facts.
    
    Analysts use data-driven decision-making and follow a step-by-step process, but there are other factors that influence the decision-making process. Gut instinct is an intuitive understanding of something with little or no explanation. This isn’t always something conscious; we often pick up on signals without even realizing. You just have a “feeling” it’s right.
    
    ### Why gut instinct can be a problem
    
    - If you ignore data by preferring to make decisions based on your own experience, your decisions may be biased
    - Decisions based on gut instinct without any data to back them up can cause mistakes.
    
    **Data + business knowledge = mystery solved**
    
    The key is figuring out the exact mix for each particular project. A lot of times, it will depend on the goals of your analysis. That is why analysts often ask, “How do I define success for this project?”
    
    In addition, try asking yourself these questions about a project to help find the perfect balance:
    
    - What kind of results are needed?
    - Who will be informed?
    - Am I answering the question being asked?
    - How quickly does a decision need to be made?
    
    ## ORIGINS OF THE DATA ANALYSIS PROCESS
    
    Data analysis is rooted in statistics, which has a pretty long history itself. Archaeologists mark the start of statistics in ancient Egypt with the building of the pyramids. The ancient Egyptians were masters of organizing data. They documented their calculations and theories on papyri (paper-like materials), which are now viewed as the earliest examples of spreadsheets and checklists. Today’s data analysts owe a lot to those brilliant scribes, who helped create a more technical and efficient process.
    
    ### Data analysis life cycle
    
    The process of going from data to decision. There might not be one single architecture that’s uniformly followed by every data analysis expert, but there are some shared fundamentals in every data analysis process.
    
    The process presented as part of the Google Data Analytics Certificate is one that will be valuable to you as you keep moving forward in your career:
    
    1. **Ask**: Business Challenge/Objective/Question
    2. **Prepare**: Data generation, collection, storage, and data management
    3. **Process**: Data cleaning/data integrity
    4. **Analyze**: Data exploration, visualization, and analysis
    5. **Share**: Communicating and interpreting results
    6. **Act**: Putting your insights to work to solve the problem
    
    ![Untitled](Google_Data_Analytics_images/Untitled.png)
    
    ### EMC's data analysis life cycle
    
    EMC Corporation's data analytics life cycle is cyclical with six steps:
    
    1. Discovery
    2. Pre-processing data
    3. Model planning
    4. Model building
    5. Communicate results
    6. Operationalize
    
    EMC Corporation is now Dell EMC. This model, created by David Dietrich, reflects the cyclical nature of real-world projects.
    
    ### SAS's iterative life cycle
    
    An iterative life cycle was created by a company called **SAS**, a leading data analytics solutions provider. It can be used to produce repeatable, reliable, and predictive results:
    
    1. Ask
    2. Prepare
    3. Explore
    4. Model
    5. Implement
    6. Act
    7. Evaluate
    
    it includes a step after the act phase designed to help analysts evaluate their solutions and potentially return to the ask phase again.
    
    ### Project-based data analytics life cycle
    
    1. Identifying the problem
    2. Designing data requirements
    3. Pre-processing data
    4. Performing data analysis
    5. Visualizing data
    
    This data analytics project life cycle was developed by Vignesh Prajapati. It doesn’t include the sixth phase, or what we have been referring to as the Act phase.
    
    ### Big data analytics life cycle
    
    Authors Thomas Erl, Wajid Khattak, and Paul Buhler proposed a big data analytics life cycle in their book, **Big Data Fundamentals: Concepts, Drivers & Techniques**. Their life cycle suggests phases divided into nine steps:
    
    1. Business case evaluation
    2. Data identification
    3. Data acquisition and filtering
    4. Data extraction
    5. Data validation and cleaning
    6. Data aggregation and representation
    7. Data analysis
    8. Data visualization
    9. Utilization of analysis results
    
    It emphasizes the individual tasks required for gathering, preparing, and cleaning data before the analysis phase.
    
    ## ANALYTICAL SKILLS
    
    Analytical skills are qualities and characteristics associated with solving problems using facts. We will focus on 5 essential points or aspects:
    
    1. Curiosity. Is all about wanting to learn something. Curious people usually seek out new challenges and experiences. This leads to knowledge.
    2. Understanding context. Context is the condition in which something exists or happens. This can be a structure or an environment. Listening and trying to understand the full picture is critical.
    3. having a technical mindset. A technical mindset involves the ability to break things down into smaller steps or pieces and work with them in an orderly and logical way. When you take something that seems like a single task, like paying your bills, and break it into smaller steps with an orderly process, that's using a technical mindset.
    4. Data design. Data design is how you organize information. As a data analyst, design typically has to do with an actual database.
    5. Data strategy. Data strategy is the management of the people, processes, and tools used in data analysis.
    
    ## THINKING ANALYTICALLY
    
    Analytical thinking involves identifying and defining a problem and then solving it by using data in an organized, step-by-step manner.
    
    ### The five key aspects of analytical thinking:
    
    1. Visualization: is the graphical representation of information. For example, graphs, maps or design elements. Visualization is important because visuals can help data analysts understand and explain information more effectively.
    2. Strategy: Strategizing helps data analysts see what they want to achieve with the data and how they can get there. Strategy also helps improve the quality and usefulness of the data we collect. By strategizing, we know all our data is valuable and can help us accomplish our goals.
    3. Problem-orientation: Data analysts use a problem- oriented approach in order to identify, describe, and solve problems. It's all about keeping the problem top of mind throughout the entire project. Data analysts also ask a lot of questions. This helps improve communication and saves time while working on a solution.
    4. Correlation: being able to identify a correlation between two or more pieces of data. A correlation is like a relationship. You can find all kinds of correlations in data. Correlation does not equal causation. In other words, just because two pieces of data are both trending in the same direction, that doesn't necessarily mean they are all related.
    5. big-picture and detail-oriented thinking: This means being able to see the big picture as well as the details. Big-picture thinking is like looking at a complete puzzle. You can enjoy the whole picture without getting stuck on every tiny piece that went into making it. If you only focus on individual pieces, you wouldn't be able to see past that, which is why big-picture thinking is so important. It helps you zoom out and see possibilities and opportunities. This leads to exciting new ideas or innovations. On the flip side, detail-oriented thinking is all about figuring out all of the aspects that will help you execute a plan. In other words, the pieces that make up your puzzle.
    
    The more ways you can think, the easier it is to think outside the box and come up with fresh ideas. But why is it important to think in different ways? in data analysis, solutions are almost never right in front of you. You need to think **critically** to find out the right questions to ask. But you also need to think **creatively** to get new and unexpected answers.
    
    Some of the questions data analysts ask when they're on the hunt for a solution are:
    
    - What is the root cause of a problem?. A root cause is the reason why a problem occurs. If we can identify and get rid of a root cause, we can prevent that problem from happening again. A simple way to wrap your head around root causes is with the process called the Five Whys.
    - where are the gaps in our process?. For this, many people will use something called gap analysis. Gap analysis lets you examine and evaluate how a process works currently in order to get where you want to be in the future. The general approach to gap analysis is understanding where you are now compared to where you want to be. Then you can identify the gaps that exist between the current and future state and determine how to bridge them.
    - what did we not consider before?. This is a great way to think about what information or procedure might be missing from a process, so you can identify ways to make better decisions and strategies moving forward.
    
    ### Five Whys:
    
    you ask "why" five times to reveal the root cause. The fifth and final answer should give you some useful and sometimes surprising insights.
    
    The way data analysts think and ask questions plays a big part in how businesses make decisions. That's why analytical thinking and understanding how to ask the right questions can have such a huge impact on the overall success of a business.
    
    ## USING DATA TO DRIVE SUCCESSFUL OUTCOMES
    
    ### Data-driven decision-making:
    
    It gives you greater confidence about your choice and your abilities to address business challenges. It helps you become more proactive when an opportunity presents itself, and it saves you time and effort when working towards a goal.
    
    First, think about curiosity and context. The more you learn about the power of data, the more curious you're likely to become. You'll start to see patterns and relationships in everyday life
    
    The analysts take their thinking a step further by using context to make predictions, research answers, and eventually draw conclusions about what they've discovered. This natural process is a great first step in becoming more data-driven. Having a technical mindset comes next.
    
    Data analysts have gut feeling, but they've trained themselves to build on those feelings and use a more technical approach to explore them. They do this by always seeking out the facts, putting them to work through analysis, and using the insights they gain to make informed decisions.
    
    Next, we come to data design, which has a strong connection to data-driven decision-making. Designing your data so that it is organized in a logical way makes it easy for data analysts to access, understand, and make the most of available information. And it's important to keep in mind that data design doesn't just apply to databases. This kind of thinking can work with all sorts of real-life situations too.
    
    The basic idea is: If you make decisions that are informed by data, you are more likely to make more informed and effective decisions.
    
    The final ability is data strategy, which incorporates the people, processes, and tools used to solve a problem. This is a big one to remember because data strategy gives you a high-level view of the path you need to take to achieve your goals.
    
    data strategy gives you a high-level view of the path you need to take to achieve your goals. Also, data-driven decision-making isn't a one-person job. It's much more likely to be successful if everyone is on board and on the same page, so it's important to make sure specific procedures are in place and that your technology being used is aligned with your data-driven strategy.
    
    ## DATA LIFE CYCLE
    
    The data life cycle starts with the right data analysis tools. These include spreadsheets, databases, query languages, and visualization software. 
    
    The life cycle of data is plan, capture, manage, analyze, archive and destroy.
    
    1. **Planning:** This actually happens well before starting an analysis project. During planning, a business decides what kind of data it needs, how it will be managed throughout its life cycle, who will be responsible for it, and the optimal outcomes.
    2. **Capture:** This is where data is collected from a variety of different sources and brought into the organization. With so much data being created everyday, the ways to collect it are truly endless. One common method is getting data from outside resources. Another way to get data is from a company's own documents and files, which are usually stored inside a database.
    3. **Manage:** how we care for our data, how and where it's stored, the tools used to keep it safe and secure, and the actions taken to make sure that it's maintained properly. This phase is very important to data cleansing.
    4. **Analyze:** In this phase, the data is used to solve problems, make great decisions, and support business goals.
    5. **Archive:** Keep relevant data stored for long-term and future reference. Archiving means storing data in a place where it's still available, but may not be used again. It makes way more sense to archive it than to keep it around.
    6. **Destroy:** Remove data from storage and delete any shared copies of the data. To destroy it, the company would use a secure data erasure software. If there were any paper files, they would be shredded too. This is important for protecting a company's private information, as well as private data about its customers.
    
    ### Different ways to see the data life cycle:
    
    Although data life cycles vary, one data management principle is universal. Govern how data is handled so that it is accurate, secure, and available to meet your organization's needs.
    
    - U.S. Fish and Wildlife Service:
        1. Plan
        2. Acquire
        3. Maintain
        4. Access
        5. Evaluate
        6. Archive
    - The U.S. Geological Survey (USGS):
        1. Plan
        2. Acquire
        3. Process
        4. Analyze
        5. Preserve
        6. Publish/Share
    - Financial institutions:
        1. Capture
        2. Qualify
        3. Transform
        4. Utilize
        5. Report
        6. Archive
        7. Purge
    - Harvard Business School (HBS):
        1. Generation
        2. Collection
        3. Processing
        4. Storage
        5. Management
        6. Analysis
        7. Visualization
        8. Interpretation
    
    ## PHASES OF DATA ANALYSIS
    
    Data analysis isn't a life cycle. It's the process of analyzing data.
    
    1. **Ask:** In this phase, we do two things. We define the problem to be solved and we make sure that we fully understand stakeholder expectations. **Stakeholders** hold a stake in the project. They are people who have invested time and resources into a project and are interested in the outcome. First, defining a problem means you look at the current state and identify how it's different from the ideal state. Usually there's an obstacle we need to get rid of or something wrong that needs to be fixed. Another important part of the ask phase is understanding stakeholder expectations. The first step here is to determine who the stakeholders are. That may include your manager, an executive sponsor, or your sales partners. There can be lots of stakeholders. But what they all have in common is that they help make decisions, influence actions and strategies, and have specific goals they want to meet. This part of the ask phase helps you keep focused on the problem itself, not just its symptoms.
        
        You want to ask all of the right questions at the beginning of the engagement so that you better understand what your leaders and stakeholders need from this analysis.
        
        - what is the problem that we're trying to solve?
        - What is the purpose of this analysis?
        - What are we hoping to learn from it?
    2. **Prepare:** This is where data analysts collect and store data they'll use for the upcoming analysis process. Any decisions made from your analysis should always be based on facts and be fair and impartial.
        
        We need to be thinking about the type of data we need in order to answer the questions that we've set out to answer based on what we learned when we asked the right questions. We also need to be thinking about how we're going to collect that data or if we need to collect that data.
        
    3. **Process:** Here, data analysts find and eliminate any errors and inaccuracies that can get in the way of results. This usually means cleaning data, transforming it into a more useful format, combining two or more datasets to make information more complete and removing outliers, which are any data points that could skew the information. This phase is all about getting the details right. So you'll also fix typos, inconsistencies, or missing and inaccurate data.
        
        This is where you get a chance to understand its structure, its quirks, its nuances, and you really get a chance to understand deeply what type of data you're going to be working with and understanding what potential that data has to answer all of your questions. we will be running through all of our quality assurance checks, for example:
        
        - do we have all of the data that we anticipated we would have?
        - Are we missing data at random or is it missing in a systematic way such that maybe something went wrong with our data collection effort?
        - If needed, did we code all of our data the right way?
        - Are there any outliers that we need to treat differently?
        
        This is the part where we spend a lot of time really digging deeply into the structure and nuance of the data to make sure that you're able to analyze it appropriately and responsibly.
        
    4. **Analyze:** Analyzing the data you've collected involves using tools to transform and organize that information so that you can draw useful conclusions, make predictions, and drive informed decision-making.
        
        This is the point where we have to take a step back and let the data speak for itself. As data analysts, we are storytellers, but we also have to keep in mind that it is not our story to tell. That story belongs to the data, and it is our job as analysts to amplify and tell that story in as unbiased and objective a way as possible.
        
    5. **Share:** Here you'll learn how data analysts interpret results and share them with others to help stakeholders make effective data-driven decisions. In the share phase, visualization is a data analyst's best friend.
    6. **Act:** This is the exciting moment when the business takes all of the insights you, the data analyst, have provided and puts them to work in order to solve the original business problem and will be acting on what you've learned throughout this program.
        
        This is where we use all of those data-driven insights to decide what types of interventions we want to introduce, not only at the organizational level, but also at the team level as well.
        
    
    ![Untitled](Google_Data_Analytics_images/Untitled%201.png)
    
    ## THE DATA ANALYST TOOLS
    
    ### Spreadsheets:
    
    two popular options are Microsoft Excel and Google Sheets. A spreadsheet is a digital worksheet. It stores, organizes, and sorts data. When you put your data into a spreadsheet, you can see patterns, group information and easily find the information you need. Spreadsheets also have some really useful features called formulas and functions.
    
    Spreadsheets structure data in a meaningful way by letting you
    
    - Collect, store, organize, and sort information
    - Identify patterns and piece the data together in a way that works for each specific data project
    - Create excellent data visualizations, like graphs and charts.
    
    A formula is a set of instructions that performs a specific calculation using the data in a spreadsheet.
    
    A function is a preset command that automatically performs a specific process or task using the data in a spreadsheet. Functions can help make you more efficient.
    
    ### Query language
    
    A query language is a computer programming language that allows you to retrieve and manipulate data from a database.
    
    SQL is a language that lets data analysts communicate with a database. SQL is the most widely used structured query language for a couple of reasons:
    
    - Allow analysts to isolate specific information from a database(s)
    - Make it easier for you to learn and understand the requests made to databases
    - Allow analysts to select, create, add, or download data from a database for analysis
    
    With SQL, data analysts can access the data they need by making a query. A query means question, but it is more like a request.
    
    ### Data visualization
    
    Is the graphical representation of information. Some examples include graphs, maps, and tables. They help data analysts communicate their insights to others, in an effective and compelling way. This makes it easier for stakeholders to draw conclusions, make decisions, and come up with strategies. Also, these tools turn complex numbers into a story that people can understand. Some popular visualization tools are Tableau and Looker. Data analysts like using Tableau because it helps them create visuals that are very easy to understand. 
    
    A career as a data analyst also involves using programming languages, like R and Python, which are used a lot for statistical analysis, visualization, and other data analysis.
    
    Depending on which phase of the data analysis process you’re in, you will need to use different tools.
    
    ![Untitled](Google_Data_Analytics_images/Untitled%202.png)
    
    You don’t have to choose one or the other because each serves its own purpose. Generally, data analysts work with a combination of the two, as both tools are very useful in data analytics.
    
    ## SPREADSHEET AND SQL
    
    Spreadsheets are a big part of data analytics.
    
    ### Three main features of a spreadsheet:
    
    - Cells: when you talk about a specific cell, you name it by combining the column letter and the row number where the cell is located.
    - Rows: the rows are organized horizontally and are ordered by number. A row is also called an observation. An observation includes all of the attributes for something contained in a row of a data table.
    - Columns: columns are organized vertically in a spreadsheet and are ordered by letter.
    
    ![Untitled](Google_Data_Analytics_images/Untitled%203.png)
    
    ### Attribute
    
    Adding labels to the top of the columns will make it easier to reference and find data later on when you're doing analysis. These column labels are usually called attributes. An attribute is a characteristic or quality of data used to label a column in a table. More commonly, attributes are referred to as column names, column labels, headers, or the header row.
    
    ### Formula
    
    A formula is a set of instructions that performs a specific action using the data in a spreadsheet. To do this, the formula uses cell references for the values it's calculating. All formulas begin with =
    
    ### SQL
    
    Structured Query Language (or SQL, often pronounced “sequel”) enables data analysts to talk to their databases. Remember, SQL can do lots of the same things with data that spreadsheets can do. You can use it to store, organize and analyze your data, among other things. But like any good sequel, it's on a larger scale, bigger, more action-packed. SQL needs a database that will understand its language.
    
    **Query:** A query is a request for data or information from a database.
    
    ### Basic Structure of a SQL query:
    
    ![Untitled](Google_Data_Analytics_images/Untitled%204.png)
    
    ### Syntax
    
    Every programming language, including SQL, follows a unique set of guidelines known as syntax. Syntax is the predetermined structure of a language that includes all required words, symbols, and punctuation, as well as their proper placement. As soon as you enter your search criteria using the correct syntax, the query starts working to pull the data you’ve requested from the target database.
    
    The syntax of every SQL query is the same:
    
    - Use **SELECT** to choose the columns you want to return. You can separate the columns with (,).
    - Use **FROM** to choose the tables where the columns you want are located.
    - Use **WHERE** to filter for certain information. You connect the conditions with (AND), (OR) or (NOT).
    
    ![Untitled](Google_Data_Analytics_images/Untitled%205.png)
    
    The most important thing to remember is how to use SELECT, FROM, and WHERE in a query.
    
    ### SQL good practices - TIPS:
    
    - using capitalization and indentation can help you read the information more easily.
    - The semicolon is a statement terminator and is part of the American National Standards Institute (ANSI) SQL-92 standard, which is a recommended common syntax for adoption by all SQL databases. However, not all SQL databases have adopted or enforce the semicolon, so it’s possible you may come across some SQL statements that aren’t terminated with a semicolon. If a statement works without a semicolon, it’s fine.
    - the LIKE clause is very powerful because it allows you to tell the database to look for a certain pattern
    - The percent sign (%) is used as a wildcard to match one or more characters.
    - with SELECT * , you would be selecting all of the columns in the table, but be careful, selecting too much data can cause a query to run slowly.
    - You can place comments alongside your SQL to help you remember what the name represents. Comments are text placed between certain characters, /* and */, or after two dashes (--)
        - Comments can also be added outside of a statement as well as within a statement. You can use this flexibility to provide an overall description of what you are going to do, step-by-step notes about how you achieve it, and why you set different parameters/conditions.
        
        ![Untitled](Google_Data_Analytics_images/Untitled%206.png)
        
    - We can assign alias to the columns to reduce the ammount of comments. An alias doesn’t change the actual name of a column or table in the database.
    - <> means "does not equal”
    - Pulling the data, analyzing it, and implementing a solution might ultimately help improve employee satisfaction and loyalty. That makes SQL a pretty powerful tool.
    
    ## DATA VISUALIZATION
    
    Because of the importance of data visualization, most data analytics tools (such as spreadsheets and databases) have a built-in visualization component while others (such as Tableau) specialize in visualization as their primary value-add.
    
    Data visualizations are pictures, they are a wonderful way to take very basic ideas around data and data points and make them come alive.
    
    ### Steps to plan a data visualization:
    
    1. Explore the data for patterns
    2. Plan your visuals
    3. Create your visuals
    
    Data visualization toolkit:
    
    - Spreadsheets: Microsoft Excel or Google Sheets
    - Visualization software: Tableau. Tableau is a popular data visualization tool that lets you pull data from nearly any system and turn it into compelling visuals or actionable insights. The platform offers built-in visual best practices, which makes analyzing and sharing data fast, easy, and (most importantly) useful.
    - Programming language: R with RStudio. As with Tableau, you can create dashboard-style data visualizations using RStudio.
    
    ## FAIRNESS
    
    Fairness means ensuring that your analysis doesn't create or reinforce bias. In other words, as a data analyst, you want to help create systems that are fair and inclusive to everyone.
    
    Sometimes conclusions based on data can be true and unfair.
    
    Most common factors: industry, tools, location, travel, and culture.
    
    The data analyst role is one of many job titles that contain the word “analyst.”
    
    - Business analyst — analyzes data to help businesses improve processes, products, or services
    - Data analytics consultant — analyzes the systems and models for using data
    - Data engineer — prepares and integrates data from different sources for analytical use
    - Data scientist — uses expert skills in technology and social science to find trends through data analysis
    - Data specialist — organizes or converts data for use in databases or software systems
    - Operations analyst — analyzes data to assess the performance of business operations and workflows
    
    ![Untitled](Google_Data_Analytics_images/Untitled%207.png)
    
    Other industry-specific specialist positions that you might come across in your data analyst job search include:
    
    - Marketing analyst — analyzes market conditions to assess the potential sales of products and services
    - HR/payroll analyst — analyzes payroll data for inefficiencies and errors
    - Financial analyst — analyzes financial status by collecting, monitoring, and reviewing data
    - Risk analyst — analyzes financial documents, economic conditions, and client data to help companies determine the level of risk involved in making a particular business decision
    - Healthcare analyst — analyzes medical data to improve the business aspect of hospitals and medical facilities
    
    ### Interview best practices:
    
    - Think about a time where you've used data to solve a problem, whether it's in your professional or personal projects
    - increase your professional network.
    - It's really important to have your LinkedIn updated along with websites like GitHub, where you can showcase a lot of the data analysts projects you've done.
    - Prepare questions for the interviewer.
    - Sometimes there is no right answer, and a lot of times interviewers are looking to see your thought process and the way you get to your solution.
    - Look for the recruiter. Look for the hiring manager online. See if you can reach out to them and set up a coffee chat or send them your resume directly.
    
- 2 - **ASK** QUESTIONS TO MAKE DATA-DRIVEN DECISIONS
    
    This is related to the ASK part of the analysis phase
    
    **Structured thinking:** is the process of recognizing the current problem or situation, organizing available information, revealing gaps and opportunities, and identifying the options. In this process, you address a vague, complex problem by breaking it down into smaller steps, and then those steps lead you to a logical solution.
    
    key things to think about when choosing an advertising method:
    
    1. your target audience.
    2. your budget
    
    Take action with data:
    
    1. Zoom out and look at the whole situation in context. That way we can be sure that she was focusing on the real problem and not just its symptoms.
    2. Collaborating with stakeholders and understanding their needs. Takeholders included the owner, the vice president of communications, and the director of marketing and finance.
    3. Prepare phase, where we collect the data for the upcoming analysis process. Here we have to understand the company’s target audience, collect the data to look for patterns.
    4. Process step. Clean the data to eliminate any errors or inaccuracies that could get in the way of the result. When you clean data, you transform it into a more useful format, create more complete information and remove outliers.
    5. Analyze. 
    6. Share the recommendation so the company could make a data driven decision. summarize the results using clear and compelling visuals of the analysis. This helped her stakeholders understand the solution to the original problem.
    
    ## THE SIX DATA ANALYSIS PHASES
    
    There are six data analysis phases that will help you make seamless decisions: ask, prepare, process, analyze, share, and act. Keep in mind, these are different from the data life cycle, which describes the changes data goes through over its lifetime.
    
    ### Step 1: Ask.
    
    It’s impossible to solve a problem if you don’t know what it is. These are some things to consider:
    
    - Define the problem you’re trying to solve
    - Make sure you fully understand the stakeholder’s expectations
    - Focus on the actual problem and avoid any distractions
    - Collaborate with stakeholders and keep an open line of communication
    - Take a step back and see the whole situation in context
        
        **Questions to ask yourself in this step:**
        
        1. What are my stakeholders saying their problems are?
        2. Now that I’ve identified the issues, how can I help the stakeholders resolve their questions?
    
    ### Step 2: Prepare.
    
    You will decide what data you need to collect in order to answer your questions and how to organize it so that it is useful. You might use your business task to decide:
    
    - What metrics to measure
    - Locate data in your database
    - Create security measures to protect that data
        
        **Questions to ask yourself in this step:**
        
        1. What do I need to figure out how to solve this problem?
        2. What research do I need to do?
        
    
    ### Step 3: Process.
    
    Clean data is the best data and you will need to clean up your data to get rid of any possible errors, inaccuracies, or inconsistencies. This might mean:
    
    - Using spreadsheet functions to find incorrectly entered data
    - Using SQL functions to check for extra spaces
    - Removing repeated entries
    - Checking as much as possible for bias in the data
        
        **Questions to ask yourself in this step:**
        
        1. What data errors or inaccuracies might get in my way of getting the best possible answer to the problem I am trying to solve?
        2. How can I clean my data so the information I have is more consistent?
    
    ### Step 4: Analyze
    
    You will want to think analytically about your data. At this stage, you might sort and format your data to make it easier to:
    
    - Perform calculations
    - Combine data from multiple sources
    - Create tables with your results
        
        **Questions to ask yourself in this step:**
        
        1. What story is my data telling me?
        2. How will my data help me solve this problem?
        3. Who needs my company’s product or service? What type of person is most likely to use it?
    
    ### Step 5: Share.
    
    Everyone shares their results differently so be sure to summarize your results with clear and enticing visuals of your analysis using data via tools like graphs or dashboards. This is your chance to show the stakeholders you have solved their problem and how you got there. Sharing will certainly help your team:
    
    - Make better decisions
    - Make more informed decisions
    - Lead to stronger outcomes
    - Successfully communicate your findings
        
        **Questions to ask yourself in this step:**
        
        1. How can I make what I present to the stakeholders engaging and easy to understand?
        2. What would help me understand this if I were the listener?
    
    ### Step 6: Act.
    
    Now it’s time to act on your data. You will take everything you have learned from your data analysis and put it to use. This could mean providing your stakeholders with recommendations based on your findings so they can make data-driven decisions.
    
    **Questions to ask yourself in this step:**
    
    1. How can I use the feedback I received during the share phase (step 5) to actually meet the stakeholder’s needs and expectations?
    
    These six steps can help you to break the data analysis process into smaller, manageable parts, which is called **structured thinking**. This process involves four basic activities:
    
    1. Recognizing the current problem or situation
    2. Organizing available information
    3. Revealing gaps and opportunities
    4. Identifying your options
    
    When you are starting out in your career as a data analyst, it is normal to feel pulled in a few different directions with your role and expectations. Following processes like the ones outlined here and using structured thinking skills can help get you back on track, fill in any gaps and let you know exactly what you need.
    
    ### SOLVE PROBLEMS WITH DATA
    
    Data analysts work with a variety of problems. There are 6 common problem types:
    
    1. **making predictions:** involves using data to make an informed decision about how things may be in the future
        - Problem: how to determine the best advertising method for anywhere gaming repair's target audience. nobody can see the future but the data helped them make an informed decision about how things would likely work out.
    2. **categorizing things:** assigning information to different groups or clusters based on common features.
        - Problem: How to improve customer satisfaction levels. Analysts might classify customer service calls based on certain keywords or scores. They could identify certain key words or phrases that come up during the phone calls and then assign them to categories such as politeness, satisfaction, dissatisfaction, empathy, and more. Categorizing these key words gives us data that lets the company identify top performing customer service representatives, and those who might need more coaching.
    3. **spotting something unusual:** In this problem type, data analysts identify data that is different from the norm.
        - Analysts who have analyzed aggregated health data can help product developers determine the right algorithms to spot and set off alarms when certain data doesn't trend normally.
    4. **identifying themes:** Identifying themes takes categorization as a step further by grouping information into broader concepts.
        - Problem: How to improve users experience.
        - Themes are most often used to help researchers explore certain aspects of data. In a user study, user beliefs, practices, and needs are examples of themes.
        - The process here is kind of like finding categories for keywords and phrases in customer service conversations.
    5. **discovering connections:** enables data analysts to find similar challenges faced by different entities, and then combine data and insights to address them.
        - Problem: How to reduce wait time.
        - A third-party logistics company working with another company to get shipments delivered to customers on time is a problem requiring analysts to discover connections. By analyzing the wait times at shipping hubs, analysts can determine the appropriate schedule changes to increase the number of on-time deliveries.
    6. **Finding patterns:** Data analysts use data to find patterns by using historical data to understand what happened in the past and is therefore likely to happen again.
        - Problem: how to stop machines from breaking down.
        - Minimizing downtime caused by machine failure is an example of a problem requiring analysts to find patterns in data. For example, by analyzing maintenance data, they might discover that most failures happen if regular maintenance is delayed by more than a 15-day window.
    
    ## CRAFT EFFECTIVE QUESTIONS
    
    The more questions you ask, the more you'll learn about your data and the more powerful your insights will be at the end of the day. Knowing the difference between effective and ineffective questions is essential for your future career as a data analyst.
    
    ### Things to avoid when asking questions:
    
    - Leading questions: it's leading you to answer in a certain way
    - Closed-ended questions: questions that ask for a one-word or brief response only. These kinds of questions rarely lead to valuable insights. It's too vague and lacks context.
    - Vague questions: questions that aren’t specific or don’t provide context.
    
    ### Effective questions follow the SMART methodology.
    
    - S: Specific. Specific questions are simple, significant and focused on a single topic or a few closely related ideas. This helps us collect information that's relevant to what we're investigating. If a question is too general, try to narrow it down by focusing on just one element.
    - M: Measurable. Measurable questions can be quantified and assessed.
    - A: Action-oriented. Action-oriented questions encourage change. This brings you answers you can act on
    - R: Relevant. Relevant questions matter, are important and have significance to the problem you're trying to solve.
    - T: Time-bound. Time-bound questions specify the time to be studied. This limits the range of possibilities and enables the data analyst to focus on relevant data
    
    ![Untitled](Google_Data_Analytics_images/Untitled%208.png)
    
    Questions should be open-ended. This is the best way to get responses that will help you accurately qualify or disqualify potential solutions to your specific problem.
    
    ## UNDERSTAND THE POWER OF DATA - data-inspired decision-making - Quantitive and qualitative data
    
    Data-inspired decision-making explores different data sources to find out what they have in common.
    
    An algorithm is a process or set of rules to be followed for a specific task.
    
    Responsibly gathering data is only part of the process. We also have to turn data into knowledge that helps us make better solutions.
    
    ![Untitled](Google_Data_Analytics_images/Untitled%209.png)
    
    The goal of all data analysts is to use data to draw accurate conclusions and make good recommendations. That all starts with having complete, correct, and relevant data.
    
    - **When data is interpreted incorrectly, it can lead to huge losses.**
    - **When data is used strategically, businesses can transform and grow their revenue.**
    
    As a data analyst, your own skills and knowledge will be the most important part of any analysis project. It is important for you to keep a data-driven mindset, ask lots of questions, experiment with many different possibilities, and use both logic and creativity along the way. You will then be prepared to interpret your data with the highest levels of care and accuracy.
    
    Analyzing user preferences to customize movie recommendations and analyzing product purchases to create better promotions are examples of using data to achieve business results.
    
    There are 2 kinds of data:
    
    ### Quantitative data
    
    Quantitative data is all about the specific and objective measures of numerical facts. This can often be the what, how many, and how often about a problem. In other words, things you can measure.
    
    Quantitative data is a specific and objective measure, such as a number, quantity or range.
    
    With quantitative data, we can see numbers visualized as charts or graphs.
    
    Tools:
    
    - Structured interviews
    - Surveys
    - Polls
    
    ### Qualitative data
    
    qualitative data describes subjective or explanatory measures of qualities and characteristics or things that can't be measured with numerical data. Qualitative data is great for helping us answer why questions.
    
    Qualitative data can then give us a more high-level understanding of why the numbers are the way they are.
    
    **Tools:**
    
    - Focus groups
    - Social media text analysis
    - In-person interviews
    
    It's your job as a data detective to know which questions to ask to find the right solution. Then you can start thinking about cool and creative ways to help stakeholders better understand the data.
    
    Usually, qualitative data can help analysts better understand their quantitative data by providing a reason or more thorough explanation. In other words, quantitative data generally gives you the what, and qualitative data generally gives you the why.
    
    ## FOLLOW THE EVIDENCE - metrics, reports, dashboards
    
    here are all kinds of tools out there to help you visualize and share your data analysis with stakeholders. Here, we'll talk about two data presentation tools, reports and dashboards. Reports and dashboards are both useful for data visualization. But there are pros and cons for each of them.
    
    ### Reports
    
    A report is a static collection of data given to stakeholders periodically.
    
    | pros | cons |
    | --- | --- |
    | Reports are great for giving snapshots of high level historical data for an organization. | Reports need regular maintenance. |
    | They can be designed and sent out periodically, often on a weekly or monthly basis, as organized and easy to reference information. | They are not very visually appealing. |
    | They're quick to design and easy to use as long as you continually maintain them. | Because they aren't automatic or dynamic, reports don't show live, evolving data. |
    | Because reports use static data or data that doesn't change once it's been recorded, they reflect data that's already been cleaned and sorted. |  |
    
    ### Dashboards
    
    A dashboard monitors live, incoming data.
    
    Data analysts use dashboards to track, analyze, and visualize data in order to answer questions and solve problems.
    
    | Pros | Cons |
    | --- | --- |
    | they give your team more access to information being recorded, you can interact through data by playing with filters, and because they're dynamic, they have long-term value. | they take a lot of time to design |
    | If stakeholders need to continually access information, a dashboard can be more efficient than having to pull reports over and over | can actually be less efficient than reports, if they're not used very often. (can be confusing). |
    | Nice to look at | If the base table breaks at any point, they need a lot of maintenance to get back up and running again. |
    | A dashboard organizes information from multiple datasets into one central location, offering huge time-savings. | Dashboards can sometimes overwhelm people with information too. |
    
    ![Untitled](Google_Data_Analytics_images/Untitled%2010.png)
    
    ### How to create a Dashboard:
    
    1. Identify the stakeholders who need to see the data and how they will use it: To get started with this, you need to ask effective questions. Check out the PDF.
    2. Design the dashboard (what should be displayed): Use these tips to help make your dashboard design clear, easy to follow, and simple:
        - Use a clear header to label the information
        - Add short text descriptions to each visualization
        - Show the most important information at the top
    3. Create mock-ups if desired: This is optional, but a lot of data analysts like to sketch out their dashboards before creating them.
    4. Select the visualizations you will use on the dashboard: it all depends on what data story you are telling. If you need to show a change of values over time, line charts or bar graphs might be the best choice. If your goal is to show how each part contributes to the whole amount being reported, a pie or donut chart is probably a better choice.
    5. Create filters as needed.
    
    ### Types of dashboards:
    
    The three most common categories are:
    
    - **Strategic**: focuses on long term goals and strategies at the highest level of metrics. A wide range of businesses use strategic dashboards when evaluating and aligning their strategic goals. These dashboards provide information over the longest time frame—from a single financial quarter to years.
    - **Operational:** short-term performance tracking and intermediate goals. It is the most common type of dashboard. Because these dashboards contain information on a time scale of days, weeks, or months, they can provide performance insight almost in real-time. This allows businesses to track and maintain their immediate operational processes in light of their strategic goals. The operational dashboard below focuses on customer service.
    
    ![Untitled](Google_Data_Analytics_images/Untitled%2011.png)
    
    - **Analytical:** consists of the datasets and the mathematics used in these sets. These dashboards contain the details involved in the usage, analysis, and predictions made by data scientists. Certainly the most technical category, analytic dashboards are usually created and maintained by data science teams and rarely shared with upper management as they can be very difficult to understand.
    
    ![Untitled](Google_Data_Analytics_images/Untitled%2012.png)
    
    ### Data VS Metrics
    
    Data starts as a collection of raw facts, until we organize them into individual metrics that represent a single type of data.
    
    | Data | Metrics |
    | --- | --- |
    | Data contains a lot of raw details about the problem we're exploring. | A metric is a single, quantifiable type of data that can be used for measurement. |
    |  | Metrics can also be combined into formulas that you can plug your numerical data into. Metrics usually involve simple math. |
    
    We see metrics used in marketing too. For example, metrics can be used to help calculate customer retention rates, or a company's ability to keep its customers over time. Customer retention rates can help the company compare the number of customers at the beginning and the end of a period to see their retention rates.
    
    By using metrics to focus on individual aspects of your data, you can start to see the story your data is telling. Metric goals and formulas are great ways to measure and understand data.
    
    ### ROI:
    
    ROI, or Return on Investment is essentially a formula designed using metrics that let a business know how well an investment is doing.
    
    The ROI is made up of two metrics, the net profit over a period of time and the cost of investment. By comparing these two metrics, profit and cost of investment, the company can analyze the data they have to see how well their investment is doing.
    
    ## CONNECTING THE DATA DOTS - Mathematical thinking, Big and small data
    
    Mathematical thinking is a powerful skill you can use to help you solve problems and see new solutions.
    
    It means looking at a problem and logically breaking it down step-by-step, so you can see the relationship of patterns in your data, and use that to analyze your problem. This kind of thinking can also help you figure out the best tools for analysis because it lets us see the different aspects of a problem and choose the best logical approach.
    
    There are a lot of factors to consider when choosing the most helpful tool for your analysis. One way you could decide which tool to use is by the size of your dataset. When working with data, you'll find that there's big and small data.
    
    ### Small data
    
    These kinds of data tend to be made up of datasets concerned with specific metrics over a short, well defined period of time. Like how much water you drink in a day. Small data can be useful for making day-to-day decisions, like deciding to drink more water. But it doesn't have a huge impact on bigger frameworks like business operations.
    
    Small data is typically stored in a Spreadsheet. Big data is typically stored in a Database.
    
    ### Big Data
    
    Big data on the other hand has larger, less specific datasets covering a longer period of time. They usually have to be broken down to be analyzed. Big data is useful for looking at large- scale questions and problems, and they help companies make big decisions.
    
    | Challenges | Benefits |
    | --- | --- |
    | A lot of organizations deal with data overload and way too much unimportant or irrelevant information. | When large amounts of data can be stored and analyzed, it can help companies identify more efficient ways of doing business and save a lot of time and money. |
    | Important data can be hidden deep down with all of the non-important data, which makes it harder to find and use. This can lead to slower and more inefficient decision-making time frames. | Big data helps organizations spot the trends of customer buying patterns and satisfaction levels, which can help them create new products and solutions that will make customers happy. |
    | The data you need isn’t always easily accessible. | By analyzing big data, businesses get a much better understanding of current market conditions, which can help them stay ahead of the competition. |
    | Current technology tools and solutions still struggle to provide measurable and reportable data. This can lead to unfair algorithmic bias. | Big data helps companies keep track of their online presence—especially feedback, both good and bad, from customers. This gives them the information they need to improve and protect their brand. |
    | There are gaps in many big data business solutions. |  |
    
    ### The three (or four) V words for big data:
    
    When thinking about the benefits and challenges of big data, it helps to think about the three Vs:
    
    - Volume: Describes the amount of data.
    - Variety: describes the different kinds of data.
    - Velocity: describes how fast the data can be processed.
    
    Some data analysts also consider a fourth V:
    
    - Veracity: refers to the quality and reliability of the data.
    
    ![Untitled](Google_Data_Analytics_images/Untitled%2013.png)
    
    ## WORKING WITH SPREADSHEETS
    
    Spreadsheets are a powerful and versatile tool, which is why they're a big part of pretty much everything we do as data analysts.
    
    Spreadsheets can do both basic and complex calculations automatically. Not only does this help you work more efficiently, but it also lets you see the results and understand how you got them.
    
    ### Spreadsheet tasks:
    
    - Organize your data with the task you've been given.
        - Pivot table
            - Sort and filter. The first steps a data analyst takes when working with data in a spreadsheet are to sort and filter the data.
    - Perform some calculations to learn more about it.
        - Formulas
        - Functions
    
    ### Spreadsheet and the data life cycle:
    
    - Plan: This can mean formatting your cells, the headings you choose to highlight, the color scheme, and the way you order your data points. When you take the time to set these standards, you will improve communication, ensure consistency, and help people be more efficient with their time.
    - Capture: by connecting spreadsheets to other data sources, such as an online survey application or a database. This data will automatically be updated in the spreadsheet. That way, the information is always as current and accurate as possible.
    - Manage: This can involve storing, organizing, filtering, and updating information. Spreadsheets also let you decide who can access the data, how the information is shared, and how to keep your data safe and secure.
    - Analyze: Some of the most common spreadsheet analysis tools include formulas to aggregate data or create reports, and pivot tables for clear, easy-to-understand visuals.
    - Archive: This is especially useful if you want to store historical data before it gets updated.
    - Destroy: Keep in mind, lots of businesses are required to follow certain rules or have measures in place to make sure data is destroyed properly.
    
    ### Definitions on a Spreadsheet:
    
    - Attribute: is a characteristic or quality of data used to label a column in a table.
    - Comment on your spreadsheet: Google Sheets lets you and your collaborators add comments to your sheet and reply to those comments. As a data analyst, this is a great way to share feedback with your teammates. To direct your comment to a specific person, enter an at sign (@) followed by their email address. You can add as many people as you want.
    
    ### Basic Step-by-step process:
    
    1. Create a tittle: Make your title short, clear, and have it state exactly what the data in the spreadsheet is about.
    2. Move it: Creating a folder on your computer specifically for spreadsheets and related files can also make it easier to find them.
    - There's a few different ways data analysts get data they work with. Depending on the job, you might use data from an open source, you might be given data to work with or you might be asked to find your own data
    1. Make the columns wider
    2. Organize the data attributes. We can stand it out from the rest of the rows by selecting it and filling it with color. We'll also make the labels bold.
    3. Adding borders to highlight the area around cells in order to see spreadsheet data more clearly.
    
    ## FORMULAS IN SPREADSHEETS
    
    One of the most valuable spreadsheet features is a formula. Formulas are built on operators which are symbols that name the type of operation or calculation to be performed. For example, a plus sign is a common operator. The formulas you use as a data analyst will usually include at least one operator.
    
    These are all examples of expressions:
    
    - 3 minus 1
    - 15 plus 8 divided by 2
    - 846 times 513.
    
    When you create a formula using an expression in a spreadsheet, you start the formula with an equal sign.
    
    In spreadsheets, the symbols used in a formula to perform a specific calculation are called operators. The operators you will use to complete formulas 
    
    - Subtraction – minus sign ( - )
    - Addition – plus sign ( + )
    - Division – forward-slash ( / )
    - Multiplication – asterisk ( * )
    
    If you already have data in your spreadsheet, you can use cell references in your formulas instead. A cell reference is a single cell or range of cells in a worksheet that can be used in a formula. Cell references contain the letter of the column and the number of the row where the data is. 
    
    A range of cells is a collection of two or more cells. A range can include cells from the same row or column, or from different columns and rows collected together. The great thing about using cell references is that they also automatically update when a formula is copied to a new cell. Talk about a time-saver.
    
    A formula has to be air tight. If there's something wrong with one of the cell references, it won't work.
    
    ### Important definitios
    
    - **Auto-Filling:**
    
    The lower-right corner of each cell has a fill handle. It is a small green square in Microsoft Excel and a small blue square in Google Sheets.
    
    ![Untitled](Google_Data_Analytics_images/Untitled%2014.png)
    
    - **Absolute referencing:**
    
    is marked by a dollar sign ($). For example, =$A$10 has absolute referencing for both the column and the row value. To easily switch between absolute and relative referencing in the formula bar, highlight the reference you want to change and press the F4 key
    
    - **Data range:**
    
    When you click into your formula, the colored ranges let you see which cells are being used in your spreadsheet. There are different colors for each unique range in your formula. In a lot of spreadsheet applications, you can press the F2 (or Enter) key to highlight the range of data in the spreadsheet that is referenced in a formula.
    
    - **Combining with functions:**
    
    Combining formulas and functions allows you to do more work with a single command. For example, COUNTIF() is a formula and a function. This means the function runs based on criteria set by the formula. In this case, COUNT is the formula; it will be executed IF the conditions you create are true.
    
    - **Spreadsheets errors and fixes:**
    
    | ERROR | WHY? | HOW TO FIX IT |
    | --- | --- | --- |
    | #DIV/0! | when a formula is trying to divide a value in a cell by zero or by an empty cell. | With IFERROR function |
    | #ERROR | tells us the formula can't be interpreted as it is input. This is also known as a parsing error. | Check delimiters or typing errors in the formula |
    | #N/A | the data in your formula can't be found by the spreadsheet. Generally, this means the data doesn't exist. This error most often occurs when using functions such as VLOOKUP | check typo errors |
    | #NAME? | when a formula's name isn't recognized or understood. | Check typo errors (example, The name of a function is misspelled) |
    | #NUM! | a formula's calculation can't be performed as specified by the data. The data doesn't make sense for that calculation. |  |
    | #VALUE! | can indicate a problem with a formula or referenced cells. It's often not clear right away what the problem is, so this error might take a little more effort to fix. | There could be problems with spaces or text, or with referenced cells in a formula; you may have additional work to find the source of the problem. |
    | #REF! | when cells being referenced in a formula have been deleted, thus making the formula unable to perform the calculation. | Example: A cell used in a formula was in a column that was deleted |
    
    Troubleshooting is a big part of data analysis, so being able to find solutions is a key skill for data analysts.
    
    ### Best practices and helpful tips:
    
    These strategies will help you avoid spreadsheet errors to begin with, making your life in analytics a whole lot less stressful:
    
    1. Filter data to make your spreadsheet less complex and busy.
    2. Use and freeze headers so you know what is in each column, even when scrolling.
    3. When multiplying numbers, use an asterisk (*) not an X.
    4. Start every formula and function with an equal sign (=).
    5. Whenever you use an open parenthesis, make sure there is a closed parenthesis on the other end to match.
    6. Change the font to something easy to read.
    7. Set the border colors to white so that you are working in a blank sheet.
    8. Create a tab with just the raw data, and a separate tab with just the data you need.
    - Conditional formatting can be used to highlight cells a different color based on their contents. This feature can be extremely helpful when you want to locate all errors in a large spreadsheet.
    
    ## FUNCTIONS IN SPREADSHEETS
    
    Formulas are a great way to become more efficient when using spreadsheets, especially when you add shortcuts like copying and pasting, into the mix. Functions give data analysts the ability to do calculations, which can be anything from simple arithmetic to complex equations.
    
    While functions are closely related to formulas, they're not exactly the same.
    
    In the world of spreadsheets a function is a preset command that automatically performs a specific process or task using the data.
    
    A formula is a set of instructions used to perform a calculation using the data in a spreadsheet. A function is a preset command that automatically performs a specific process or task using the data in a spreadsheet.
    
    ### Tips:
    
    - A colon between the cell references shows that you're using a range.
    - Functions can be copied and pasted into other cells in the same column.
    - Spreadsheets have something called a fill handle. It's a little box that appears in the lower right-hand corner when you click on a cell.
    - Different functions perform different calculations, but they work in the same way. Keep in mind, not every calculation you'll come across has its own function to help you.
    - Highlight key data. Add color to the cell in your data set to make it stand out. Colored data ranges help prevent you from getting lost in complex functions.
    - Just like formulas, start all of your functions with an equal sign
    
    ### Popular functions
    
    - Keyboard shortcuts like cut, save, and find are actually functions. Using shortcuts lets you do more with less effort. They can make you more efficient and productive because you are not constantly reaching for the mouse and navigating menus.
    
    ![Untitled](Google_Data_Analytics_images/Untitled%2015.png)
    
    ![Untitled](Google_Data_Analytics_images/Untitled%2016.png)
    
    ### Relative, absolute, and mixed references
    
    Relative references (cells referenced without a dollar sign, like A2) will change when you copy and paste the function into a different cell. With relative references, the location of the cell that contains the function determines the cells used by the function.
    
    Absolute references (cells fully referenced with a dollar sign, like $A$2) will not change when you copy and paste the function into a different cell. With absolute references, the cells referenced always remain the same.
    
    Mixed references (cells partially referenced with a dollar sign, like $A2 or A$2) will change when you copy and paste the function into a different cell. With mixed references, the location of the cell that contains the function determines the cells used by the function, but only the row or column is relative (not both).
    
    In spreadsheets, you can press the F4 key to toggle between relative, absolute, and mixed references in a function. Click the cell containing the function, highlight the referenced cells in the formula bar, and then press F4 to toggle between and select relative, absolute, or mixed referencing.
    
    ## SAVE TIME WITH STRUCTURED THINKING
    
    If you define the problem clearly from the start, it'll be easier to solve, which saves a lot of time, money, and resources. In the data world, we call this first piece the problem domain: the specific area of analysis that encompasses every activity affecting or affected by the problem.
    
    Before we can do anything else, we need to understand the problem domain and all of its parts and relationships so that we can discover the whole story.
    
    Data analysts aren't always given the complete picture at the start of a project. A big part of their job is to develop a structured approach and use critical thinking to find the best solution. That starts with understanding the problem domain. This is where structured thinking comes into play. To successfully solve a problem as a data analyst, you need to train your brain to think structurally.
    
    Structures thinking in other words, it's a way of being super prepared. It's having a clear list of what you are expected to deliver, a timeline for major tasks and activities, and checkpoints so the team knows you're making progress. Structured thinking will help you understand problems at a high level so that you can identify areas that need deeper investigation and understanding.
    
    ### Tips:
    
    - The starting place for structured thinking is the **problem domain**. Once you know the specific area of analysis, you can set your base and lay out all your requirements and hypotheses before you start investigating.
    - With a solid base in place, you'll be ready to deal with any obstacles that come up. Missing variables can lead to inaccurate conclusions.
    
    ### Scope of work (SOW)
    
    using a scope of work. Scope of work or SOW is an agreed- upon outline of the work you're going to perform on a project. For many businesses, this includes things like work details, schedules, and reports that the client can expect. Now, as a data analyst, your scope of work will be a bit more technical and include those basic items we just mentioned, but you'll also focus on things like data preparation, validation, analysis of quantitative and qualitative datasets, initial results, and maybe even some visuals to really get the point across.
    
    - At this point, try not to confuse statement of work with scope of work, which are both abbreviated as SOW. A statement of work is a document that clearly identifies the products and services a vendor or contractor will provide to an organization. It includes objectives, guidelines, deliverables, schedule, and costs. A scope of work is project-based and sets the expectations and boundaries of a project. A scope of work may be included in a statement of work to help define project outcomes.
    - As a junior data analyst, It's more typical to be asked to create a scope of work than a statement of work.
    
    With a solid scope of work, you'll be able to address any confusion, contradictions, or questions about the data up- front and make sure these sneaky setbacks don't stand in your way.
    
    **Pieces:**
    
    - Deliverables: What work is being done, and what things are being created as a result of this project? When the project is complete, what are you expected to deliver to the stakeholders? Be **specific** here. Will you collect data for this project? How much, or for how long?. Use numbers and aim for hard, measurable goals and objectives.
    - Milestones: This is closely related to your timeline. What are the major milestones for progress in your project? How do you know when a given part of the project is considered complete?. It can be identified by you, by stakeholders, or by other team members such as the Project Manager.
    - Timeline: Your timeline will be closely tied to the milestones you create for your project. The timeline is a way of mapping expectations for how long each step of the process should take. The timeline should be specific enough to help all involved decide if a project is on schedule. When will the deliverables be completed? How long do you expect the project will take to complete? If all goes as planned, how long do you expect each component of the project will take? When can we expect to reach each milestone?
    - Reports: Good SOWs also set boundaries for how and when you’ll give status updates to stakeholders. How will you communicate progress with stakeholders and sponsors, and how often? Will progress be reported weekly? Monthly? When milestones are completed? What information will status reports contain?
    
    At a minimum, any SOW should answer all the relevant questions in the above areas. Note that these areas may differ depending on the project. But at their core, the SOW document should always serve the same purpose by containing information that is specific, relevant, and accurate. If something changes in the project, your SOW should reflect those changes.
    
    SOWs should also contain information specific to what is and isn’t considered part of the project. The scope of your project is everything that you are expected to complete or accomplish, defined to a level of detail that doesn’t leave any ambiguity or confusion about whether a given task or item is part of the project or not.
    
    ### Staying objective
    
    In the world of data, numbers don't mean much without context.
    
    We use data at many different levels. Sometimes our data is descriptive, answering questions like, how much did we spend on travel last month? Data becomes more valuable, as we generate diagnostic and predictive insights, like understanding why travel spend increased last month. Data is most valuable, however, when we can generate prescriptive insights. For example, how can we leverage data to incentivize more efficient travel? 
    
    Figuring out what data means, is just as important as collecting it. As a data analyst, a big part of your job, is putting data into context. It's also up to you, to remain objective and recognize all sides of an argument, before drawing conclusions.
    
    The thing about context, is that it's very personal. Conclusions can be influenced by your own conscious and subconscious biases, which are based on cultural, social and market norms. 
    
    To really understand what the data is about and put information into context, you have to think through who, what, where, when, how and why.
    
    It's good to ask yourself questions like:
    
    - Who: The person or organization that created, collected, and/or funded the data collection
    - What: The things in the world that data could have an impact on
    - Where: The origin of the data
    - When: The time when the data was created or collected
    - Why: The motivation behind the creation or collection. The why can have a particularly strong relationship with bias. Why? Because sometimes, data is collected, or even made up, to serve an agenda.
    
    The best thing you can do for the fairness and accuracy of your data, is to make sure you start with an accurate representation of the population, and collect the data in the most appropriate, and objective way. Then, you'll have the facts so you can pass on to your team
    
    ### The importance of context
    
    Understanding the context behind the data can help us make it more meaningful at every stage of the data analysis process.
    
    Understanding and including the context is important during each step of your analysis process, so it is a good idea to get comfortable with it early in your career. For example, when you collect data, you’ll also want to ask questions about the context to make sure that you understand the business and business process. During organization, the context is important for your naming conventions, how you choose to show relationships between variables, and what you choose to keep or leave out. And finally, when you present, it is important to include contextual information so that your stakeholders understand your analysis.
    
    ## BALANCE TEAM AND STAKEHOLDER NEEDS
    
    Communication is key.
    
    The stakeholders hold stakes in what you're doing. Your stakeholders will want to discuss things like the project objective, what you need to reach that goal, and any challenges or concerns you have. These conversations help build trust and confidence in your work.
    
    Focusing on stakeholder expectations will help you understand the goal of a project, communicate more effectively across your team, and build trust in your work.
    
    ### Types of stakeholders
    
    - Executive team: They set goals, develop strategy, and make sure that strategy is executed effectively. The executive team might include vice presidents, the chief marketing officer, and senior-level professionals who help plan and direct the company’s work. Working closely with your project manager can help you pinpoint the needs of the executive stakeholders for your project, so don’t be afraid to ask them for guidance.
    - Customer-facing team: includes anyone in an organization who has some level of interaction with customers and potential customers. Typically they compile information, set expectations, and communicate customer feedback to other parts of the internal organization. You want to be sure that your analysis and presentation focuses on what is actually in the data-- not on what your stakeholders hope to find.
    - Data science team:
        
        ![Untitled](Google_Data_Analytics_images/Untitled%2017.png)
        
    
    ### Working effectively with stakeholders
    
    - Discuss goals: Start a discussion. Ask about the kind of results the stakeholder wants. Sometimes, a quick chat about goals can help set expectations and plan the next steps.
    - Feel empowered to say “no.”: Whatever the case may be, don’t be afraid to push back when you need to. You should feel empowered to say no-- just remember to give context so others understand why.
    - Plan for the unexpected: Before you start a project, make a list of potential roadblocks. Then, when you discuss project expectations and timelines with your stakeholders, give yourself some extra time for problem-solving at each stage of the process.
    - Know your project: Get to know how your project connects to the rest of the company and get involved in providing the most insight possible. If you have a good understanding about why you are doing an analysis, it can help you connect your work with other goals and be more effective at solving larger problems.
    - Start with words and visuals: To help avoid the illusion of agreement, start with a description and a quick visual of what you are trying to convey.
    - Communicate often: use resources like notes or change-log to create a shareable report.
    
    It is important to concentrate on what matters and not get distracted. By asking yourself a few simple questions at the beginning of each task, you can ensure that you're able to stay focused on your objective while still balancing stakeholder needs:
    
    1. who are the primary and secondary stakeholders?
    2. who is managing the data. By understanding who's managing the data, you can spend your time more productively.
    3. where can you go for help?
    
    ## COMMUNICATION IS KEY
    
    You need effective communication. Before you communicate, think about:
    
    - who your audience is
    - what they already know
    - what they need to know
    - how you can communicate that effectively to them.
    
    When you communicate thoughtfully and think about your audience first, you'll build better relationships and trust with your team members and stakeholders.
    
    ### Tips for effective communication
    
    - learn as you go and ask questions when you aren't sure of something.
    - You'll want your emails to be just as professional as your in-person communications.
        - Good writing practices will go a long way to make your emails professional and easy to understand.
        - read important emails out loud before you hit send; that way, I can hear if they make sense and catch any typos.
        - the tone of your emails can change over time, but being professional is always a good place to start.
        - Don’t let your emails to be too long. You'll want to make sure that your emails are clear and concise so they don't get lost in the shuffle. If what you need to say is too long for a meeting, you might want to set up a meeting instead.
        - Answer in a timely manner.
    - set a reasonable and realistic timeline for the project. Setting expectations for a realistic timeline will help you in the long run.
    - Flag problems early for stakeholders.
    - Set realistic expectations at every stage of the project.
    - You have to make sure that the data tells you the stories. Sometimes people think that data can answer everything and sometimes we have to acknowledge that that is simply untrue.
    - Communication is one of the most valuable tools for working with teams. It's important to start with structured thinking and a well-planned scope of work. If you start with a clear understanding of your stakeholders' expectations, you can then develop a realistic scope of work that outlines agreed upon expectations, timelines, milestones, and reports.
    
    ### Speed VS accuracy
    
    At the end of the day, it's your job to balance fast answers with the right answers.
    
    - Take time to address the bigger problem. You could re-frame the question, outline the problem, challenges, potential solutions, and time-frame. You might say, "I can certainly check out the rates of completion, but I sense there may be more to the story here. Could you give me two days to run some reports and learn what's really going on?”
    - With more time, you can gain context.
    - Redirecting the conversation will help you find the real problem which leads to more insightful and accurate solutions.
    - Communicating about problems, potential solutions and different expectations can help you move forward on a project instead of getting stuck.
    - The fastest answer and the most accurate answer aren't usually the same answer. But by making sure that you understand their needs and setting expectations clearly, you can balance speed and accuracy.
    
    ### Limitations of Data
    
    - Incomplete or nonexistent data: You can still use the data, but you will need to make the limits of your analysis clear. But to be safe, you should be up front about the incomplete dataset until that data becomes available.
    - Don’t miss misaligned data: establishing how to measure things early on standardizes the data across the board for greater reliability and accuracy. This will make sure comparisons between teams are meaningful and insightful.
    - Deal with dirty data: When you find and fix the errors - while tracking the changes you made - you can avoid a data disaster.
    - Tell a clear story:
        - compare the same types of data and double check that any segments in your chart definitely display different metrics.
        - Visualize with care. To make sure your audience sees the full story clearly, it is a good idea to set your Y-axis to 0.
        - Leave out needless graphs.
        - Run statistical tests to see how much confidence you can place in that difference.
        - If you find that you have too little data, be careful about using it to form judgments. Look for opportunities to collect more data, then chart those trends over longer periods.
    
    When you know the limitations of your data, you can make judgment calls that help people make better decisions supported by the data.
    
    ### Think about your process and outcome
    
    does your analysis answer the original question? Are there other angles you haven't considered? Can you answer any questions that may get asked about your data and analysis? How detailed should you be when sharing your results? Would a high level analysis be okay?. Above all else, your data analysis should help your team make better, more informed decisions.
    
    ## AMAZING TEAMWORK
    
    ### Meeting best practices: Do and don’t
    
    Meetings are a huge part of how you communicate with team members and stakeholders.
    
    | DO’S | DON’TS |
    | --- | --- |
    | Come prepare. Read the meeting agenda ahead of time and be ready to provide any updates on your work. | Show up unprepared |
    | Be on time. | Arrive late |
    | Pay attention. Also means asking questions when you need clarification | Be distracted. |
    | Ask questions | Dominate the conversation |
    |  | Talk over others |
    |  | Distract people with unfocused discussions |
    
    ### Leading great meetings
    
    Before the meeting you should:
    
    - Identify your objective. Establish the purpose, goals, and desired outcomes of the meeting, including any questions or requests that need to be addressed.
    - Acknowledge participants and keep them involved with different points of view and experiences with the data, the project, or the business.
    - Organize the data to be presented.
    - Prepare and distribute an agenda. An agenda should include some basic parts as:
        - Meeting start and end time
        - Meeting location, including information to participate remotely, if that option is available.
        - Objectives
        - Background material or data the participants should review beforehead.
    
    During the meeting:
    
    - Make introductions (if necessary) and review key messages
    - Present the data
    - Discuss observations, interpretations, and implications of the data
    - Take notes during the meeting
    - Determine and summarize the next steps for the group
    
    After the meeting:
    
    - Distribute any notes or data
    - Confirm next steps and timeline for additional actions
    - Ask for feedback (this is an effective way to figure out if you missed anything in your recap)
    
    ### From conflict to collaboration
    
    A conflict can pop up for a variety of reasons, and there are some ways to resolve it and move foward:
    
    - Try and be objective and stay focused on the team's goals.
    - re-frame the problem. Instead of focusing on what went wrong or who to blame, change the question you're starting with. Try asking, how can I help you reach your goal?
    - If you find yourself in the middle of a conflict, try to communicate, start a conversation or ask things like, are there other important things I should be considering?
    - If you find yourself feeling emotional, give yourself some time to cool off so you can go into the conversation with a clearer head.
    - If you find you don't understand what your team member or stakeholder is asking you to do, try to understand the context of their request. Ask them what their end goal is, what story they're trying to tell with the data or what the big picture is.
    
- 3 - **PREPARE** DATA FOR EXPLORATION
    
    when you prepare the data correctly, understand the different types of data and structure that comes in. Knowing this lets you figure out what type of data is right for the question you're answering. Plus, you'll gain practical skills about how to extract, use, organize, and protect your data.
    
    ### How data is collected
    
    - Interview
    - Observations
    - Forms
    - Questionnaires
    - Surveys
    - Cookies. Cookies are small files stored on computers that contain information about users. Cookies can help inform advertisers about your personal interests and habits based on your online surfing, without personally identifying you (track people's online activities and interests).
    
    ### Factors to consider when you are collecting data
    
    - How the data will be collected. Decide if you will collect the data using your own resources or receive (and possibly purchase it) from another party.
        - First-party data: Data collected by an individual or group using their own resources. It is typically the preferred method because you know exactly where it came from.
    - Choose data sources: If you don’t collect the data using your own resources, you might get data from second-party or third-party data providers.
        - Second-party data: data collected by a group directly from its audience and then sold.
        - Third-party data: Data collected from outside sources who did not collect it directly. This data might have come from a number of different sources before you investigated it. It might not be as reliable, but that doesn't mean it can't be useful. You'll just want to make sure you check it for accuracy, bias, and credibility.
    - Decide what data to use. Be sure to choose data that can actually help solve your problem question.
    - How much data to collect. If you are collecting your own data, make reasonable decisions about sample size, or we may want to choose a sample of the population if it is too challenging. Each project has its own needs.
    - Select the right data type
    - Determine the time frame. If you are collecting your own data, decide how long you will need to collect it, especially if you are tracking trends over a long period of time. If you need an immediate answer, you might not have time to collect new data. In this case, you would need to use historical data that already exists.
    
    ![Untitled](Google_Data_Analytics_images/Untitled%2018.png)
    
    ## DIFFERENTIATE BETWEEN DATA FORMATS AND STRUCTURES
    
    ### Qualitative data
    
    it can't be counted, measured, or easily expressed using numbers. Qualitative data is usually listed as a name, category, or description. 
    
    - **Nominal data:** it’s data categorized without a set order (this data doesn't have a sequence). For example:
        - First time customer, returning customer, regular customer
        - New job applicant, existing applicant, internal applicant
        - New listing, reduced price listing, foreclosure
        - Yes/no questions
    - **Ordinal data:** it is a type of qualitative data with a set order or scale. For example:
        - Movie ratings (number of stars: 1 star, 2 stars, 3 stars)
        - Ranked-choice voting selections (1st, 2nd, 3rd)
        - Income level (low income, middle income, high income)
    
    ### Quantitative data
    
    It can be measured or counted and then expressed as a number. This is data with a certain quantity, amount, or range. We can break it down into discrete or continuous data
    
    - **Discrete data:** This is data that's counted and has a limited number of values. When partial measurements (half-stars or quarter-points) aren't allowed, the data is discrete. If you don't accept anything other than full stars or points, the data is considered discrete.
    - **Continuous data:** data can be measured using a timer, and its value can be shown as a decimal with several places.
    
    ### Internal data
    
    It is data that lives within a company's own systems. It's usually more reliable and easier to collect
    
    ### External data
    
    Data that lives and is generated outside of an organization. External data becomes particularly valuable when your analysis depends on as many sources as possible; and it is structured.
    
    ### Structured data
    
    Data that's organized in a certain format, such as rows and columns. Spreadsheets and relational databases are two examples of software that can store data in a structured way. We will be working with structured data most of the time. Structured data works nicely within a data model. Data models help to keep data consistent and provide a map of how data is organized. Structured data can be applied directly to charts, graphs, heat maps, dashboards and most other visual representations of data.
    
    - Spreadsheets
    - Databases that store datasets
    
    ### Unstructured data
    
    Data that is not organized in any easily identifiable manner (there's no clear way to identify or organize their content). Unstructured data might have an internal structure, but the data doesn't fit neatly in rows and columns like structured data. For example (These can be harder to analyze in their unstructured format.):
    
    - Social media posts
    - Emails
    - Videos and audio files
    - Photos
    
    ![Untitled](Google_Data_Analytics_images/Untitled%2019.png)
    
    | **STRUCTURED DATA** | **UNSTRUCTURED DATA** |
    | --- | --- |
    | Defined data types | Varied data types |
    | Most often quantitative data | Most often qualitative data |
    | Easy to organize | Difficult to search |
    | Easy to search | Provides more freedom for analyisis |
    | Easy to analyze | Stored in data lakes, data warehouses, and NoSQL databases |
    | Stored in relational databases and data warehouses | Can’t be put in rows and columns |
    | Examples: Excel, Google sheets, SQL, customer data, phone records, transaction history | Examples: text messages, social media comments, phone call transcriptions, various log files, images, audio, video |
    
    The lack of structure makes unstructured data difficult to search, manage, and analyze. But recent advancements in artificial intelligence and machine learning algorithms are beginning to change that. Now, the new challenge facing data scientists is making sure these tools are inclusive and unbiased. Otherwise, certain elements of a dataset will be more heavily weighted and/or represented than others. And an unfair dataset does not accurately represent the population, causing skewed outcomes, low accuracy levels, and unreliable analysis.
    
    ## DATA MODELING LEVELS AND TECHNIQUES
    
    Data models help keep data consistent and enable people to map out how data is organized. Data modeling is the process of creating diagrams that visually represent how data is organized and structured. These visual representations are called data models. Different users might have different data needs, but the data model gives them an understanding of the structure as a whole.
    
    ### The three most common types of data modeling
    
    - **Conceptual data modeling:** gives a high-level view of the data structure, such as how data interacts across an organization. A conceptual data model doesn't contain technical details.
    - **Logical data modeling:** focuses on the technical details of a database such as relationships, attributes, and entities.
    - **Physical data modeling:** depicts (describes) how a database operates. A physical data model defines all entities and attributes used
    
    ![Untitled](Google_Data_Analytics_images/Untitled%2020.png)
    
    ### Data-modeling techniques
    
    There are two common methods:
    
    - **Entity Relationship Diagram (ERD):** a visual way to understand the relationship between entities in the data model.
    - **Unified Modeling Language (UML) diagram:** are very detailed diagrams that describe the structure of a system by showing the system's entities, attributes, operations, and their relationships.
    
    There are different data modeling techniques, but in practice, you will probably be using your organization’s existing technique.
    
    ### Data analysis and data modeling
    
    Data modeling can help you explore the high-level details of your data and how it is related across the organization’s information systems. Data modeling sometimes requires data analysis to understand how the data is put together; that way, you know how to map the data. And finally, data models make it easier for everyone in your organization to understand and collaborate with you on your data.
    
    ## DATA TYPES, FIELDS AND VALUES
    
    A data type is a specific kind of data attribute that tells what kind of value the data is. A data type tells you what kind of data you're working with. Data types can be different depending on the query language you're using.
    
    ### Data types in spreadsheets
    
    - Number
    - Text or string: a sequence of characters and punctuation that contains textual information. It can also contain numbers, but wouldn’t be used for calculations, so they are treated like texts, not numbers.
    - Boolean: A data type with only two possible values, such as TRUE or FALSE.
    
    ### Boolean logic
    
    Data analysts use Boolean statements to do a wide range of data analysis tasks, such as creating queries for searches and checking for conditions when writing programming code. The boolean logic is easy to understand with a Venn diagram:
    
    ![Untitled](Google_Data_Analytics_images/Untitled%2021.png)
    
    - AND: is the center of the Venn diagram, where two conditions overlap. The AND operator lets you stack multiple conditions.
    - OR: includes either condition. The OR operator lets you move forward if either one of your two conditions is met.
    - NOT: includes only the part of the Venn diagram that doesn't contain the exception. The NOT operator lets you filter by subtracting specific conditions from the results.
    
    ### Data table components
    
    A data table, or tabular data is arranged in rows and columns. You can call the rows "records" and the columns "fields." They basically mean the same thing, but records and fields can be used for any kind of data table, while rows and columns are usually reserved for spreadsheets. People in data analytics usually go with "records" and "fields”. Sometimes a field can also refer to a single piece of data, like the value in a cell. Each separate field has the same data type, but different fields can have different types.
    
    ### Wide data and long data
    
    - Wide data:
        - Every data subject has a single row with multiple columns to hold the values of various attributes of the subject.
        - each row contains multiple data points for the particular items identified in the columns.
        - It lets you easily identify and quickly compare different columns.
        - Wide data is easier to read and understand. That is why data analysts typically transform long data to wide data more often than they transform wide data to long data.
        - Each column contains a unique data variable.
        
        ![Untitled](Google_Data_Analytics_images/Untitled%2022.png)
        
    - Long data:
        - Each row is one time point per subject, so each subject will have data in multiple rows.
        - each row contains a single data point for a particular item.
        - It is a great format for storing and organizing data when there's multiple variables for each subject at each time point that we want to observe.
        - We can store and analyze all of this data using fewer columns. Plus, if we added a new variable we'd only need one more column.
        - The long data format keeps everything nice and compact.
        - Separate columns contain the values and the context for the values
        
        ![Untitled](Google_Data_Analytics_images/Untitled%2023.png)
        
    
    | **Wide data is preferred when:** | **Long data is preferred when:** |
    | --- | --- |
    | Creating tables and charts with a few variables about each subject | Storing a lot of variables about each subject. For example, 60 years worth of interest rates for each bank |
    | Comparing straightforward line graphs | Performing advanced statistical analysis or graphing |
    
    ### Transforming data
    
    Data transformation is the process of changing the data’s format, structure, or values. Data transformation usually involves:
    
    - Adding, copying, or replicating data
    - Deleting fields or records
    - Standardizing the names of variables
    - Renaming, moving, or combining columns in a database
    - Joining one set of data with another
    - Saving a file in a different format. For example, saving a spreadsheet as a comma-separated values (CSV) file.
    
    The goals for data transformation might be:
    
    - Data **organization**: a better organized data is easier to use
    - Data **compatibility**: different applications or systems can then use the same data
    - Data **migration**: data with matching formats can be moved from one system to another
    - Data **merging**: data with the same organization can be merged together
    - Data **enhancement**: data can be displayed with more detailed fields
    - Data **comparison**: apples-to-apples comparisons of the data can then be made
    
    ## ANALYZE DATA FOR BIAS AND CREDIBILITY
    
    Even the most sound data can be skewed or misinterpreted.
    
    ### Biased and unbiased data
    
    Bias is a preference in favor of or against a person, group of people, or thing. It can be conscious or subconscious. We're biased when we have preferences based on our own preconceived or even subconscious notions.
    
    Data bias is a type of error that systematically skews results in a certain direction.
    
    Bias can also happen if a sample group lacks inclusivity. The way you collect data can also bias a data set.
    
    ### Types of data bias
    
    - **Sampling bias:** A sample that isn't representative of the population as a whole. You can avoid this by making sure the sample is chosen at random, so that all parts of the population have an equal chance of being included. If you don't use random sampling during data collection, you end up favoring one outcome.
        - Unbiased sampling results in a sample that's representative of the population being measured. Another great way to discover if you're working with unbiased data is to bring the results to life with visualizations.
    - **Observer bias** (sometimes referred to as experimenter bias or research bias): it's the tendency for different people to observe things differently.
    - **Interpretation bias:** The tendency to always interpret ambiguous situations in a positive, or negative way. It can lead to two people seeing or hearing the exact same thing, and interpreting it in a variety of different ways, because they have different backgrounds, and experiences.
    - **Confirmation bias:** the tendency to search for, or interpret information in a way that confirms preexisting beliefs. This happens all the time in everyday life.
    
    All these types of data bias are unique, but they have one thing in common: They each affect the way we collect, and make sense of the data.
    
    ## DATA CREDIBILITY
    
    There's some best practices to follow that'll help you measure the reliability of data sets before you use them.
    
    ### How to identify “good” data
    
    We can apply the process “ROCC”:
    
    - **Reliable.** With this data you can trust that you're getting accurate, complete and unbiased information that's been vetted and proven fit for use.
    - **Original.** To make sure you're dealing with good data, be sure to validate it with the original source.
    - **Comprehensive.** The best data sources contain all critical information needed to answer the question or find the solution.
    - **Current.** The best data sources are current and relevant to the task at hand. For example:
        - data.gov, which is home to the U.S. government's open data.
    - **Cited.** Citing makes the information you're providing more credible. When you're choosing a data source, think about three things. Who created the data set? Is it part of a credible organization? When was the data last refreshed?
    
    If you have original data from a reliable organization and it's comprehensive, current, and cited, it ROCCCs! (or more seriously: it's good).
    
    For good data, stick with vetted public data sets, academic papers, financial data and governmental agency data.
    
    ### How to identify “bad” data
    
    They're not reliable, original, comprehensive, current or cited. Even worse, they could be flat-out wrong or filled with human error.
    
    - **NOT Reliable.** Bad data can't be trusted because it's inaccurate, incomplete, or biased. For example:
        - data that has sample selection bias because it doesn't reflect the overall population
        - data visualizations and graphs that are just misleading.
    - **NOT Original.** If you can't locate the original data source and you're just relying on second or third party information, that can signal you may need to be extra careful in understanding your data.
    - **NOT Comprehensive.** Bad data sources are missing important information needed to answer the question or find the solution. What's worse, they may contain human error, too.
    - **NOT Current.** Bad data sources are out of date and irrelevant.
    - **NOT Cited.** If your source hasn't been cited or vetted, it's a no-go.
    
    It's important for data analysts to understand and keep an eye out for bad data because it can have serious and lasting impacts. Whether it's an incorrect conclusion leading to one bad business decision, or inaccurate information causing processes to fail and putting populations at risk, every good solution is found by avoiding bad data.
    
    ## DATA ETHICS AND PRIVACY
    
    One practical view is that ethics refers to well-founded standards of right and wrong that prescribe what humans ought to do, usually in terms of rights, obligations, benefits to society, fairness or specific virtues.
    
    Data ethics refers to well- founded standards of right and wrong that dictate how data is collected, shared, and used.
    
    The GDPR (General Data Protection Regulation of the European Union) of the European Union is a data protection legislation to help protect people and their data.
    
    Data ethics tries to get to the root of the accountability companies have in protecting and responsibly using the data they collect.
    
    ### Aspects of data ethics (data ethics concerns)
    
    - **Ownership:** This answers the question who owns data?: individuals who own the raw data they provide, and they have primary control over its usage, how it's processed and how it's shared.
    - **Transaction transparency:** All data processing activities and algorithms should be completely explainable and understood by the individual who provides their data. This is in response to concerns over data bias.
    - **Consent:** an individual's right to know explicit details about how and why their data will be used before agreeing to provide it. They should know answers to questions like why is the data being collected? How will it be used? How long will it be stored?. The best way to give consent is probably a conversation between the person providing the data and the person requesting it, but it usually looks like a terms and conditions checkbox with links to more details. Consent is important because it prevents all populations from being unfairly targeted which is a very big deal for marginalized groups who are often disproportionately misrepresented by biased data.
    - **Currency:** Individuals should be aware of financial transactions resulting from the use of their personal data and the scale of these transactions.
    - **Privacy:** Privacy is personal. We may all define privacy in our own way, and we're all entitled to it. When talking about data, privacy means preserving a data subject's information and activity any time a data transaction occurs. This is sometimes called information privacy or data protection. It's all about access, use, and collection of data. It also covers a person's legal right to their data; this means people should have:
        - protection from unauthorized access to our private data
        - freedom from inappropriate use of our data
        - the right to inspect, update, or correct our data
        - ability to give consent to use our data
        - legal right to access our data
        
        For companies, it means putting privacy measures in place to protect the individuals' data. The importance of data privacy has been recognized by governments worldwide, and they've started creating data protection legislation to help protect people and their data.
        
    - **Openness:** When referring to data, openness refers to free access, usage and sharing of data. We should still be transparent, respect privacy, and make sure we have consent for data that's owned by others.
    
    ### Data anonymization
    
    Personally identifiable information, or PII, is information that can be used by itself or with other data to track down a person's identity (is data that is reasonably likely to identify a person and make information known about them). Data anonymization is the process of protecting people's private or sensitive data by eliminating that kind of information. Typically, data anonymization involves blanking, hashing, or masking personal information, often by using fixed-length codes to represent data columns, or hiding data with altered values.
    
    As a data analyst, you might be expected to understand what data needs to be anonymized, but you generally wouldn't be responsible for the data anonymization itself. A rare exception might be if you work with a copy of the data for testing or development purposes. In this case, you could be required to anonymize the data before you work with it.
    
    Healthcare and financial data are two of the most sensitive types of data. Data in these two industries usually goes through de-identification, which is a process used to wipe data clean of all personally identifying information. The data that is often anonymized can be:
    
    - Telephone numbers
    - Names
    - License plates and license numbers
    - Social security numbers
    - IP addresses
    - Medical records
    - Email addresses
    - Photographs
    - Account numbers
    
    Data anonymization is one of the ways we can keep data private and secure!
    
    ## OPEN DATA
    
    Open data is part of data ethics, which has to do with using data ethically. Openness refers to free access, usage, and sharing of data.
    
    ### Open-data standards
    
    Open data must meet three standards:
    
    - Be available and accessible to the public as a complete dataset. Be available as a whole, preferably by downloading over the Internet in a convenient and modifiable form. The website [data.gov](http://data.gov/) is a great example.
    - Be provided under terms that allow it to be reused and redistributed. Open data must be provided under terms that allow reuse and redistribution including the ability to use it with other datasets.
    - Universal participation. Everyone must be able to use, reuse, and redistribute the data. There shouldn't be any discrimination against fields, persons, or groups. No one can place restrictions on the data like making it only available for use in a specific industry.
    
    ### Pros or benefits of open data
    
    Just imagine the impact that would have on scientific collaboration, research advances, analytical capacity, and decision-making.
    
    - credible databases can be used more widely.
    - All of that good data can be leveraged, shared, and combined with other data.
    - Interoperability is key to open data's success. But this kind of interoperability requires a lot of cooperation. While there is serious potential in the open, timely, fair, and simple sharing of data, its future will depend on how effectively larger challenges are addressed.
    - It improves public service by giving people ways to be a part of public planning or provide feedback to the government.
    - open data leads to innovation and economic growth by helping people and companies better understand their markets.
    
    ### Sites and resources for open data
    
    - [U.S. government data site]([https://data.gov/](https://data.gov/)): This resource gives users the data and tools that they need to do research, and even helps them develop web and mobile applications and design data visualizations.
    - [U.S. Census Bureau]([https://www.census.gov/data.html](https://www.census.gov/data.html)): It offers demographic information from federal, state, and local governments, and commercial entities in the U.S. too.
    - [Open Data Network]([https://www.opendatanetwork.com/](https://www.opendatanetwork.com/)): It has a really powerful search engine and advanced filters. Here, you can find data on topics like finance, public safety, infrastructure, and housing and development.
    - [Google Cloud Public Datasets]([https://cloud.google.com/datasets?hl=es-419](https://cloud.google.com/datasets?hl=es-419)): There are a selection of public datasets available through the Google Cloud Public Dataset Program that you can find already loaded into BigQuery.
    - [Dataset Search]([https://datasetsearch.research.google.com/](https://datasetsearch.research.google.com/)): It is a search engine designed specifically for data sets; you can use this to search for specific data sets.
    - [Kaggle datasets]([https://www.kaggle.com/datasets](https://www.kaggle.com/datasets)): you can create a new dataset or search for datasets created by other Kagglers.
    
    ## WORKING WITH DATABASES
    
    Databases store and organize data, making it much easier for data analysts to manage and access information. They help us get insights faster, make data-driven decisions, and solve problems.
    
    ### Database features
    
    - **Relational database:** is a database that contains a series of related tables that can be connected via their relationships. They allow data analysts to organize and link data based on what the data has in common. For two tables to have a relationship, one or more of the same fields must exist inside both tables.
        
        In a non-relational table, you will find all of the possible variables you might be interested in analyzing all grouped together. This is one reason why relational databases are so common in data analysis: they simplify a lot of analysis processes and make data easier to find and use across an entire database.
        
    - **Primary key:** is an identifier that references a column in which each value is unique, meaning no two rows can have the same primary key, and it cannot be null or blank. In other words, it's a column of a table that is used to uniquely identify each record within that table.
        - A primary key is used to ensure data in a specific column is unique.
        - It uniquely identifies a record in a relational database table.
        - Only one primary key is allowed in a table
        - Cannot contain null or blank values.
        - Some tables don't require a primary key.
    - **Foreign keys:** is a column or group of columns in a relational database table that provides a link between the data and two tables. These keys are what create the relationships between tables in a relational database, which helps organize and connect data across multiple tables in the database.
        - It is a field within a table that's a primary key in another table.
        - A foreign key is how one table can be connected to another.
        - More than one foreign key is allowed to exist in a table.
    - **Composite key:** is a candidate key that consists of two or more attributes (table columns) that together uniquely identify an entity occurrence (table row). A primary key may also be constructed using multiple columns of a table. This type of primary key is called a composite key.
    - **SQL:** Databases use a special language to communicate called a query language. Structured Query Language (SQL) is a type of query language that lets data analysts communicate with a database.
    
    ### Database normalization
    
    Normalization is a process of organizing data in a relational database. For example, creating tables and establishing relationships between those tables. It is applied to eliminate data redundancy, increase data integrity, and reduce complexity in a database.
    
    ### Inspecting a dataset
    
    Inspecting your dataset will help you pinpoint what questions are answerable and what data is still missing. You may be able to recover this data from an external source or at least recommend to your stakeholders that another data source be used.
    
    ## METADATA
    
    Metadata is not the data itself, instead is data about data. Metadata is extremely important when working with databases. Think of it like a reference guide. Metadata tells you where the data comes from, when and how it was created, and what it's all about.
    
    Metadata is information that's used to describe the data that's contained in something, like a photo or an email.
    
    In database management, it provides information about other data and helps data analysts interpret the contents of the data within a database.
    
    In essence, metadata tells the who, what, when, where, which, how, and why of data.
    
    ### Elements of metadata
    
    - **Title and description:** What is the name of the file or website you are examining? What type of content does it contain?
    - **Tags and categories:** What is the general overview of the data that you have? Is the data indexed or described in a specific way?
    - **Who created it and when:** Where did the data come from, and when was it created? Is it recent, or has it existed for a long time?
    - **Who last modified it and when:** Were any changes made to the data? If yes, were the modifications recent?
    - **Who can access or update it:** Is this dataset public? Are special permissions needed to customize or modify the dataset?
    
    ### Types of metadata
    
    As a data analyst, there are three common types of metadata that you’ll come across:
    
    - **Descriptive:** metadata that describes a piece of data and can be used to identify it at a later point in time.
    - **Structural:** metadata that indicates how a piece of data is organized and whether it's part of one or more than one data collection. Structural metadata also keeps track of the relationship between two things.
    - **Administrative:** metadata that indicates the technical source of a digital asset.
    
    ### Examples of metadata
    
    - Photos. Whenever a photo is captured with a camera, metadata such as camera filename, date, time, and geolocation are gathered and saved with it.
    - Emails. When an email is sent or received, there is lots of visible metadata such as subject line, the sender, the recipient and date and time sent. There is also hidden metadata that includes server names, IP addresses, HTML format, and software details.
    - Spreadsheets and documents. Titles, author, creation date, number of pages, user comments as well as names of tabs, tables, and columns are all metadata that one can find in spreadsheets and documents.
    - Websites. Every web page has a number of standard metadata fields, such as tags and categories, site creator’s name, web page title and description, time of creation and any iconography.
    - Digital files. Usually, if you right click on any computer file, you will see its metadata. This could consist of file name, file size, date of creation and modification, and type of file.
    - Books. Metadata is not only digital. Every book has a number of standard metadata on the covers and inside that will inform you of its title, author’s name, a table of contents, publisher information, copyright description, index, and a brief description of the book’s contents.
    
    ### Benefits of using metadata
    
    - Putting data into context is probably the most valuable thing that metadata does.
    - Metadata creates a single source of truth by keeping things consistent and uniform. Plus, when a database is consistent, it's so much easier to discover relationships between the data inside it and the data elsewhere.
    - Metadata makes data more reliable by making sure it's accurate, precise, relevant, and timely. This also makes it easier for data analysts to identify the root causes of any problems that might pop up.
    
    One of the ways data analysts make sure their data is consistent and reliable is by using something called a metadata repository. 
    
    ### Important aspects of metadata
    
    metadata is stored in a single, central location and it gives the company standardized information about all of its data. This is done in 2 ways:
    
    1. Metadata includes information about where each system is located and where the data sets are located within those systems.
    2. The metadata describes how all of the data is connected between the various systems.
    
    Another important aspect of metadata is something called data **governance**. Data governance is a process to ensure the formal management of a company’s data assets. This gives an organization better control of their data and helps a company manage issues related to data security and privacy, integrity, usability, and internal and external data flows.
    
    Data governance is about more than just standardizing terminology and procedures. It's about the roles and responsibilities of the people who work with the metadata every day. These are metadata specialists, and they organize and maintain company data, ensuring that it's of the highest possible quality. These people create basic metadata identification and discovery information, describe the way different data sets work together, and explain the many different types of data resources. Metadata specialists also create very important standards that everyone follows and the models used to organize the data. Metadata analysts are great team players. They're passionate about making data accessible by sharing with colleagues and other stakeholders.
    
    ### Metadata repository
    
    A metadata repository is a database specifically created to store metadata. These repositories describe where metadata came from, keep it in an accessible form so it can be used quickly and easily, and keep it in a common structure for everyone who may need to use it. Metadata repositories make it easier and faster to bring together multiple sources for data analysis. They do this by:
    
    - describing the state and location of the metadata
    - describing the structure of the tables inside
    - describing how data flows through the repository
    - Keeping track of who accesses the metadata and when
    
    They help ensure that my team is pulling the right content for the particular project and using it appropriately. We can confirm this because the metadata clearly describes how and when the data was collected, how it's organized, and much more.
    
    Using a metadata repository, a data analyst can find it easier to bring together multiple sources of data, confirm how or when data was collected, and verify that data from an outside source is being used appropriately.
    
    ## ACCESSING DIFFERENT DATA SOURCES
    
    There are two basic types of data used by data analysts: internal (also described as primary data) and external (sometimes called secondary data).
    
    ### Benefits of internal and external data
    
    | INTERNAL | EXTERNAL |
    | --- | --- |
    | It provides information that's relevant to problems you're trying to solve | Its is used when internal data doesn’t give you the full picture |
    | it's free to access because the company already owns it | use their data to create deeper analyses and add some more industry- level perspective. |
    | analysts can work on all data projects without ever looking beyond their own walls. |  |
    
    ### Importing data from other spreadsheets
    
    - Google Sheets: You can use the **IMPORTRANGE** function. It enables you to specify a range of cells in the other spreadsheet to duplicate in the spreadsheet you are working in. You must allow access to the spreadsheet containing the data the first time you import the data.
    - Microsoft excel:
        1. Select **Data** from the main menu.
        2. Click **Get Data**, and then select **From File** within the toolbar. In the drop down, choose **From Excel Workbook**
        3. Browse for and select the spreadsheet file and then click **Import**.
        4. In the Navigator, select which worksheet to import.
        5. Click **Load** to import all the data in the worksheet; or click **Transform Data** to open the Power Query Editor to adjust the columns and rows of data you want to import.
        6. If you clicked Transform Data, click **Close & Load** and then select one of the two options:
            - **Close & Load** - import the data to a new worksheet
            - **Close & Load to...** - import the data to an existing worksheet
    
    ### Importing data from CSV files
    
    CSV files use plain text and they're delineated by characters. So each column or field is clearly distinct from another when importing. CSVs are comma-separated, and usually the spreadsheet app will auto-detect those separations. But sometimes, you might need to indicate that the separator is another character or a space by selecting the different options. A CSV file also Examine a small subset of a large dataset
    
    - Google Sheets:
        1. Open the **File** menu in your spreadsheet and select **Import** to open the Import file window.
        2. Select **Upload** and ****then ****select the CSV file you want to import.
        3. You will have a few options. For Import location, you can choose to replace the current spreadsheet, create a new spreadsheet, insert the CSV data as a new sheet, add the data to the current spreadsheet, or replace the data in a specific cell. The data will be inserted as plain text only if you uncheck the Convert text to numbers, dates, and formulas checkbox, which is the default setting. Sometimes a CSV file uses a separator like a semi-colon or even a blank space instead of a comma. For Separator type, you can select Tab or Comma, or select Custom to enter another character that is being used as the separator.
        4. Select **Import data**. The data in the CSV file will be loaded into your sheet, and you can begin using it!
        
        You can also use the **IMPORTDATA** function in a spreadsheet cell to import data using the URL to a CSV file.
        
    - Microsoft excel:
        1. Open a new or existing spreadsheet
        2. Click **Data** in the main menu and select the **From Text/CSV** option.
        3. Browse for and select the CSV file and then click **Import**.
        4. From here, you will have a few options. You can change the delimiter from a comma to another character such as a semicolon. You can also turn automatic data type detection on or off. And, finally, you can transform your data by clicking **Transform Data** to open the Power Query Editor.
        5. In most cases, accept the default settings in the previous step and click **Load** to load the data in the CSV file to the spreadsheet. The data in the CSV file will be loaded into the spreadsheet, and you can begin working with the data.
    
    ### Importing HTML tables from web pages
    
    Importing HTML tables is a very basic method to extract or "scrape" data from public web pages.
    
    - Google Sheets: you can use the **IMPORTHTML** function. It enables you to import the data from an HTML table (or list) on a web page.
    - Microsfot excel: You can import data from web pages using the **From Web** option:
        1. Open a new or existing spreadsheet.
        2. Click Data in the main menu and select the **From Web** option.
        3. Enter the URL and click OK.
        4. In the Navigator, select which table to import.
        5. Click **Load** to load the data from the table into your spreadsheet.
    
    ## SORTING AND FILTERING
    
    Having lots of data can make it difficult to quickly find and analyze the information you need. No two analytics projects are the same. Often data analysts process, view, and use data very differently, even if it comes from the exact same source. 
    
    Sorting and filtering the data in a spreadsheet helps us:
    
    - customize the way data is presented.
    - organize data so analysts can zoom in on the pieces that matter.
    
    ### Sorting
    
    Sorting involves arranging data into a meaningful order to make it easier to understand, analyze, and visualize.
    
    - Data can be sorted in ascending or descending order, and alphabetically or numerically (choosing the column > right click > sort).
        - A → Z to sort in ascending order.
        - Z → A to sort in descending order.
    - Sorting can be done across all of a spreadsheet or just in a single column or table.
    - You can also sort by multiple variables (multiple criteria sorting).
    - The details across each row are automatically kept together when sorting a particular section
    
    Anytime you're sorting data, it's always a good idea to freeze the header row first. To do this, we'll highlight the row. Then from the view menu, choose freeze and one row.
    
    ![Untitled](Google_Data_Analytics_images/Untitled%2024.png)
    
    To do multiple criteria sorting:
    
    1. Select the entire dataset
    2. Select Data and Sort range, and then choose Advanced range sorting options to view the dialog box
    3. In the dialog box, make sure that "Data has header row" is highlighted.
    4. Now add another all the sort columns you need.
    5. Finally, select Sort.
    
    ### Filter
    
    Filtering means showing only the data that meets a specific criteria while hiding the rest.
    
    - simplifies a spreadsheet by only showing us the information we need.
    - The filter temporarily hides anything that doesn't meet the condition. But note that, even though they aren't visible, they're still there. When it's time to view the entire area spreadsheet again, simply turn off the filter.
    
    ### Ways to clean your data
    
    - Sorting
    - Removing incorrect data (using filters)
    - Filling in missing data (using filters)
    - Converting data. it's sometimes necessary to change text data (words) to numeric data (numbers). To do this, you can match specific number values to the text data in each column (using find and replace is easier).
        
        ![Untitled](Google_Data_Analytics_images/Untitled%2025.png)
        
    
    ## SPREADSHEETS AND DATASETS: COMPARE AND CONTRAST
    
    | QUESTION | SPREADSHEETS | DATABASES |
    | --- | --- | --- |
    | Can be relational? | are better-suited to self-contained data, where the data exists in one place. | you can use databases to store data from external tables, allowing you to change data in several places by editing in only one place. |
    | How do they store data? | Stores data in cells | Stores data in tables. |
    | How are they used to interact with data? | You can input functions, or math equations, that allow you to manipulate this data. | The primary way to work with a relational database is to use SQL |
    | How powerful is each? | They’re a powerful tool for tasks like crunching numbers, storing lists, and budget tracking. |  |
    | What are their pros and cons when sorting? |  |  |
    | What are their pros and cons when filtering? |  |  |
    
    ## WORKING WITH LARGE DATASETS IN SQL
    
    [BigQuery]([https://cloud.google.com/bigquery/docs?hl=es-419](https://cloud.google.com/bigquery/docs?hl=es-419)) is a data warehouse on Google Cloud that data analysts can use to query, filter large datasets, aggregate results, and perform complex operations.
    
    `https://console.cloud.google.com/bigquery`
    
    ### Setting up BigQuery
    
    There are two account types without charges:
    
    - Sandbox: A Sandbox account is available at no charge and anyone with a Google account can log in and use it. There are a couple of limitations to this account type. There are a couple of limitations:
        - you get a maximum of 12 projects at a time.
        - Cannot insert new records to a database
        - Cannot update the field values of existing records.
        
        These last Data Manipulation Language or DML operations aren't supported
        
    - Free Trial: The free trial gives you access to more of what BigQuery has to offer with fewer overall limitations.
        - $300 in credit for use in Google Cloud during the first 90 days.
        - select to upgrade to a paid account to keep working in Google Cloud.
        - You will never be automatically charged
    
    ### How to: BigQuery
    
    Link to the BigQuery landing page: [BigQuery console](https://www.notion.so/console.cloud.google.com/bigquery)
    
    - Go to the SQL workspace (from the BigQuery landing page):
        
        ![Untitled](Google_Data_Analytics_images/Untitled%2026.png)
        
    - Search for public datasets.
        
        ![Untitled](Google_Data_Analytics_images/Untitled%2027.png)
        
    - Follow these steps to find and pin the bigquery-public-data.
        1. Navigate to the Explorer menu in BigQuery.
        2. Type the word public in the search box and enter.
        3. Click "Broaden search to all projects"
        4. Find the bigquery-public-data and pin it.
    - Create a new editor window with the template for a query already populated (the * does not display automatically).
        
        ![Untitled](Google_Data_Analytics_images/Untitled%2028.png)
        
        ![Untitled](Google_Data_Analytics_images/Untitled%2029.png)
        
    - Add your own data to BigQuery
        
        ![Untitled](Google_Data_Analytics_images/Untitled%2030.png)
        
        ![Untitled](Google_Data_Analytics_images/Untitled%2031.png)
        
        ![Untitled](Google_Data_Analytics_images/Untitled%2032.png)
        
        ![Untitled](Google_Data_Analytics_images/Untitled%2033.png)
        
        ![Untitled](Google_Data_Analytics_images/Untitled%2034.png)
        
    
    ### Queries
    
    ```sql
    SELECT * 
    FROM '*DatabaseName.TableName'*
    WHERE *ColumnName (condition)*
    ```
    
    - Most Queries begin with the word SELECT
    - “*” means we want to include all columns.
    - SQL queries can be written in a lot of different ways, but still provide the same results. The additional lines and spaces don't impact the query's outcome, but they keep your query organized and easier to read for yourself and others.
    - **SELECT** is the section of a query that indicates what data you want SQL to return to you
    - **FROM** is the section of a query that indicates which table the desired data comes from.
    - **WHERE** is the section of a query ****that indicates any filters you’d like to apply to your dataset
    
    ### SQL best practices
    
    Check Google drive for a SQL best practices document; here is a small resume:
    
    - use **all caps** for clause starters (e.g., SELECT, FROM, WHERE, etc.), functions (e.g., SUM()).
    - Column names should be all **lowercase**
    - Table names should be in **CamelCase**
    - Vendors of SQL databases may use slightly different variations of SQL. These variations are called **SQL dialects**. Some SQL dialects are case sensitive. BigQuery is one of them. Vertica is another. But most, like MySQL, PostgreSQL, and SQL Server, aren’t case sensitive. This means if you searched for country_code = ‘us’, it will return all entries that have 'us', 'uS', 'Us', and 'US'. This isn’t the case with BigQuery. BigQuery is case sensitive, so that same search would only return entries where the country_code is exactly 'us'. If the country_code is 'US', BigQuery wouldn’t return those entries as part of your result.
    - For the most part, it also doesn’t matter if you use single **quotes ' '** or **double quotes " "** when referring to strings. But there are two situations where it does matter what kind of quotes you use:
        1. When you want strings to be identifiable in *any* SQL dialect. A general rule across almost all SQL dialects is to use single quotes for strings. This helps get rid of a lot of confusion.
        2. When your string contains an apostrophe (this should be the only time you would use double quotes instead of single quotes) or quotation marks
    - For **comments as reminders**, it is best to use -- and be consistent with it. Keep in mind that # isn’t recognized in all SQL dialects (MySQL doesn’t recognize #). When you add a comment to a query using --, the database query engine will ignore everything in the same line after --. It will continue to process the query starting on the next line.
    - Give your columns useful names, especially when using functions, and those names should never have spaces in them (spaces are bad in SQL names. Never use spaces.) The best practice is to use **snake_case**.
    - You can also use **CamelCase** capitalization when naming your table. CamelCase capitalization means that you capitalize the start of each word, like a two-humped (Bactrian) camel.
    - **indentation** doesn’t matter in SQL. But, as a general rule, you want to keep the length of each line in a query <= 100 characters. This makes your queries easy to read.
    - If you make **comments that take up multiple lines**, you can use -- for each line. Or, if you have more than two lines of comments, it might be cleaner and easier is to use /* to start the comment and */ to close the comment. The /* and */ method for multi-line comments usually looks cleaner and helps separate the comments from the query.
    - Keep in mind that not all SQL platforms provide native script editors to write SQL code. SQL text editors give you an interface where you can write your SQL queries in an easier and color-coded way. We can use text editors like [Sublime Text](https://www.sublimetext.com/) or [Atom](https://atom.io/).
    
    ## EFFECTIVELY ORGANIZE DATA
    
    ### Benefits of organizing data
    
    - makes it easier to find and use
    - helps you avoid making mistakes during your analysis
    - helps to protect it
    
    ### Best practices when organizing data
    
    It makes all my data more accessible and useful:
    
    - **Naming conventions (file naming):** consistent guidelines that describe the content, date, or version of a file in its name. You want to use logical and descriptive names for your files to make them easier to find and use. These file naming conventions help us organize, access, process, and analyze our data.
        - Work out and agree on file naming conventions early on in a project to avoid renaming files again and again.
        - Align your file naming with your team's or company's existing file-naming conventions.
        - Ensure that your file names are meaningful; consider including information like project name and anything else that will help you quickly identify (and use) the file for the right purpose.
        - keep your file name short and sweet. They're supposed to be quick reference points that tell you what's in a file.
        - Include the date and version number in file names; common formats are YYYYMMDD for dates and v## for versions (or revisions).
        - Lead revision numbers with 0, so that if you run into double digits of revisions, it's already built into your conventions.
        - Create a text file as a sample file with content that describes (breaks down) the file naming convention and a file name that applies it.
        - Avoid spaces and special characters in file names. Instead, use dashes, underscores, or capital letters. Spaces and special characters might not be recognized by your software. Plus avoiding spaces definitely makes it easier to work in SQL.
    - **Foldering:** Organizing your files into folders helps keep project-related files together in one place. You can also break folders down into subfolders
        - Create folders and subfolders in a logical hierarchy so related files are stored together.
        - Separate ongoing from completed work so your current project files are easier to find. Archive older files in a separate folder, or in an external storage location.
        - If your files aren't automatically backed up, manually back them up often to avoid losing important work.
    - Archiving older files: move old projects to a separate location to create an archive and cut down on clutter.
    
    there are two more things you'll want to consider when organizing data for work use (in addition to the previous three):
    
    - Align your naming and storage practices with your team to avoid any confusion.
    - develop metadata practices like creating a file that outlines project naming conventions for easy reference.
    - think about how often you're making copies of data and storing it in different places. If data is stored in lots of different databases or spreadsheets, it can contradict itself and lead to mistakes later on. Relational databases can help you avoid data duplication and store your data more efficiently.
    
    ## SECURING DATA
    
    Spreadsheets come with security features already built in. Security features can be designed to keep unauthorized users from viewing certain files, or just lock your worksheets so that you don't accidentally break your formulas. This is called data security.
    
    Data security means protecting data from unauthorized access or corruption by adopting safety measures. Usually the purpose of data security is to keep unauthorized users from accessing or viewing sensitive data.
    
    As a data analyst, you'll run into Google Sheets and Excel a lot, and both have some security features in common:
    
    - Both programs have features that let you protect your spreadsheets or parts of your spreadsheets from being edited, from the entire worksheet down to single cells in a table.
    - Both have access control features like password protection and user permissions.
    
    Because these programs are located in different places, these features are slightly different:
    
    | EXCEL | GOOGLE SHEET |
    | --- | --- |
    | you can encrypt files and worksheets with passwords before emailing them to other users. | these settings are found under the sharing menu, which allows you to control who can see or edit the sheet online. |
    
    ### Data security options
    
    - **Encryption:** Encryption uses a unique algorithm to alter data and make it unusable by users and applications that don’t know the algorithm. This algorithm is saved as a “key” which can be used to reverse the encryption; so if you have the key, you can still use the data in its original form.
    - **Tokenization:** Tokenization replaces the data elements you want to protect with randomly generated data referred to as a “token.” The original data is stored in a separate location and mapped to the tokens. To access the complete original data, the user or application needs to have permission to use the tokenized data and the token mapping. This means that even if the tokenized data is hacked, the original data is still safe and secure in a separate location.
    
    There are a lot of others, like using authentication devices for AI technology
    
    ## CREATE OR ENHANCE YOUR ONLINE PRESENCE
    
    A proffesional online presence can:
    
    - help potential employers find you.
    - make connections with other data analysts in your field
    - learn and share data findings
    - participate in community events.
    
    ### LinkedIn y Github
    
    | **LINKEDIN** | **GITHUB** |
    | --- | --- |
    | Make connections | Share insights and resources |
    | Follow industry trends | Read forums and wikis |
    | Find job opportunities | Manage team projects |
    | lets you connect with people and build a network. | Hosts community events |
    
    LinkedIn has become one of the standard professional social media sites, so it's a good starting place for building your online presence. GitHub offers a lot of really great tools for data analysts in the community.
    
    ## BUILD A DATA ANALYTICS NETWORK
    
    Once you've learned the skills and developed a strong portfolio, the next step is to connect with people in your profession or industry who can help you use those strengths to build a career.
    
    Networking can be called professional relationship building. It's all about meeting people both on and offline and building relationships with them. Networking is the most effective way to connect with fellow data analysts. When you’re networking, you can meet other professionals and participate in industry-related groups. How?:
    
    - Search for public meetups in your area. Just google data analytics meetups near you or search on [meetup.com](http://meetup.com/). Then you can learn more about different types of data analytics or share your interest with other people in the field.
    - Follow interesting companies or thought leaders on LinkedIn, Twitter, Facebook, and Instagram, interact with them, and share their content.
    - There's also a ton of blogs and online communities like O'Reilly, Kaggle, KDnuggets, GitHub and Medium, that can help you connect with peers and experts.
    
    | ONLINE CONNECTIONS | IN-PERSON GATHERINGS |
    | --- | --- |
    | Subscriptions to newsletters like Data Elixir. | Conferences usually present innovative ideas and topics. |
    | Hackathons (competitions) like those sponsored by Kaggle | Associations or societies gather members to promote a field like data science. |
    | Meetups | User communities and summits offer events for users of data analysis tools |
    | Platforms like LinkedIn and Twitter. | Non-profit organizations that promote the ethical use of data science and might offer events for the professional advancement of their members. |
    | Webinars may showcase a panel of speakers and are usually recorded for convenient access and playback. |  |
    
    ### Benefits of mentorship
    
    A mentor is a professional who shares their knowledge, skills, and experience to help you develop and grow. Mentors come in many forms. They can be trusted advisors, sounding boards, critics, resources or all of the above.
    
    For instance, websites like [Score.org](http://score.org/) and [MicroMentor.org](http://micromentor.org/) and an app called Mentorship allow you to look for specific credentials that match your needs.
    
    Now, while a mentor will help you gain critical skills and navigate challenges at work, a lot of people find that having a sponsor can take their career even further. A sponsor is a professional advocate who's committed to moving a sponsee's career forward with an organization. A mentor helps you skill up, a sponsor helps you move up.
    
    First, build and nurture your LinkedIn presence. Next, look at your current social media presence and make sure it's helping you put your best foot forward. Finally, always be open to connecting with peers and colleagues. You never know what great things a conversation will bring.
    
- 4 - **PROCESS** DATA FROM DIRTY TO CLEAN
    
    Clean data is the key to making sure your data has integrity before you analyze it. So the first step in processing data is learning about data integrity. Testing data is another important step to take when processing data.
    
    ## DATA INTEGRITY AND ANALYTICS OBJECTIVES
    
    A strong analysis depends on the integrity of the data. If the data you're using is compromised in any way, your analysis won't be as strong as it should be. Data integrity is the accuracy, completeness, consistency, and trustworthiness of data throughout its lifecycle. When data integrity is low, it can cause anything from the loss of a single pixel in an image to an incorrect medical decision.
    
    Data integrity can be compromised or threatened in lots of different ways:
    
    - **Data** **Replication:** Data replication is the process of storing data in multiple locations. If you're replicating data at different times in different places, there's a chance your data will be out of sync.
    - **Data transfer:** the process of copying data from a storage device to memory, or from one computer to another. If your data transfer is interrupted, you might end up with an incomplete data set, which might not be useful for your needs.
    - **Data manipulation:** the process of changing the data to make it more organized and easier to read. Data manipulation is meant to make the data analysis process more efficient, but an error during the process can compromise efficiency.
    - Human error
    - Viruses
    - Malware
    - Hacking
    - System failures
    
    In a lot of companies, the data warehouse or data engineering team takes care of ensuring data integrity. After you've found out what data you're working with, it's important to double-check that your data is complete and valid before analysis. This will help ensure that your analysis and eventual conclusions are accurate.
    
    ### Data constraints and examples
    
    | **Data constraint** | **Definition** | **Examples** |
    | --- | --- | --- |
    | **Data type** | Values must be of a certain type: date, number, percentage, Boolean, etc. | If the data type is a date, a single number like 30 would fail the constraint and be invalid |
    | **Data range** | Values must fall between predefined maximum and minimum values | If the data range is 10-20, a value of 30 would fail the constraint and be invalid |
    | **Mandatory** | Values can’t be left blank or empty | If age is mandatory, that value must be filled in |
    | **Unique** | Values can’t have a duplicate | Two people can’t have the same mobile phone number within the same service area |
    | **Regular expression (regex) patterns** | Values must match a prescribed pattern | A phone number must match ###-###-#### (no other characters allowed) |
    | **Cross-field validation** | Certain conditions for multiple fields must be satisfied | Values are percentages and values from multiple fields must add up to 100% |
    | **Primary-key** | (Databases only) value must be unique per column | A database table can’t have two rows with the same primary key value. A primary key is an identifier in a database that references a column in which each value is unique. More information about primary and foreign keys is provided later in the program. |
    | **Set-membership** | (Databases only) values for a column must come from a set of discrete values | Value for a column must be set to Yes, No, or Not Applicable |
    | **Foreign-key** | (Databases only) values for a column must be unique values coming from a column in another table | In a U.S. taxpayer database, the State column must be a valid state or territory with the set of acceptable values defined in a separate States table |
    | **Accuracy** | The degree to which the data conforms to the actual entity being measured or described | If values for zip codes are validated by street location, the accuracy of the data goes up. |
    | **Completeness** | The degree to which the data contains all desired components or measures | If data for personal profiles required hair and eye color, and both are collected, the data is complete. |
    | **Consistency** | The degree to which the data is repeatable from different points of entry or collection | If a customer has the same address in the sales and repair databases, the data is consistent. |
    
    ![Untitled](Google_Data_Analytics_images/Untitled%2035.png)
    
    ### Balancing objectives with data integrity
    
    It's important to check that the data you use aligns with the business objective. This adds another layer to the maintenance of data integrity because the data you're using might have limitations that you'll need to deal with. Before digging into any analysis, you need to consider a few limitations that might affect it
    
    - duplicate data
    - Not enough data
    
    ### Well-aligned objectives and data
    
    You can gain powerful insights and make accurate conclusions when data is well-aligned to business objectives. Allignment is something you will need to judge. Good alignment means that the data is relevant and can help you solve a business problem or determine a course of action to achieve a given business objective.
    
    **Clean data + alignment to business objective = accurate conclusions.** When there is clean data and good alignment, you can get accurate insights and make conclusions the data supports.
    
    **Alignment to business objective + additional data cleaning = accurate conclusions.** If there is good alignment but the data needs to be cleaned, clean the data before you perform your analysis.
    
    **Alignment to business objective + newly discovered variables + constraints = accurate conclusions.** If the data only partially aligns with an objective, think about how you could modify the objective, or use data constraints to make sure that the subset of data better aligns with the business objective.
    
    ## DEALING WITH INSUFFICIENT DATA
    
    Once you know your business objective, you'll be able to recognize whether you have enough data. And if you don't, you'll be able to deal with it before you start your analysis.
    
    ### Types of insufficient data
    
    - Data from only one source. If a limitation like this impacts your analysis, you can stop and go back to your stakeholders to figure out a plan.
    - Data that keeps udating. You might want to wait a month to gather data. Or you can check in with the stakeholders and ask about adjusting the objective. For example, you might analyze trends from week to week instead of month to month. You could also base your analysis on trends over the past three months and say, "Here's what attendance at the attraction for month four could look like.”
    - Outdated data. In this case, your best bet might be to find a new data set to work with.
    - Geographically-limited data.
    
    ### Ways to address insuficient data
    
    - Identify trends with tha available data
    - Wait for more data if time allows
    - Talk with stakeholders and adjust your objective
    - Look for a new dataset
    
    ### What to do when you find an issue with your data
    
    When you are getting ready for data analysis, you might realize you don’t have the data you need or you don’t have enough of it. In some cases, you can use what is known as proxy data in place of the real data. Think of it like substituting oil for butter in a recipe when you don’t have butter. In other cases, there is no reasonable substitute and your only option is to collect more data.
    
    - no data:
    
    | **Possible Solutions** | **Examples of solutions in real life** |
    | --- | --- |
    | Gather the data on a small scale to perform a preliminary analysis and then request additional time to complete the analysis after you have collected more data. | If you are surveying employees about what they think about a new performance and bonus plan, use a sample for a preliminary analysis. Then, ask for another 3 weeks to collect the data from all employees. |
    | If there isn’t time to collect data, perform the analysis using proxy data from other datasets. 
    *This is the most common workaround.* | If you are analyzing peak travel times for commuters but don’t have the data for a particular city, use the data from another city with a similar size and demographic. |
    - Too little data
    
    | **Possible Solutions** | **Examples of solutions in real life** |
    | --- | --- |
    | Do the analysis using proxy data along with actual data. | If you are analyzing trends for owners of golden retrievers, make your dataset larger by including the data from owners of labradors. |
    | Adjust your analysis to align with the data you already have. | If you are missing data for 18- to 24-year-olds, do the analysis but note the following limitation in your report: *this conclusion applies to adults 25 years and older* *only*. |
    - wrong data, including data with errors (sometimes data with errors can be a warning sign that the data isn’t reliable. Use your best judgment.)
    
    | **Possible Solutions** | **Examples of solutions in real life** |
    | --- | --- |
    | If you have the wrong data because requirements were misunderstood, communicate the requirements again. | If you need the data for female voters and received the data for male voters, restate your needs. |
    | Identify errors in the data and, if possible, correct them at the source by looking for a pattern in the errors. | If your data is in a spreadsheet and there is a conditional statement or boolean causing calculations to be wrong, change the conditional statement instead of just fixing the calculated values. |
    | If you can’t correct data errors yourself, you can ignore the 
    wrong data and go ahead with the analysis if your sample size is still large enough and ignoring the data won’t cause systematic bias. | If your dataset was translated from a different language and some of the translations don’t make sense, ignore the data with bad translation and go ahead with the analysis of the other data. |
    
    ![Untitled](Google_Data_Analytics_images/Untitled%2036.png)
    
    ### What to do when there is no data
    
    Sometimes the data to support a business objective isn’t readily available. This is when proxy data is useful. 
    
    Null most often means that a data field was unassigned (left empty), but sometimes Null can be interpreted as the value, 0. It is important to understand how Null was used before you start analyzing a dataset with Null data.
    
    ### The importance of sample size
    
    The goal is to get enough information from a small group within a population to make predictions or conclusions about the whole population. The sample size helps ensure the degree to which you can be confident that your conclusions accurately represent the population. Using a sample for analysis is more cost-effective and takes less time.
    
    But when you only use a small sample of a population, it can lead to uncertainty. You can't really be 100 percent sure that your statistics are a complete and accurate representation of the population. This leads to sampling bias, but using random sampling can help address some of those issues with sampling bias.
    
    Random sampling is a way of selecting a sample from a population so that every possible type of the sample has an equal chance of being chosen.
    
    Companies usually create sample sizes before data analysis so analysts know that the resulting dataset is representative of a population.
    
    ### Things to remember when determining the size of your sample
    
    - Don’t use a sample size less than 30. It has been statistically proven that 30 is the smallest sample size where an average result of a sample starts to represent the average result of a population. This recommendation is based on the Central Limit Theorem (CLT) in the field of probability and statistics. As sample size increases, the results more closely resemble the normal (bell-shaped) distribution from a large number of samples.
    - The confidence level most commonly used is 95%, but 90% can work in some cases.
    - For a **higher** confidence level, use a larger sample size
    - To **decrease** the margin of error, use a larger sample size
    - For **greater** statistical significance, use a larger sample size
    - Sample size will vary based on the type of business problem you are trying to solve, and It depends on the stakes.
    - Larger sample sizes have a higher cost
    - Knowing the basics will help you make the right choices when it comes to sample size. Sample size calculators let you enter a desired confidence level and margin of error for a given population size. They then calculate the sample size needed to statistically achieve those results.
    
    ## TESTING YOUR DATA
    
    Statistical power is the probability of getting meaningful results from a test. For data analysts, your projects might begin with the test or study. Hypothesis testing is a way to see if a survey or experiment has meaningful results. Usually, the larger the sample size, the greater the chance you'll have statistically significant results with your test. And that's statistical power.
    
    Statistical power is usually shown as a value out of one. So if your statistical power is 0.6, that's the same thing as saying 60%.
    
    If a test is statistically significant, it means the results of the test are real and not an error caused by random chance. Usually, you need a statistical power of at least 0.8 or 80% to consider your results statistically significant.
    
    [A Gentle Introduction to Statistical Power and Power Analysis in Python]([https://machinelearningmastery.com/statistical-power-and-power-analysis-in-python/](https://machinelearningmastery.com/statistical-power-and-power-analysis-in-python/))
    
    "**Statistical power** can be calculated and reported for a completed experiment to comment on the confidence one might have in the conclusions drawn from the results of the study. It can also be used as a tool to estimate the number of observations or sample size required in order to detect an effect in an experiment."
    
    ### Determine the best sample size
    
    It can be both expensive and time-consuming to analyze an entire population of data. Using sample size usually makes the most sense and can still lead to valid and useful findings. There are handy calculators online that can help you find sample size. You need to input the:
    
    - **confidence level:** the probability that your sample accurately reflects the greater population. It's how strongly you feel that you can rely on something or someone. Having a 99 percent confidence level is ideal. But most industries hope for at least a 90 or 95 percent confidence level. Industries like pharmaceuticals usually want a confidence level that's as high as possible when they are using a sample size.
    - **population size.**
    - **margin of error:** tells you how close your sample size results are to what your results would be if you use the entire population that your sample size represents.
    
    Check Sample size calculator excel at google drive.
    
    The confidence level and margin of error don't have to add up to 100 percent. They're independent of each other.
    
    ### Sample size calculator
    
    A sample size calculator tells you how many people you need to interview (or things you need to test) to get results that represent the target population. When you use a sample size calculator you will come across with some terms:
    
    - **Confidence level**: The probability that your sample size accurately reflects the greater population.
    - **Margin of error**: The maximum amount that the sample results are expected to differ from those of the actual population.
    - **Population**: This is the total number you hope to pull your sample from.
    - **Sample**: A part of a population that is representative of the population.
    - **Estimated response rate**: If you are running a survey of individuals, this is the percentage of people you expect will complete your survey out of those who received the survey.
    
    In order to use a sample size calculator, you need to have the population size, confidence level, and the acceptable margin of error already decided so you can input them into the tool.
    
    You can use the next sample size calculators:
    
    - [surveymonkey]([https://www.surveymonkey.com/mp/sample-size-calculator/](https://www.surveymonkey.com/mp/sample-size-calculator/))
    - [Raosoft]([http://www.raosoft.com/samplesize.html](http://www.raosoft.com/samplesize.html))
    
    The calculated sample size is the minimum number to achieve what you input for confidence level and margin of error.
    
    ## MARGIN OF ERROR
    
    Margin of error is the maximum that the sample results are expected to differ from those of the actual population. It defines a range of values below and above the average result for the sample. The average result for the entire population is expected to be within that range.
    
    Margin of error helps you understand how reliable the data from your hypothesis testing is. The closer to zero the margin of error, the closer your results from your sample would match results from the overall population.
    
    If you've already been given the sample size, you can calculate the margin of error yourself. Then you can decide yourself how much of a chance your results have of being statistically significant based on your margin of error. In general, the more people you include in your survey, the more likely your sample is representative of the entire population.
    
    to calculate margin of error, you need three things: 
    
    - population size
    - sample size
    - confidence level
    
    Check Margin of error calculator at google Drive
    
    Calculators are just one of the many tools you can use to ensure data integrity. And it's also good to remember that checking for data integrity and aligning the data with your objectives will put you in good shape to complete your analysis.
    
    In most cases, a 90% or 95% confidence level is used. But, depending on your industry, you might want to set a stricter confidence level. A 99% confidence level is reasonable in some industries, such as the pharmaceutical industry.
    
    ### Marging of error in marketing
    
    The margin of error is also important in marketing. Let’s use A/B testing as an example. **A/B testing** (or split testing) tests two variations of the same web page to determine which page is more successful in attracting user traffic and generating revenue. User traffic that gets monetized is known as the **conversion rate**. A/B testing allows marketers to test emails, ads, and landing pages to find the data behind what is working and what isn’t working. Marketers use the **confidence interval** (determined by the conversion rate and the margin of error) to understand the results. Examining the margin of error is important when making conclusions based on your test results.
    
    ## DATA CLEANING IS A MUST
    
    According to IBM, the yearly cost of poor-quality data is $3.1 trillion in the US alone. The #1 cause of poor-quality data is human error.
    
    Dirty data can be the result of someone typing in a piece of data incorrectly, inconsistent formatting, blank fields; or the same piece of data being entered more than once, which creates duplicates.
    
    Dirty data is data that's incomplete, incorrect, or irrelevant to the problem you're trying to solve.
    
    Clean data is data that's complete, correct, and relevant to the problem you're trying to solve.
    
    ### Why data cleaning is important
    
    Clean data is incredibly important for effective analysis. If a piece of data is entered into a spreadsheet or database incorrectly, or if it's repeated, or if a field is left blank, or if data formats are inconsistent, the result is dirty data. Small mistakes can lead to big consequences in the long run.
    
    Let's talk about some people you'll work with as a data analyst:
    
    - Data engineers: transform data into a useful format for analysis and give it a reliable infrastructure. This means they develop, maintain, and test databases, data processors and related systems.
    - Data warehousing specialist: develop processes and procedures to effectively store and organize data. They make sure that data is available, secure, and backed up to prevent loss.
    
    If data passes through the hands of a data engineer or a data warehousing specialist first, you know you're off to a good start on your project.
    
    But data cleaning becomes even more important when working with external data, especially if it comes from multiple sources.
    
    ### Types of dirty data
    
    - Duplicate data: Any data record that shows up more than once. The possible causes can be:
        - Manual data entry
        - batch data imports
        - Data migration
    - Outdated data: Any data that is old which should be replaced with newer and more accurate information. The possible causes can be:
        - People changing roles or companies
        - software and systems becoming obsolete
    - Incomplete data: Any data that is missing important fields. The possible causes can be:
        - Improper data collection
        - incorrect data entry
    - Incorrect/inaccurate data: Any data that is complete but inaccurate. The possible causes can be:
        - Human error inserted during data input
        - fake information
        - mock data
    - Inconsistent data: Any data that uses different formats to represent the same thing. The possible causes can be:
        - Data stored incorrectly
        - errors inserted during data transfer
    - Labeling.
    - having an inconsistent field length. Field length is a tool for determining how many characters can be keyed into a field. Assigning a certain length to the fields in your spreadsheet is a great way to avoid errors.
    
    | TYPE | POTENTIAL HARM TO BUSINESSES |
    | --- | --- |
    | Duplicated data | Skewed metrics or analyses, inflated or inaccurate counts or predictions, or confusion during data retrieval |
    | Outdated data | Inaccurate insights, decision-making, and analytics |
    | Incomplete data | Decreased productivity, inaccurate insights, or inability to complete essential services |
    | Incorrect/inaccurate data | Inaccurate insights or decision-making based on bad information resulting in revenue loss |
    | Inconsistent data | Contradictory data points leading to confusion or inability to classify or segment customers |
    
    ## CLEANING DATA
    
    Clean data depends largely on the data integrity rules that an organization follows, such as spelling and punctuation guidelines.
    
    The techniques for data cleaning will be different depending on the specific data set you're working with. We can clean the data by:
    
    1. removing unwanted data:
        1. It's always a good practice to make a copy of the data set before.
        2. Once that's done, then you can move on to getting rid of the duplicates or data that isn't relevant to the problem you're trying to solve. Typically, duplicates appear when you're combining data sets from more than one source or using data from multiple departments within the same business.
        3. Irrelevant data, which is data that doesn't fit the specific problem that you're trying to solve, also needs to be removed.
        4. Removing irrelevant data takes a little more time and effort because you have to figure out the difference between the data you need and the data you don't.
    2. cleaning up text to remove extra spaces and blanks. Extra spaces can cause unexpected results when you sort, filter, or search through your data.
    3. fix typos. You can use spreadsheet tools, such as spellcheck, autocorrect, and conditional formatting to fix those problems.
        1. Fixing misspellings
        2. Inconsistent capitalization
        3. incorrect punctuation and other typos.
    4. make formatting consistent. This is particularly important when you get data from lots of different sources. Every database has its own formatting, which can cause the data to seem inconsistent.
    
    ### Cleaning data from multiple sources
    
    Cleaning data that comes from two or more sources is very common for data analysts, but it does come with some interesting challenges like:
    
    - Merge. All the data from each organization would need to be combined using data merging. Data merging is the process of combining two or more datasets into a single dataset. When merging datasets, We should begin by asking myself some key questions to help me avoid redundancy and to confirm that the datasets are compatible:
        - do I have all the data I need?
        - does the data I need exist within these datasets?
        - Do the datasets need to be cleaned, or are they ready for me to use?
        - are the datasets cleaned to the same standard? For example, what fields are regularly repeated? How are missing values handled? How recently was the data updated?
        
    
    ### Common data-cleaning pitfalls
    
    Some of the errors you might come across while cleaning your data could include:
    
    - Not checking for spelling errors. if you don’t double-check for spelling errors and catch this, your analysis will have mistakes in it.
    - Forgetting to document errors. Documenting your errors can be a big time saver, as it helps you avoid those errors in the future by showing you how you resolved them. It also helps you keep track of changes in your work, so that you can backtrack if a fix didn’t work.
    - Not checking for misfielded values. A misfielded value happens when the values are entered into the wrong field. These values might still be formatted correctly, which makes them harder to catch if you aren’t careful.
    - Overlooking missing values. Missing values in your dataset can create errors and give you inaccurate conclusions. As a best practice, try to keep your data as clean as possible by maintaining completeness and consistency.
    - Looking at a subset of data and not the whole picture. It is important to think about all of the relevant data when you are cleaning. This helps make sure you understand the whole story the data is telling, and that you are paying attention to all possible errors. If you want to avoid common errors like duplicates, each field of your data requires equal attention.
    - Losing track of the business objectives. Being curious is great! But try not to let it distract you from the task at hand.
    - Not fixing the source of the error. Addressing the source of the errors in your data will save you a lot of time in the long run.
    - Not analyzing the system prior to data cleaning. First, you figure out where the errors come from. Then, once you understand where bad data comes from, you can control it and keep your data clean.
    - Not backing up your data prior to data cleansing. The simple procedure of backing up your data can save you hours of work-- and most importantly, a headache.
    - Not accounting for data cleaning in your deadlines/process. When you set aside time for data cleaning, it helps you get a more accurate estimate for ETAs for stakeholders and can help you know when to request an adjusted ETA.
    
    ### Techniques to clean the data
    
    - Select and remove blank cells using filters
    - Transpose the data. It will help you convert the data from the current long format (more rows than columns) to the wide format (more columns than rows). This action is called transposing. You can do it by selecting the data, copying it and Paste Special from the right-click menu. Select the Transposed option.
    - Get rid of extra spaces in cells with string data. In Excel, you can use the TRIM command to get rid of white spaces
    - Change Text Lower/Uppercase/Proper Case. Just Click on the Add-Ons tab and select ChangeCase. Select the option All uppercase. Notice the other options that you could have chosen if needed.
    - Delete all formatting. Select the data and click the Format tab and select the Clear Formatting option.
    
    ### Data-cleaning features in spreadsheets
    
    There's a lot of great efficiency tools that data analysts use all the time, such as:
    
    - conditional formatting: a spreadsheet tool that changes how cells appear when values meet specific conditions.
    - removing duplicates. "Remove duplicates" is a tool that automatically searches for and eliminates duplicate entries from a spreadsheet. Go to data > Data deletion > remove duplicates. Choose "Data has header row”. This removes duplicated rows
    - formatting dates. Use a standard date format to make all of our dates consistent
    - fixing text strings and substrings. A text string is a group of characters within a cell, most often composed of letters. An important characteristic of a text string is its length. A substring is a smaller subset of a text string.
    - splitting text to columns. Split is a tool that divides a text string around the specified character and puts each fragment into a new and separate cell. Split is helpful when you have more than one piece of data in a cell and you want to separate them out. To do it Highlight the column, then select "Data," and "Split text to columns.” Split text to columns is also helpful for fixing instances of numbers stored as text.
        - Split text to columns is also helpful for fixing instances of numbers stored as text. Sometimes values in your spreadsheet will seem like numbers, but they're formatted as text. This can happen when copying and pasting from one place to another or if the formatting's wrong.
    
    ### Optimize the data-cleaning process (with functions)
    
    Every function has a certain syntax that needs to be followed for it to work.
    
    - COUNTIF: a function that returns the number of cells that match a specified value. Basically, it counts the number of times a value appears in a range of cells. We'll use COUNTIF to check for some common problems, like negative numbers or a value that's much less or much greater than expected.
    - LEN: a function that tells you the length of the text string by counting the number of characters it contains. This is useful when cleaning data if you have a certain piece of information in your spreadsheet that you know must contain a certain length.
    - LEFT: a function that gives you a set number of characters from the left side of a text string.
    - RIGHT: a function that gives you a set number of characters from the right side of a text string.
    - MID: a function that gives you a segment from the middle of a text string.
    - CONCATENATE: a function that joins together two or more text strings.
    - TRIM: a function that removes leading, trailing, and repeated spaces in data. Sometimes when you import data, your cells have extra spaces, which can get in the way of your analysis. TRIM fixed the extra spaces.
    - SPLIT: The SPLIT function divides text around a specified character or string, and puts each fragment of text into a separate cell in the row.
    
    ### Workflow automation
    
    Workflow automation is the process of automating parts of your work. That could mean creating an event trigger that sends a notification when a system is updated. Or it could mean automating parts of the data cleaning process.
    
    Automating different parts of your work can save you tons of time, increase productivity, and give you more bandwidth to focus on other important aspects of the job.
    
    | **Task** | **Can it be automated?** | **Why?** |
    | --- | --- | --- |
    | Communicating with your team and stakeholders | No | Communication is key to understanding the needs of your team and stakeholders as you complete the tasks you are working on. There is no replacement for person-to-person communications. |
    | Presenting your findings | No | Presenting your data is a big part of your job as a data analyst. Making data accessible and understandable to stakeholders and creating data visualizations can’t be automated for the same reasons that communications can’t be automated. |
    | Preparing and cleaning data | Partially | Some tasks in data preparation and cleaning can be automated by setting up specific processes, like using a programming script to automatically detect missing values. |
    | Data exploration | Partially | Sometimes the best way to understand data is to see it. Luckily, there are plenty of tools available that can help automate the process of visualizing data. These tools can speed up the process of visualizing and understanding the data, but the exploration itself still needs to be done by a data analyst. |
    | Modeling the data | Yes | Data modeling is a difficult process that involves lots of different factors; luckily there are tools that can completely automate the different stages. |
    
    One of the most important ways you can streamline your data cleaning is to clean data where it lives. This will benefit your whole team, and it also means you don’t have to repeat the process over and over.
    
    ### Different data perspectives
    
    - sorting and filtering: sorting and filtering data helps data analysts customize and organize the information the way they need for a particular project. But these tools are also very useful for data cleaning. For data cleaning you can use sorting to:
        - put things in alphabetical or numerical order, so you can easily find a piece of data.
        - bring duplicate entries closer together for faster identification.
        
        Filters are very useful in data cleaning when you want to find a particular piece of information. When cleaning data, you might use a filter to:
        
        - find values above a certain number
        - Find even or odd values.
    - Pivot table: In data cleaning, pivot tables are used to give you a quick, clutter- free view of your data. You can choose to look at the specific parts of the data set that you need to get a visual in the form of a pivot table.
    - VLOOKUP stands for vertical lookup. It's a function that searches for a certain value in a column to return a corresponding piece of information. When data analysts look up information for a project, it's rare for all of the data they need to be in the same place. Usually, you'll have to search across multiple sheets or even different databases.
        
        ![Untitled](Google_Data_Analytics_images/Untitled%2037.png)
        
        - To put it simply, VLOOKUP searches for the value in the first argument in the leftmost column of the specified location. Then the value of the third argument tells VLOOKUP to return the value in the same row from the specified column.
        - False at the end of the syntax means that an exact match is what we're looking for.
    - Plotting: When you plot data, you put it in a graph chart, table, or other visual to help you quickly find what it looks like. Plotting is very useful when trying to identify any skewed data or outliers.
    
    ### Even more data-cleaning techniques
    
    Knowing how to fix specific problems, either manually with spreadsheet tools, or with functions, is extremely valuable. But it's also important to think about how your data has moved between systems and how it's evolved along it's journey to your data analysis project. To do this, data analysts use something called data mapping.
    
    Data mapping is the process of matching fields from one database to another. This is very important to the success of data migration, data integration, and lots of other data management activities.
    
    The first step to data mapping is identifying what data needs to be moved. This includes the tables and the fields within them. We also need to define the desired format for the data once it reaches its destination.
    
    Depending on the schema and number of primary and foreign keys in a data source, data mapping can be simple or very complex.
    
    When selecting a software program to map your data, you want to be sure that it supports the file types you're working with, such as Excel, SQL, Tableau, and others.
    
    - Mapping data (manually):
        1. we need to determine the content of each section to make sure the data ends up in the right place. This step makes sure that each piece of information ends up in the most appropriate place in the merged data source.
        2. transforming the data into a consistent format. This is a great time to use concatenate.
        3. Now that everything's compatible, it's time to transfer the data to its destination. There's a lot of different ways to move data from one place to another, including querying, import wizards, and even simple drag and drop.
        4. testing phase of data mapping. For this, you inspect a sample piece of data to confirm that it's clean and properly formatted. It's also a smart practice to do spot checks on things such as the number of nulls. For the test, you can use a lot of the data cleaning tools we discussed previously, such as data validation, conditional formatting, COUNTIF, sorting, and filtering. Here you can ask yourself:
            1. Was all the data merged?
            2. Was data migrated correctly?
            3. Are the data formats in the merged spreadsheet consistent?
        
        Data mapping is so important because even one mistake when merging data can ripple throughout an organization, causing the same error to appear again and again. This leads to poor results.
        
    
    ## USING SQL TO CLEAN DATA
    
    SQL is a core skill and highly sought after by everybody. SQL is the primary way data analysts extract data from databases.
    
    SQL is a structured query language that analysts use to work with databases. Data analysts usually use SQL to deal with large datasets because it can handle huge amounts of data. And I mean trillions of rows. 
    
    Development on SQL actually began in the early 70s. In 1970, Edgar F.Codd developed the theory about relational databases. In 1979, after extensive testing SQL, now just spelled S-Q-L, was released publicly. By 1986, SQL had become the standard language for relational database communication, and it still is. This is another reason why data analysts choose SQL. It's a well-known standard within the community.
    
    ### Spreadsheets functions and formulas or SQL queries?
    
    When it comes down to it, where the data lives will decide which tool you use. If you are working with data that is already in a spreadsheet, that is most likely where you will perform your analysis. And if you are working with data stored in a database, SQL will be the best tool for you to use for your analysis.
    
    | **Features of Spreadsheets** | **Features of SQL Databases** |
    | --- | --- |
    | Generated with a program like Excel or google sheets | A language used to interact with database programs like Oracle MySQL or Microsoft SQL Server |
    | Smaller data sets; or when you are working independently | Larger datasets, like more than a million rows. Tracks changes across team |
    | Enter data manually. Access to the data you input | Access tables across a database. Can pull information from different sources in the database |
    | Stored locally | Stored across a database |
    | Create graphs and visualizations in the same program | Prepare data for further analysis in another software |
    | Built-in spell check and other useful functions | Fast and powerful functionality |
    | Best when working solo on a project | Great for collaborative work and tracking queries run by all users |
    
    Spreadsheets and SQL have some things in common, like there're tools you can use in both spreadsheets and SQL to achieve similar results. For example, you can still perform arithmetic, use formulas and join data when you're using SQL
    
    As a junior data analyst, it is important to know that there are slight differences between dialects. But by mastering Standard SQL, which is the dialect you will be working with in this program, you will be prepared to use SQL in any database.
    
    ### Processing time with SQL
    
    Data is measured by the number of bits it takes to represent it. All information in a computer can be represented as a binary number consisting solely of 0’s and 1’s. Each 0 or 1 in a number is a bit. A bit is the smallest unit of storage in computers. Since computers work in binary (Base 2), this means that all the important numbers that differentiate between different data sizes will be powers of 2. A byte is a collection of 8 bits.
    
    BigQuery caches the query results to avoid extra work if the query needs to be rerun.
    
    ### Widely used SQL queries
    
    Queries can help you do a lot of things, but there are some common ones that data analysts use all the time:
    
    - SELECT: We can use SELECT to specify exactly what data we want to interact with in a table. If we combine SELECT with FROM, we can pull data from any table in this database as long as they know what the columns and rows are named.
        
        ```sql
        SELECT *column(s)_name*
        FROM *personal_project_name*.*database_name.table_name*
        ```
        
    - INSERT INTO: used to insert information to the table
        
        ```sql
        INSERT INTO *database_name.table_name*
        (*column1,...,columnN*)
        VALUES
        (*value1,...,valueN*)
        ```
        
    - UPDATE: update information to a row
        
        ```sql
        UPDATE *database_name.table_name*
        SET *columnA* = '*value*'
        WHERE *columnB = 'condition'*
        ```
        
    - CREATE TABLE IF NOT EXISTS: Create a new table for the database
        - Keep in mind, just running a SQL query doesn't actually create a table for the data we extract. It just stores it in our local memory. To save it, we'll need to download it as a spreadsheet or save the result into a new table.
    - if you're creating lots of tables within a database, you'll want to use the DROP TABLE IF EXISTS statement to clean up after yourself.
    - DISTINCT: we can remove duplicates including DISTINCT in our SELECT statement.
    
    ### Cleaning string variables using SQL
    
    - LENGTH: we can use LENGTH to double-check that our string variables are consistent. For some databases, this query is written as LEN, but it does the same thing.
        
        ```sql
        SELECT 
        	LENGTH (*column_name*) AS *new_column_name*
        FROM *personal_project_name*.*database_name.table_name*
        ```
        
    - SUBSTRING(): function extracts some characters from a string.
        
        ```sql
        SUBSTRING(*string_or_column, start, length*)
        ```
        
    - TRIM: This is really useful if you find entries with extra spaces and need to eliminate those extra spaces for consistency. To use the TRIM function, we tell SQL the column we want to remove spaces from
        
        ```sql
        TRIM(*column_we_want_to_remove_spaces_from*) = '*text*'
        ```
        
    
    ### Advanced data cleaning functions
    
    - CAST: When you import data that doesn't already exist in your SQL tables, the datatypes from the new dataset might not have been imported correctly. This is where the CAST function comes in handy. CAST can be used to convert anything from one data type to another.
        
        ```sql
        SELECT
        	CAST(*column_name* AS *datatype*)
        FROM *personal_project_name*.*database_name.table_name*
        ORDER BY
        	**CAST(*column_name* AS *datatype*)
        ```
        
    - CONCAT(): lets you add strings together to create new text strings that can be used as unique keys.
    - COALESCE(): Can be used to return non-null values in a list
        
        ```sql
        SELECT
        	COALESCE(*column_name,column_replacement_if_value_is_null*) AS *new_column_name*
        FROM *personal_project_name*.*database_name.table_name
        ...*
        ```
        
        COALESCE can save you time when you're making calculations too by skipping any null values and keeping your math correct.
        
    
    ## MANUALLY CLEANING DATA
    
    ### Verifying and reporting results
    
    Verification is a process to confirm that a data cleaning effort was well- executed and the resulting data is accurate and reliable. It involves rechecking your clean dataset, doing some manual clean ups if needed, and taking a moment to sit back and really think about the original purpose of the project. That way, you can be confident that the data you collected is credible and appropriate for your purposes.
    
    Making sure your data is properly verified is important because:
    
    - it allows you to double-check that the work you did to clean up your data was thorough and accurate.
    - lets you catch mistakes before you begin analysis.
    - Without it, any insights you gain from analysis can't be trusted for decision-making.
    
    The other big part of the verification process is reporting on your efforts. Reports are a super effective way to show your team that you're being 100 percent transparent about your data cleaning. Reporting is also a great opportunity to show stakeholders that you're accountable, build trust with your team, and make sure you're all on the same page of important project details.
    
    A changelog is a file containing a chronologically ordered list of modifications made to a project. It's usually organized by version and includes the date followed by a list of added, improved, and removed features. Changelogs are very useful for keeping track of how a dataset evolved over the course of a project.
    
    ### Steps in the verification process
    
    1. Going back to your original unclean data set and comparing it to what you have now. Review the dirty data and try to identify any common problems.
    2. clean up the problems manually. For example, by eliminating extra spaces or removing an unwanted quotation mark. But there's also some great tools for fixing common errors automatically, such as TRIM and remove duplicates. Now sometimes you had an error that shows up repeatedly, and it can't be resolved with a quick manual edit or a tool that fixes the problem automatically. In these cases, it's helpful to create a pivot table. Pivot tables sort, reorganize, group, count, total or average data stored in a database.
    3. Taking a big-picture view of your project. This is an opportunity to confirm you're actually focusing on the business problem that you need to solve and the overall project goals and to make sure that your data is actually capable of solving that problem and achieving those goals. Taking a big picture view of your project involves doing three things:
        1. consider the business problem you're trying to solve with the data.
        2. consider the goal of the project.
        3. consider whether your data is capable of solving the problem and meeting the project objectives. That means thinking about where the data came from and testing your data collection and cleaning processes.
        
        ask yourself, do the numbers make sense?
        
    
    ### Data-cleaning verification: A checklist
    
    - Correct the most common problems:
        - **Sources of errors**: Did you use the right tools and functions to find the source of the errors in your dataset?
        - **Null data**: Did you search for NULLs using conditional formatting and filters?
        - **Misspelled words**: Did you locate all misspellings?
        - **Mistyped numbers**: Did you double-check that your numeric data has been entered correctly?
        - **Extra spaces and characters**: Did you remove any extra spaces or characters using the **TRIM** function?
        - **Duplicates**: Did you remove duplicates in spreadsheets using the **Remove Duplicates** function or **DISTINCT** in SQL?
        - **Mismatched data types**: Did you check that numeric, date, and string data are typecast correctly?
        - **Messy (inconsistent) strings**: Did you make sure that all of your strings are consistent and meaningful?
        - **Messy (inconsistent) date formats**: Did you format the dates consistently throughout your dataset?
        - **Misleading variable labels (columns)**: Did you name your columns meaningfully?
        - **Truncated data:** Did you check for truncated or missing data that needs correction?
        - **Business Logic**: Did you check that the data makes sense given your knowledge of the business?
    - Review the goal of your project: Once you have finished these data cleaning tasks, it is a good idea to review the goal of your project and confirm that your data is still aligned with that goal. This is a continuous process that you will do throughout your project-- but here are three steps you can keep in mind while thinking about this:
        - Confirm the business problem
        - Confirm the goal of the project
        - Verify that data can solve the problem and is aligned to the goal
        
        ![Untitled](Google_Data_Analytics_images/Untitled%2038.png)
        
    
    ## DOCUMENTING RESULTS AND THE CLEANING PROCESS
    
    When you clean your data, all the incorrect or outdated information is gone, leaving you with the highest-quality content. But all those changes you made to the data are valuable too. Documentation is the process of tracking changes, additions, deletions, and errors involved in your data cleaning effort.
    
    Having a record of how a data set evolved does three very important things:
    
    1. it lets us recover data-cleaning errors. It's also a good idea to create a clean table rather than overriding your existing table. This way, you still have the original data in case you need to redo the cleaning.
    2. documentation gives you a way to inform other users of changes you've made.
    3. documentation helps you to determine the quality of the data to be used in analysis.
    
    The first two benefits assume the errors aren't fixable. But if they are, a record gives the data engineer more information to refer to.
    
    Data analysts usually use a changelog to access this information. You can use and view a changelog in spreadsheets and SQL to achieve similar results:
    
    - Spreadsheet: We can use Sheet's version history, which provides a real-time tracker of all the changes and who made them from individual cells to the entire worksheet. To find this feature:
        1. click the File tab >  Version history > see version history
        2. In the right panel, choose an earlier version.
        3. We can find who edited the file and the changes they made in the column next to their name.
        4. To return to the current version, go to the top left and click "Back." If you want to check out changes in a specific cell, we can right-click and select Show Edit History.
        5. if you want others to be able to browse a sheet's version history, you'll need to assign permission.
    - SQL: The way you create and view a changelog with SQL depends on the software program you're using. Essentially, all you have to do is specify exactly what you did and why when you commit a query to the repository as a new and improved query.
        
        Another option is to just add comments as you go while you're cleaning data in SQL. This will help you construct your changelog after the fact.
        
        There is an option called “Query history” which tracks all the queries you’ve run. You can click on any of them to revert back to a previous version of your query or to bring up an older version to find what you've changed.
        
    
    But there's another way to keep the communication flowing, and that's reporting.
    
    ### Embrace changelogs
    
    Data analysts use changelogs to keep track of data transformation and cleaning. It is common to keep changelogs as a readme file in a code repository. Here are some examples of these:
    
    ![Untitled](Google_Data_Analytics_images/Untitled%2039.png)
    
    ![Untitled](Google_Data_Analytics_images/Untitled%2040.png)
    
    Most software applications have a kind of history tracking built in. For example, in Google sheets, you can check the version history of an entire sheet or an individual cell and go back to an earlier version. In Microsoft Excel, you can use a feature called Track Changes. And in BigQuery, you can view the history to check what has changed.
    
    | Google Sheets | 1. Right-click the cell and select **Show edit history**.
    2. Click the left-arrow < or right arrow > to move backward and forward in the history as needed. |
    | --- | --- |
    | Microsoft Excel | 1. If Track Changes has been enabled for the spreadsheet: click **Review**. ****
    2. Under **Track Changes**, click the **Accept/Reject Changes** option to accept or reject any change made. |
    | BigQuery | Bring up a previous version (without reverting to it) and figure out what changed by comparing it to the current version. |
    
    Beneffits of changelogs:
    
    - are super useful for helping us understand the reasons changes have been made
    - have no set format and you can even make your entries in a blank document. But if you are using a shared changelog, it is best to agree with other data analysts on the format of all your log entries.
    - By following up, you would ensure data integrity outside your project. You would also be showing personal integrity as someone who can be trusted with data.
    - a changelog is important for when lots of changes to a spreadsheet or query have been made.
    
    Typically, a changelog records this type of information:
    
    - Data, file, formula, query, or any other component that changed
    - Description of what changed
    - Date of the change
    - The person who made the change
    - The person who approved the change
    - Version number
    - Reason for the change
    
    ### Best practices for changelogs
    
    - Changelogs are for humans, not machines, so write legibly.
    - Every version should have its own entry.
    - Each change should have its own line.
    - Group the same types of changes. Types of changes usually fall into one of the following categories:
        - Added: new features introduced
        - Changed: changes in existing functionality
        - Deprecated: features about to be removed
        - Removed: features that have been removed
        - Fixed: bug fixes
        - Security: lowering vulnerabilities
    - Versions should be ordered chronologically starting with the latest.
    - The release date of each version should be noted.
    
    Consider what changes you need to record in a changelog. To start, you record the various changes, additions, and fixes that were discussed above. Arrange them using bullets or numbering with one change per line. Group similar changes together with a label describing the change immediately above them.
    
    Use different version numbers for each milestone reached in your project. Within each version, place the logged changes that were made since the previous version (milestone). Dates are not generally necessary for each change, but they are recommended for each version.
    
    ### Why documentation is important
    
    Since changelog is staged chronologically, it provides a real-time account of every modification.
    
    There're plenty of ways we could go about documenting what we did. One common way is to just create a doc listing out the steps we took and the impact they had.
    
    This helps build our credibility as witnesses who can be trusted to present all the evidence accurately during testimony.
    
    ### Feedback and cleaning
    
    By now it's safe to say that verifying, documenting and reporting are valuable steps in the data-cleaning process. The next step is getting feedback about the evidence and using it for good.
    
    The feedback we get when we report on our cleaning can transform data collection processes, and ultimately business development.
    
    One of the biggest challenges of working with data is dealing with errors. Some of the most common errors involve:
    
    - human mistakes like mistyping or misspelling
    - flawed processes like poor design of a survey form
    - system issues where older systems integrate data incorrectly.
    
    With consistent documentation and reporting, we can uncover error patterns in data collection and entry procedures and use the feedback we get to make sure common errors aren't repeated.
    
    In more extreme cases, the feedback we get can even send us back to the drawing board to rethink expectations and possibly update quality control procedures.
    
    ### Advanced functions for speedy data cleaning
    
    ![Untitled](Google_Data_Analytics_images/Untitled%2041.png)
    
    ## THE DATA ANALYST HIRING PROCESS
    
    The most common way to start is by checking out available jobs. There's a lot of job sites that are built specifically for people seeking employment. You can also go to company websites where they usually post job listings too.
    
    Once you find a few that you like, do some research to learn more about the companies and the details about the specific positions you'll be applying for.
    
    Then you can update your resume or create a new one. You'll want it to be specific and reflect what each company is looking for. But you can definitely have a master resume that you tweak for each position. It can also help to create a spreadsheet with all of your experiences and accomplishments to help you decide what to include in your resume for each possition.
    
    One of the most challenging part of a job search ir hearing the word "no.” The key is to stay focused. Don't get discouraged, and above all else believe in yourself.
    
    A recruiter might also reach out to you based on their own research. So when you talk with the recruiter, whether on the phone, online or in person, be professional and personable. it can help to refer back to your resume to wow them with your knowledge of the data analytics industry. And remember, recruiters are also looking for someone and they're hoping it'll be you.
    
    - Using technical terms like "SQL" and "clean data" will show recruiters that you know what you're doing.
    - they want to see that you know what you're talking about. They might also give you prep materials or other recommendations.
    
    Next up is usually the hiring manager. This is the most important step. The hiring manager's job is to evaluate whether you have the ability to do the work and whether you'd be a good fit for their team. Your job is to convince them that yes, you do, and yes, you would be. A good thing you can do here is use LinkedIn or other professional sites to research the hiring managers or even other analysts who have a similar role to the one you're applying for. The more information you have about the job, the better your chances of actually getting it. You should also use this opportunity to ask lots of questions to help you figure out if the company's a good fit for you. You can do this when you talk to recruiters too.
    
    Now if the hiring manager sees you as a fit, it's very possible you'll have at least one more interview. The point of these interviews is to give your future stakeholders and teammates a chance to decide if you're the best candidate for the position.
    
    If all goes well, you'll get an official offer. Usually by phone first and maybe followed by an official letter. Make sure it's a competitive offer before you sign. Remember, if they reach out to you with an offer, that means they want you as much as you want them.
    
    You should also research salaries, benefits, vacation time, and any other factors that are important to you for similar jobs. Keep in mind, you'll need to find a balance between what you want, what they want to give you, and what's fair. So know your own worth but also understand that the company hiring you has already placed a certain value on your role.
    
    If you're already employed somewhere else during your job search, it's customary and polite to give at least a two-week notice at your old job before starting at the new one. Plus, it's good to give yourself a break before starting your exciting new adventure. You've earned it.
    
    ### Creating a resume
    
    A strong resume is essential to moving forward as a data analytics professional. You want your resume to be a snapshot of all that you've done both in school and professionally.
    
    - The key is to be breif. Try to keep everything in one page and each description to just a few bullet points. Two to four bullet points is enough but remember to keep your bullet points concise.
    - Sticking to one page will help you stay focused on the details that best reflect who you are or who you want to be professionally. One page might also be all that hiring managers and recruiters have time to look at. They're busy people, so you want to get their attention with your resume as quickly as possible.
    - A template is a great way to build a brand new resume or reformat one you already have. Programs like Microsoft Word or Google Docs and even some job search websites all have templates you can use. A template has placeholders for the information you'll need to enter and its own design elements to make your resume look inviting.
    - There's more than one way to build a resume, but most have contact information at the top of the document. This includes your:
        - name
        - address
        - phone number
        - email address. If you have multiple email addresses or phone numbers, use the ones that are most reliable and sound professional. It's also great if you can use your first and last name in your email address, like [janedoe17@email.com](mailto:janedoe17@email.com).
    - Make sure that your contact information matches the details that you've included on professional websites.
    - While most resumes have contact information in the same place, it's up to you how you organize that info.
        - A format that focuses more on skills and qualifications and less on work history is great for people who have gaps in their work history. It's also good for those who are just starting out their career or making a career change.
        - If you do want to highlight your work history, feel free to include details of your work experience starting with your most recent job. If you've had lots of jobs that are related to a new position you're applying for, this format makes sense.
    - Once you've decided on your format, you can start adding your details.
        - Some resumes begin with the summary, but this is optional. A summary can be helpful if you have an experience that is not traditional for a data analyst or if you're making a career transition. If you decide to include a summary, keep it to one or two sentences that highlight your strengths and how you can help the company you're applying to. You'll also want to make sure your summary includes positive words about yourself, like dedicated and proactive. You can support those words with data, like the number of years you've worked or the tools you're experienced in like SQL and spreadsheets.
        - A summary might start off with something like hardworking customer service representative with over five years of experience.
        - Once you've completed this program and have your certificate, you'll be able to include that too, which could sound like this, "entry-level data analytics professional recently completed the Google Data Analytics Professional Certificate.”
        - Another option is leaving a placeholder for your summary while you build the rest of your resume and then writing it after you finish the other sections. This way, you can review the skills and experience you've mentioned and grab two or three of the highlights to use in your summary.
        - The summary might change a little as you apply for different jobs.
    - If you're including a work experience section, there's lots of different types of experience you could add. Outside of jobs with other companies, you could also include volunteer positions you've had and any freelance or side work you've done. The key here is the way in which you describe these experiences.
        - Try to describe the work you did in a way that relates to the position you're applying for. It's important to clearly state the minimum qualifications or requirements shown in the job description in your resume.
    - check out preferred qualifications, which lots of job descriptions also include. These aren't required, but every additional qualification you match makes you a more competitive candidate for the role. Including any part of your skills and experience that matches a job description will help your resume rise above the competition.
    - It's helpful to describe your skills and qualifications in the same way. For example, if a listing talks about organization and partnering with others, try to think about relevant experiences you've had. In your descriptions, you want to highlight the impact you've had in your role, as well as the impact the role had on you.
    - After you've added work experience and skills, you should include a section for any education you've completed. You can add this course as part of your education, and you can also refer to it in your summary and skill sections.
        
        ![Untitled](Google_Data_Analytics_images/Untitled%2042.png)
        
    - Depending on the format of your resume, you might want to add a section for technical skills you've acquired both in this course and elsewhere. Besides technical skills like SQL, you could also include language proficiencies in this section. Having some ability in a language other than English can only help your job search.
    
    ### Elements of a data analyst resume
    
    For data analytics, one of the most important things your resume should do is show that you are a clear communicator. Companies looking for analysts want to know that the people they hire can do the analysis, but also can explain it to any audience in a clear and direct way.
    
    - Summary section:
        - it's a good spot to point out if you're transitioning into a new career role.
        - P-A-R, or PAR statements. PAR stands for Problem, Action, Result. This is a great way to help you write clearly and concisely. Adding PAR statements to your job descriptions or skill section can help with the organization and consistency in your resume.
    - Skill section:
        - make sure you include any skills and qualifications you've acquired through this course and on your own. You don't need to be super technical. But talking about your experience with spreadsheets, SQL, Tableau, and R will enhance your resume and your chances of getting a job.
        - You might want to prioritize technical skills over soft skills, such as:
            - Strong analytical skills
            - Pattern recognition
            - Relational databases and SQL
            - Strong data visualization skills
            - Proficiency with spreadsheets, SQL, R and Tableau
        - Some common professional skills for entry-level data analysts are:
            - SQL: SQL is considered a basic skill that is pivotal to any entry-level data analyst position.
            - Spreadsheets: Although SQL is popular, 62% of companies still prefer to use spreadsheets for their data insights.
            - Data visualization tools: Data visualization tools help to simplify complex data and enable the data to be visually understood. Tableau is best known for its ease of use, so it is a must-have for beginner data analysts. Also, studies show that data analysis jobs requiring Tableau are expected to grow about 34.9% over the next decade.
            - R or Python programming.
    
    ### Highlighting experiences on resumes
    
    What matters for your resume is how you present the work you've done. 
    
    Transferable skills are skills and qualities that can transfer from one job or industry to another. All of there are known as soft skills. Some of those are:
    
    - Communication: When job descriptions say they want strong communication skills for a data analyst, it usually means they want someone who can speak about what they do to people who aren't as technical or analytical. In your work history section, you can highlight how your effective communication skills have helped you. You can also refer to specific presentations you've made and the outcomes of those presentations, and you can even include the audience for your presentations, especially if you present it to large groups or people in senior positions.
        - "effectively implemented and communicated daily workflow to fellow team members, resulting in a 15% increase in productivity.”
    - Problem-solving: When problems arise in a database or lines of code, data analysts need to be able to find and troubleshoot the problem.
        
        ![Untitled](Google_Data_Analytics_images/Untitled%2043.png)
        
    - Teamwork: While you might have plenty of work to do on your own, it'll always be for the benefit of the team. Team means not only the data team you're part of, but the whole company as well.
    - detail-oriented
    - perseverance
    - Presentation skills
    - Collaboration: your ability to share ideas, insights, and criticisms will be crucial.
    - Research: To analyze the data and draw conclusions, you will need to conduct research to stay in-line with industry trends.
    - Adaptability
    - Attention to detail
    
    Using PAR statements and focusing on your transferable soft skills can really add to the power of your resume.
    
    ### Exploring areas of interest
    
    Finding a job you love is even better. Always keep in mind that data analytics is constantly evolving within lots of different industries. Job titles and hiring needs might also change. But the opportunities, no matter what they are when you're searching, will be there.
    
    The certificate you earn here will be most applicable to junior or associate data analyst positions. But that doesn't mean you have to limit your job search to only postings for junior or associate analysts.
    
    - Health care analysts gather and interpret data from sources like electronic health records and patient surveys. Their work helps organizations improve the quality of their care.
    - Data analysts in marketing complete quantitative and qualitative market analysis. They identify important statistics and interpret and present their findings to help stakeholders understand the data behind their marketing strategies.
    - Business intelligence analysts help companies use data they've collected to increase their efficiency and maximize their profits. These analysts usually work with large amounts of data to identify trends and generate business insights
    - Financial analysts use the data to identify and potentially recommend business and investment opportunities.
    
- 5 - **ANALYZE** DATA TO ANSWER QUESTIONS
    
    Organizing your data is one of the most important steps for analysis! Once you get organized, you can perform calculations to find clear and objective answers to any data question.
    
    ## DATA ANALYSIS BASICS
    
    The analysis is the process used to make sense of the data collected. It means taking the right steps to proceed and think about your data in different ways. The goal of analysis is to identify **trends** and **relationships** within the data so that you can accurately answer the question you're asking. To do this, you should stick to the 4 phases of analysis:
    
    1. **Organize data**
    2. **Format and adjust data.** Formatting data streamlines things and saves you time. You can adjust the data in a way that makes it easy to digest by filtering and sorting your data. Sorting involves arranging data into a meaningful order to make it easier to understand, analyze, and visualize.
    3. **Get input from others.** Input from other people can also be really helpful when analyzing information and making decisions. Gaining input from others is important because it gives you a viewpoint you might not understand or have access to. It's also important to seek out others' perspectives early. That way, if they predict any obstacles or challenges, you'll know beforehand.
    4. **Transform data** by observing relationships between data points and making calculations based on the data you have.
    
    ## ORGANIZE DATA
    
    Organization isn't just about making things look orderly. It's also about making it easier to search and locate the data you need in a quick and easy way.
    
    Most of the data you'll use in your analysis will be organized in tables. Tables help you organize similar kinds of data into categories and subject areas that you can focus on as you analyze.
    
    Tables allow you to make decisions about data types. They help you to figure out what variables you need and the data type those variables should have. Tables are helpful because they let you manipulate your data and categorize it. Having distinct categories and classifications lets you focus on, and differentiate between, your data quickly and easily.
    
    Once you have the data organized and formatted, you'll be ready to sort and filter it to find the data you need. Filters and sorts are affected by the type of data we're working with.
    
    The bottom line is that it's important to have your data in the right format. So always be prepared to adjust, no matter how far into your analysis you are.
    
    ### Sorting and filters
    
    Sorting and filtering are two ways you can keep things organized when you format and adjust data to work with it.
    
    - A filter can help you find errors or outliers so you can fix or flag them before your analysis.
    - After you fix errors or identify outliers, you can remove the filter and return the data to its original organization.
    - It is important to point out that, after you filter data, you can **sort the filtered data**, too.
    - Items in the row and column areas of a pivot table are sorted in ascending order by any custom list first.
    - If the items aren’t in a custom list, they will be sorted in ascending order by default. But, if you sort in descending order, you are setting up a rule that controls how the field is sorted even after new data fields are added.
    
    | SORTING | FILTERING |
    | --- | --- |
    | Is when you arrange data into a meaningful order to make it easier to understand, analyze, and visualize. | Is used to show only the data that meets a specific criteria, and hiding the rest. Filtering is really useful when you have lots of data. When it comes to sifting through large, disorganized piles of data |
    | It ranks your data based on a specific metric you choose. Like ascending or descending order, alphabetic | You can save time by zeroing in on the data that is really important or the data that has bugs or errors. |
    | It helps you to group similar data together by a classification. | It gives you the ability to find what you are looking for without too much effort. It reduces the amount of data that is displayed. |
    | You can sort data in spreadsheets, SQL databases (when your dataset is too large for spreadsheets), and tables in documents. | Most spreadsheets and SQL databases allow you to filter your data in a variety of ways. |
    | You can use sort to quickly order the data | You can use filter to display only the data that meets the criteria that you have chosen. |
    - You can also **filter** data in SQL using the WHERE clause. The WHERE clause works similarly to filtering in a spreadsheet because it returns rows based on a condition you name. Keep in mind that capitalization matters here, so we have to make sure that the letter casing matches the column name exactly. The AND clause allows you to write a query with more than one condition.
    - you can filter data and then sort the filtered results. Using the FILTER and SORT functions together in a range of cells can programmatically and automatically achieve these results for you.
    - while sorting puts data in a specific order, filters narrow down data, so you only see data that fits the filter.
    
    ### Sorting data in spreadsheets
    
    When you sort data based on a specific metric, you can uncover new patterns and relationships within datasets you might not have otherwise noticed.
    
    - In spreadsheets, you can sort data by ascending or descending order, or using numbers or letters.
    - If cells are labeled with color, you can sort them by color, too.
    - When sorting data in a spreadsheet, you can choose to:
        - "Sort sheet": all of the data in a spreadsheet is sorted by the conditions of a single column, but the related information across each row stays together.
        - "Sort range.” (the values in the column): doesn't keep the information across rows together. When you sort a range, you're selecting a specific collection of cells or the range that you want the sorting limited to. Nothing else on the spreadsheet gets rearranged but the specified cells.
    
    There are 2 methods for sorting spreadsheet data: one involves using the menu; the other involves writing out the sort function.
    
    - Using the menu is: Select the column > Data > Short sheet By.. or Short range By…
    - Using de SORT function:  =SORT(1,2,3)
        
        Where:
        
        1. range in which data is collected from
        2. the range from what we're sorting by. this part of the function doesn't recognize column letters. So we use the corresponding number instead (2 for column B, 4 for column D, etc)
        3. You'll need to decide whether you want the data in this column to be in ascending or descending order. A TRUE statement is in ascending order, and FALSE is descending. 
    
    After you've tackled writing SORT functions, you'll want to customize sort orders, too. A customized sort order is when you sort data in a spreadsheet using multiple conditions. This means that sorting will be based on the order of the conditions you select:
    
    1. Click Data, then Sort range, then Advanced range sorting options.
    2. Check the option: Data has a header row
    3. Click on “Sort by” > choose the column you want to use a condition.
    4. If you want to add another sorting condition, click on “Add another sort column”
    
    ### Sorting data in SQL
    
    When a spreadsheet has too much data, you can get error messages, or it can cause your program to crash. SQL shortens processes that would otherwise take a very long time or be impossible to complete in a spreadsheet. It's much quicker than a spreadsheet.
    
    - You can use the ORDER BY clause to sort results returned in a query.
        
        ```sql
        SELECT ...
        FROM ...
        ORDER BY *Column_name* DESC #ASC for ascending order, DESC for descending order
        ```
        
    - You can combine sorts and filters to display information differently.
        
        ```sql
        SELECT ...
        FROM ...
        WHERE *Column_name* = "*Condition*"
        ORDER BY *Column_name* DESC #ASC for ascending order, DESC for descending order
        
        ```
        
    - We can filter for two conditions at once using the AND filter.
        
        ```sql
        SELECT ...
        FROM ...
        WHERE *Column_name1* = "*Condition1*" AND *Column_nameN* = "*Condition2*"
        ORDER BY *Column_name* DESC #ASC for ascending order, DESC for descending order
        ```
        
    
    ## CONVERT AND FORMAT DATA
    
    Sometimes, you need to convert data when you're working with spreadsheets. That might mean changing numbers into dates, strings, percentages, or even currency.
    
    Until you change the data type, you won't be able to sort them the way you want. It's also possible that your datasets contain inconsistent units of measurement that you'll need to convert.
    
    Incorrectly formatted data can:
    
    - Lead to time-consuming mistakes in your analysis
    - Take time to fix
    - Affect stakeholder’s decision-making.
    
    But taking the time early on to convert and format your data can help you avoid that.
    
    ### How to convert and format your data in Spreadsheet
    
    - On the toolbar at the top of the sheet, you'll find a menu that can help you convert these numbers into specific data types. It gives you a lot of choices just from the drop-down menu, such as number, currency, date, percentage.... And if you click to open the full menu, there's even more options, including one for a custom number format.
    - You can go even further and convert the unit of measurement you're using: use the CONVERT function to change the unit of measurement. We'll input the CONVERT function in a new column: CONVERT(A,”B”,”C”) where:
        - A: Cell we want to convert
        - B: original measurement
        - C: New measurement
        
        When adding data to tables using a formula, go back and paste the data in as values afterwards. That way they're locked in. There's an option for "Paste special." And there's an option to "Paste values only.”
        
        - For details on the correct syntax, check [HERE]([https://support.google.com/docs/answer/6055540?hl=en](https://support.google.com/docs/answer/6055540?hl=en))
    
    ### Data validation
    
    Data validation allows you to control what can and can’t be entered in you worksheet. Usually, data validation is used to add drop-down lists to cells with predetermined options for users to choose from. Since you control what's being entered into the worksheet, it cuts down on how much data cleaning you have to do later on.
    
    With data validation, you can:
    
    - Add dropdown lists with predetermined options:
        1. select the column
        2. click on Data > Data validation > + Add rule
        3. Click +Add rule to callup a  new Criteria window that will allow you to define the validation values
        4. Choose Dropdown menu as criteria
    - Create custom checkboxes:
        1. select the cell
        2. click on Data > Data validation > + Add rule
        3. Click +Add rule to callup a  new Criteria window that will allow you to define the validation values
        4. choose Checkbox as a criteria
        5. you can use custom cell values also
    - Protect structured data and formulas
        1. The data validation menu has the option to Reject inputs, which helps make sure our custom tools will continue to run correctly, even if someone puts the wrong data in by mistake.
    
    Data validation can help your team track progress, protect your tables from breaking when working in big teams, and help you customize tables to your needs.
    
    ### Conditional formatting
    
    Conditional formatting is a spreadsheet tool that changes how cells appear when values meet specific conditions. It can be used to change a cell’s color in order to highlight it.
    
    We click on format > Conditional formatting. This brings up a sidebar where we can select our range rule in formatting style.
    
    1. Range: which rows to apply our formatting to when the condition we set is met. We can click this button in the range options to select all of the rows we're applying the formatting to instead of typing it in.
    2. we can choose the rule that we want to apply to these cells.
    3. Then we'll choose a color to apply to those cells
    
    ### Transforming data in SQL (BigQuery dialect)
    
    - CAST function: CAST is an American National Standards Institute (ANSI) function used in lots of programming languages, including BigQuery. CAST indicates that you will be converting the data you select to a different data type
        
        ```sql
        CAST(*expression* AS *typename*)
        ```
        
        Where *expression* is the data to be converted and *typename* is the data type to be returned.
        
        You can do different types of convertions, like:
        
        - Number to a string
        - String to a number
        - Date to a string
        - Date to a datetime
    - [Conversion Rules in Standard SQL]([https://cloud.google.com/bigquery/docs/reference/standard-sql/conversion_rules](https://cloud.google.com/bigquery/docs/reference/standard-sql/conversion_rules))
    - To avoid errors in the event of a failed query, use the SAFE_CAST function instead. The SAFE_CAST function returns a value of Null instead of an error when a query fails. The syntax for **SAFE_CAST** is the same as for **CAST**. Simply substitute the function directly in your queries.
    - Browse these resources for more information about data conversion using other SQL dialects (instead of BigQuery):
        - [SQL Server reference documentation]([https://learn.microsoft.com/en-us/sql/t-sql/functions/cast-and-convert-transact-sql?view=sql-server-ver15](https://learn.microsoft.com/en-us/sql/t-sql/functions/cast-and-convert-transact-sql?view=sql-server-ver15))
        - [MySQL reference documentation]([https://dev.mysql.com/doc/refman/8.0/en/cast-functions.html](https://dev.mysql.com/doc/refman/8.0/en/cast-functions.html))
        - [Blog about type casting that has links to other SQL short guides]([https://www.rudderstack.com/guides/how-to-sql-type-casting/](https://www.rudderstack.com/guides/how-to-sql-type-casting/))
    
    ## COMBINE MULTIPLE DATASETS
    
    We can use CONCAT to combine strings from multiple tables to create new strings (in SQL).
    
    ![Untitled](Google_Data_Analytics_images/Untitled%2044.png)
    
    ### Strings in spreadsheets
    
    - LEN: Return the length of a string of text by counting the number of characters it contains
    - LEFT: Return a set number of characters from the left side of a text string
    - RIGHT: Return a set number of characters from the right side of a text string.
    - FIND: Locate specific characters in a string
    
    | **Function** | **Usage** | **Example** |
    | --- | --- | --- |
    | CONCAT | A function that adds strings together to create new text strings that can be used as unique keys | CONCAT (‘Google’, ‘.com’); |
    | CONCAT_WS | A function that adds two or more strings together with a separator | CONCAT_WS (‘ . ’, ‘www’, ‘google’, ‘com’)
    
    *The separator (being the period) gets input before and after Google when you run the SQL function |
    | CONCAT with + | Adds two or more strings together using the + operator | ‘Google’ + ‘.com’ |
    - JOIN: Combine rows from two or more tables based on a related column
    - LIMIT: Return a certain number of records
    - CONCAT: Add strings together to create new text strings that can be used as unique keys
    - CONVERT: Change the unit of measurement of a value in data
    - ROUND: Limit records to a certain number of decimal places
    
    ### Best practices for searching online
    
    - Thinking skills: it starts with how you approach a problem mentally. This helps you build your mental model. Data analysts use these thinking skills to approach a problem logically and break it into smaller parts. Building this into your own problem-solving process can help you pinpoint specific questions, which you can use to find resources more easily.
    - Data analytics terms: Knowing how to frame data analytics questions with the same language other analysts are using will help you get more search results, and it'll help you understand what other analysts are saying.
    - Basic knowledge of tools. That way, when an online resource is walking you through a new function and a tool that you've used before, you'll know how those tools work.
    - Being able to modify example code is a must. Understanding the syntax of formulas and functions for different tools will allow you to take what you learned online and make it work for you, and maybe even build on it to create a whole new solution.
    
    ### Seeking help on Stack Overflow
    
    Having a variety of tools in your tool kit is important as a data analyst, but just as important is knowing when to use them. If you find yourself stuck on a problem, it can be a good idea to take a step back and reconsider how you're approaching a task. Do you have too much data for a single spreadsheet? Switch to SQL. Are you spending more time debugging queries than actually analyzing data? Maybe you should consider R.
    
    - If you would like to view only questions that have a certain tag, include the tag name in brackets with your search. For example, if you want to only find questions that have the tag “SQL”, then type[SQL] in the search field, along with your keywords or question.
    - When asking a question on Stack Overflow, keep it specific. Don’t use it to ask questions with opinion-based answers.
    - The form for asking a question has three sections:
        - Tittle: this is where you ask your question
        - Body: summarize your problem and include expected and actual results.
        - Tags: tags include specific keywords, like program names.
    
    ### When to use which tool
    
    Having a variety of tools in your tool kit is important as a data analyst, but just as important is knowing when to use them. If you find yourself stuck on a problem, it can be a good idea to take a step back and reconsider how you're approaching a task. Do you have too much data for a single spreadsheet? Switch to SQL. Are you spending more time debugging queries than actually analyzing data? Maybe you should consider R.
    
    R is another programming language, but it's not a database language like SQL. It's a programming language frequently used for statistical analysis, visualization, and other data analysis.
    
    ## VLOOKUP FOR DATA AGGREGATION
    
    Aggregation means collecting or gathering many separate pieces into a whole. 
    
    Data aggregation is the process of gathering data from multiple sources in order to combine it into a single summarized collection. In data analytics, a summarized collection, or summary, describes identifying the data you need and gathering it all together in one place.
    
    So in data, the puzzle pieces represent the data that lives in different, separate datasets. Getting them organized is the aggregation process. Then the piles of pieces that complete a single puzzle become your summary. And finally, putting those pieces back together is like analyzing them to gain important insights.
    
    Data aggregation helps data analyst:
    
    - Identify trends
    - Make comparisons
    - Gain insights that wouldn't be possible if each of the data elements were analyzed on its own.
    
    Data can also be aggregated over a given time period to provide statistics, such as averages, minimums, maximums, and sums.
    
    Functions are a big help in making data aggregation possible. VLOOKUP is a data aggregation tool.
    
    ### Using VLOOKUP
    
    Before using VLOOKUP, we have to clean the data and fix some common errors, so we will do some common data-cleaning tasks:
    
    - different data types: we can change the format manually, or we can use functions like:
        - VALUE: a function that converts a text string that represents a number to a numerical value
    - Extra spaces: We can use TRIM and delete any extra spaces added to the cell
    - Duplicates: If there are duplicate rows in the search, it will return only the first match it finds. Using Remove duplicates is a great way to get rid of duplicates and help make sure you find the right record during the lookup.
    
    Here is the syntax:
    
    **VLOOKUP( *search_key, range, index, [is_sorted]*)**
    
    Lets explain VLOOKUP with an example:
    
    **VLOOKUP(103, A2:B26, 2, FALSE)**
    
    - **103:** a value to search for.
    - **A2:B26:**  the range that will be searched.
    - **2:** VLOOKUP will not recognize column names such as A, B, or C, so we use a number to indicate the column (The column index of the value to be returned).
        - • If index is not between 1 and the number of columns in range, #VALUE! is returned.
    - **FALSE:** tells VLOOKUP to find an exact match. If this said true, the function will return only a close match, which might not be what we want.
    
    One of the most common things data analysts do with VLOOKUP is populating data in one spreadsheet from another. VLOOKUP can connect two sheets together on a matching column to populate one single sheet.
    
    Two common reasons to use VLOOKUP are:
    
    - Populating data in a spreadsheet
    - Merging data from one spreadsheet with data in another
    
    ### Identifying common VLOOKUP errors
    
    Troubleshooting has to do with asking the right questions, some of them are:
    
    - How should I prioritize these issues?
    - In a single sentence, what’s the issue I’m facing?. This helps to clarify what's really going on, so I don't get bogged down with extra details.
    - What resources can help me solve the problem?
    - How can I stop this problem from happening in the future?
    
    VLOOKUP have some limitations:
    
    - VLOOKUP only returns the first match it finds, even if there are lots of possible matches. VLOOKUP only looks at data to the right after a match is found. In other words, the index for VLOOKUP indicates columns to the right only. This may require you to move columns around before you use VLOOKUP.
    - Let's say the first few rows of a VLOOKUP have returned the correct result. But when you drive the function down the column, problems start popping up. This is probably because the table array part of the function hasn't been locked or made absolute. An absolute reference is a reference that is locked so that rows and columns won't change when copied. You can fix this issue by wrapping the table array in dollar signs.
    - Version control issues. In other words, a function worked perfectly at first, but then something in the spreadsheet it was referencing changed. There are a few actions data analysts can take to ensure this doesn't happen:
        - lock the spreadsheet. This stops other people from making changes. Go to select data > Protected sheets and ranges. Next, choose what you want to protect
        - Use MATCH: A function used to locate the position of a specific lookup value and can help you with version control.
    - Exact and approximate matching. TRUE tells VLOOKUP to look for approximate matches, and FALSE tells VLOOKUP to look for exact matches. It's important to know that VLOOKUP starts at the top of a specified range and searches downward vertically in each cell to find the right value. It stops searching when it finds any value that's greater than or equal to the lookup value. That's why data analysts typically use FALSE, like this. That way VLOOKUP only returns the exact match to what you've entered in the lookup value.
    - #N/A: #N/A indicates that a matching value can't be returned as a result of the VLOOKUP. The error doesn’t mean that anything is actually wrong with the data, You can use the IFNA function to replace the #N/A error with something more descriptive, like “Does not exist”.
    - • After you have populated data with the VLOOKUP formula, you may copy and paste the data as values only to remove the formulas so you can manipulate the data again.
    
    ## UNDERSTANDING JOINS
    
    JOIN is a SQL clause that's used to combine rows from two or more tables based on a related column. Basically, you can think of a JOIN as a SQL version of VLOOKUP.
    
    There are four common JOINs data analysts use:
    
    - INNER JOIN: A function that returns records with matching values in both tables. For the records to appear in the results table, they'll have to be key values in both tables.  INNER is *optional* in this SQL query because it is the default as well as the most commonly used JOIN operation.
    - LEFT JOIN: A function that will return all the records from the left table and only the matching records from the right table. Each row in the left table appears in the results even if there are no matches in the right table. You may see this as LEFT OUTER JOIN, but most users prefer LEFT JOIN.
    - RIGHT JOIN: A function that will return all records from the right table and only the matching records from the left. You may see this as RIGHT OUTER JOIN or RIGHT JOIN. Practically speaking, RIGHT JOIN is rarely used. Most people simply switch the tables and stick with LEFT JOIN.
    - OUTER JOIN: A function that combines RIGHT and LEFT JOIN to return all matching records in both tables. You may sometimes see this as FULL JOIN.
    
    ![Untitled](Google_Data_Analytics_images/Untitled%2045.png)
    
    JOINs help you combine matching or related columns from different tables. We refer to these values as primary and foreign keys. JOIN use these keys to identify relationships and corresponding values. A JOIN combines tables by using a primary or foreign key to align the information coming from both tables in the combination process.
    
    When we input JOIN into SQL, it usually defaults to inner JOIN. 
    
    In English and SQL we read from left to right. The table mentioned first is left and the table mentioned second is right. You can also think of left as a table name to the left of the JOIN statement and right as a table name to the right of the JOIN statement.
    
    ### The general JOIN syntax
    
    This is an example of a SQL clause that brings exactly the same:
    
    ![Untitled](Google_Data_Analytics_images/Untitled%2046.png)
    
    ```sql
    SELECT
    	--table columns from tables are inserted here
    	*table_name1.column_name*,
    	*table_name2.column_name*
    FROM
    	*dataset_name.table_name1*
    JOIN
    	*dataset_name.table_name2*
    ON *table_name1.column_name* = *table_name2.column_name*
    ```
    
    **ON** in SQL identifies how the tables are to be matched for the correct information to be combined from both.
    
    Example of INNER JOIN:
    
    ![Untitled](Google_Data_Analytics_images/Untitled%2047.png)
    
    BUT! if we are using a public data set, the query will look something like this:
    
    ![Untitled](Google_Data_Analytics_images/Untitled%2048.png)
    
    And you can make it easier to read using alias for each table:
    
    ![Untitled](Google_Data_Analytics_images/Untitled%2049.png)
    
    Another examples:
    
    ![Untitled](Google_Data_Analytics_images/Untitled%2050.png)
    
    ### Secret identities: The importance of aliases
    
    Aliasing is the process of using aliases. Aliases are used in SQL queries to create temporary names for a column or table. Aliases make referencing tables and columns in your SQL queries much simpler when you have table or column names that are too long or complex to make use of in queries.
    
    In SQL queries, aliases are implemented by making use of the AS command. The basic syntax for the AS command can be seen in the following query for aliasing a table:
    
    ```sql
    SELECT *column_name(s)*
    FROM *table_name* AS *alias_name*
    ```
    
    Notice that AS is preceded by the table name and followed by the new nickname. It is a similar approach to aliasing a column:
    
    ```sql
    SELECT *column_name* AS *alias_name*
    FROM *table_name*
    ```
    
    If using AS results in an error when running a query because the SQL database you are working with doesn't support it, you can leave it out. In the previous examples, the alternate syntax for aliasing a table or column would be:
    
    - FROM table_name alias_name
    - SELECT column_name alias_name
    
    The key takeaway is that queries can run with or without using AS for aliasing, but using AS has the benefit of making queries more readable. It helps to make aliases stand out more clearly.
    
    ## COUNT AND COUNT DISTINCT
    
    COUNT can be used to count the total number of numerical values within a specific range in spreadsheets. COUNT in SQL does the same thing. COUNT is a query that returns the number of rows in a specified range
    
    COUNT DISTINCT is a query that only returns the distinct values in that range. Basically, this means COUNT DISTINCT doesn't count repeating values. 
    
    COUNT returns the number of rows in a specified range. COUNT DISTINCT only returns the distinct values in a specified range.
    
    You'll use COUNT and COUNT DISTINCT anytime you want to answer questions about how many
    
    ![Untitled](Google_Data_Analytics_images/Untitled%2051.png)
    
    ## WORK WITH SUBQUERIES
    
    A subquery is a SQL query that is nested inside of a larger query.
    
    With subqueries you can combine different pieces of logic together. Because the logic of your outer query relies on the inner query, you can get more done with a single query. This means all of the logic is in one place, which makes it more efficient and easier to read. The statement containing the subquery can also be called the outer query or the outer select. This makes the subquery the inner query or inner select. The inner query executes first so that the results can be passed on to the outer query to use.
    
    Usually you'll find subqueries nested in FROM or WHERE clauses.
    
    ![Untitled](Google_Data_Analytics_images/Untitled%2052.png)
    
    We can also add a subquery inside the FROM clause:
    
    ![Untitled](Google_Data_Analytics_images/Untitled%2053.png)
    
    Or inside of a WHERE clause.
    
    Usually, you will find subqueries nested in the SELECT, FROM, and/or WHERE clauses. There is no general syntax for subqueries, but the syntax for a basic subquery is as follows:
    
    ![Untitled](Google_Data_Analytics_images/Untitled%2054.png)
    
    You will find that, within the first SELECT clause is another SELECT clause. The second SELECT clause marks the start of the subquery in this statement.
    
    ### Subquery rules
    
    - Subqueries must be enclosed within parentheses
    - A subquery can have only one column specified in the SELECT clause. But if you want a subquery to compare multiple columns, those columns must be selected in the main query.
    - Subqueries that return more than one row can only be used with multiple value operators, such as the IN operator which allows you to specify multiple values in a WHERE clause.
    - A subquery can’t be nested in a SET command. The SET command is used with UPDATE to specify which columns (and values) are to be updated in a table.
    - Comparison operators such as >, <, or = help you compare data in subqueries. You can also use multiple row operators including IN, ANY, or ALL.
    
    To learn more about Subqueries, check the next links:
    
    - [**SQL subqueries:**](https://www.w3resource.com/sql/subqueries/understanding-sql-subqueries.php) This detailed introduction includes the definition of a subquery, its purpose in SQL, when and how to use it, and what the results will be
    - [**Writing subqueries in SQL](https://mode.com/sql-tutorial/sql-sub-queries/):** Explore the basics of subqueries in this interactive tutorial, including examples and practice problems that you can work through
    
    ### Using subqueries to aggregate data
    
    The WHERE function can't be used with aggregate functions. For example, you can use WHERE on a statement and follow it with GROUP BY. But when you want to use GROUP BY first and then use WHERE on that output, you'll need a different function. This is where HAVING comes in.
    
    HAVING allows you to add a filter to your query instead of the underlying table when you're working with aggregate functions. That way it only returns records that meet your specific conditions.  The HAVING clause was added to SQL because the WHERE keyword could not be used with aggregate functions.
    
    ```sql
    ##SQL HAVING Syntax:
    SELECT *column_name*, aggregate_function(*column_name*)
    FROM *table_name*
    WHERE *column_name operator* value
    GROUP BY *column_name*
    HAVING aggregate_function(*column_name*) operator value;
    ```
    
    Similarly, CASE returns records with your conditions by allowing you to include if/then statements in your query.
    
    The `CASE` expression goes through conditions and returns a value when the first condition is met (like an if-then-else statement). So, once a condition is true, it will stop reading and return the result. If no conditions are true, it returns the value in the `ELSE` clause. If there is no `ELSE` part and no conditions are true, it returns NULL.
    
    ```sql
    ##SQL CASE Syntax:
    CASE
        WHEN *condition1* THEN *result1*
        WHEN *condition2* THEN *result2*
        WHEN *conditionN* THEN *resultN*
        ELSE *result*
    END;
    ```
    
    ![Untitled](Google_Data_Analytics_images/Untitled%2055.png)
    
    ## DATA CALCULATIONS
    
    Formulas are one of the many shortcuts that data analysts use. 
    
    ### Functions
    
    - COUNTIF: returns the number of cells that match a specified value. The basic syntax for COUNTIF is: **=COUNTIF(range, criterion)**
    - SUMIF. The basic syntax of a SUMIF function is: **=SUMIF(range, criterion, sum_range).** The first range is where the function will search for the condition that you have set. The criterion is the condition you are applying and the sum_range is the range of cells that will be included in the calculation.
    - AVERAGEIF
    - SUMIFS. SUMIFS can include multiple conditions. The basic syntax is: **=SUMIFS(sum_range, criteria_range1, criterion1, [criteria_range2, criterion2, ...])**
    - COUNTIFS. COUNTIFS has the same basic syntax as SUMIFS: **=COUNTIFS(criteria_range1, criterion1, [criteria_range2, criterion2, ...])**
    - IFS
    - VLOOKUP
    - INDEX
    - MATCH
    - IF with AND, OR, and NOT.
    - MAXIFS. The basic syntax is: **=MAXIFS(max_range, range1, criteria1, [range2], [criteria2],…)**
    - SUMPRODUCT: multiplies arrays and returns the sum of those products (multiplies each of the values in two or more arrays together.).  The basic syntax is: **=SUMPRODUCT(array1, [array2],…)**. An array is kind of like a range in a spreadsheet. It can be used to find the total revenue, for example.
    
    ### Pivot table
    
    Pivot tables let you view data in multiple ways to find insights and trends. But pivot tables can also help with calculations, organize, and filter data. It is a tool used to sort, reorganize, group, count, total, or average data in spreadsheets. They can help you quickly make sense of larger data sets by comparing metrics, performing calculations, and generating reports.
    
    A pivot table has four **basic parts**:
    
    - Rows: organize and group data you select horizontally.
    - Columns: organize and display values from your data vertically. Similar to rows, columns can be pulled directly from the data set or created using **values**.
    - Values: are used to calculate and count data. This is where you input the variables you want to measure. This is also how you create calculated fields in your pivot table.
    - Filters: enables you to apply filters based on specific criteria — just like filters in regular spreadsheets
    
    When you're in your job, you want to answer the questions that your manager and stakeholders ask. But you also want to answer the ones that come up while you're doing your analysis.
    
    To **create a pivot table**:
    
    1. Insert menu > Pivot table
    2. Adding it a new sheet is especially helpful when working on a large dataset. It helps keep our calculations together in one place and separate from the rest of the data.
    3. It’s a good practice to rename the sheet according to what you are trying to do there.
    4. Once you have created your pivot table, there will be a pivot table editor that you can access to the right of your data. This is where you will be able to customize your pivot table, including what variables you want to include for your analysis. ****We can build our pivot table starting with the rows.
        
        ![Untitled](Google_Data_Analytics_images/Untitled%2056.png)
        
        1. We can create a pivot date group for date rows to see only by year or by month. Right-click on any value of the column > Create pivot date group
    5. Next, we can work with the values.
        1. The Summarize By drop-down menu changes the function applied to the values. The SUM function is the default function, but there are other options, such as COUNT.
    6. After that, we can start adding columns but with the option “values”.
    7. You can create some basic visualizations based on your custom tables to share your findings with stakeholders. Select any cell in your pivot table and then navigate to the Insert menu. Select Insert Chart.
    - We can copy and paste the pivot table to create more
    - The filters will be applied to the entire table.
    - We can create calculated fields as Value columns. In excel we can create it using the field settings and the create formula menu. This is useful to check the accuracy of some of our data before analyzing it.
    
    ### More SQL calculations
    
    The operators are embedded in the queries when pulling data from a database. Just like spreadsheet formulas, there are a few different ways to perform calculations using queries. 
    
    The syntax of a query is its structure. It should include all the specific details of the data you want to pull into a new table where those details should be placed.
    
    For arithmetic calculation, you just need to use the symbols inside the SELECT statement.
    
    A lot of times, you can use functions instead of operators to complete calculations. For example, the SUM function can complete addition problems in spreadsheets and SQL. The AVERAGE function in a spreadsheet is the same as the AVG function in SQL. They both return the average value of a set of numbers. In SQL, these functions are considered aggregate functions because they perform a calculation on one or more values and return a single value.
    
    Sometimes you will need to group data before completing calculations, and the **GROUP BY** and **ORDER BY** commands are great for this. These commands are usually paired with aggregate functions like SUM or COUNT.
    
    - GROUP BY: A command that groups rows that have the same values from a table into summary rows. It comes at the end of the query
    - ORDER BY: It orders the data. By default, it is in ascending order. We can add DESC at the end of the query to order it in descending order.
    
    ## THE DATA-VALIDATION PROCESS
    
    Using data validation lets you control what can and can't be entered into your worksheet. One of its uses is protecting structured data and formulas in your spreadsheets. The data validation function is just one part of a larger data validation process. The data validation function is just one part of a larger data validation process.
    
    While the data validation process is a form of data cleaning, you should use it throughout your analysis. This will help you understand your data, check that it's clean, and make sure you're aligning with your business objectives. In other words, it's what you do to make sure your data makes sense.
    
    Keep in mind, you'll build your business knowledge with time and experience. And here's a pro tip. Asking as many questions as possible whenever you need to will make this much easier.
    
    You should always do data validation no matter what analysis tool you're using.
    
    By building in these types of checks as part of your data validation process, you can avoid errors in your analysis and complete your business objectives to make everyone happy.
    
    ### Types of data validation
    
    1. Data type: Validating a data type means checking that the data matches the data type defined for the field.
        - **Purpose**: Check that the data matches the data type defined for a field.
        - **Example**: Data values for school grades 1-12 must be a numeric data type.
        - **Limitations**: The data value 13 would pass the data type validation but would be an unacceptable value. For this case, data range validation is also needed.
    2. Data range: Validating a data range means checking that the data falls within an acceptable range of values defined for the field.
        - **Purpose**: Check that the data falls within an acceptable range of values defined for the field.
        - **Example**: Data values for school grades should be values between 1 and 12.
        - **Limitations**: The data value 11.5 would be in the data range and would also pass as a numeric data type. But, it would be unacceptable because there aren't half grades. For this case, data constraint validation is also needed.
    3. Data constraints: Validating data constraints means checking that data meets certain conditions or criteria, such as type of characters.
        - **Purpose**: Check that the data meets certain conditions or criteria for a field. This includes the type of data entered as well as other attributes of the field, such as number of characters.
        - **Example**: Content constraint: Data values for school grades 1-12 must be whole numbers.
        - **Limitations**: The data value 13 is a whole number and would pass the content constraint validation. But, it would be unacceptable since 13 isn’t a recognized school grade. For this case, data range validation is also needed.
    4. Data consistency: Validating data consistency means checking that the data makes sense in the context of other related data.
        - **Purpose**: Check that the data makes sense in the context of other related data.
        - **Example**: Data values for product shipping dates can’t be earlier than product production dates.
        - **Limitations**: Data might be consistent but still incorrect or inaccurate. A shipping date could be later than a production date and still be wrong.
    5. Data Structure: Validating data structure means checking that the data follows or conforms to a set structure, such as MP3 files or HTML code.
        - **Purpose**: Check that the data follows or conforms to a set structure.
        - **Example**: Web pages must follow a prescribed structure to be displayed properly.
        - **Limitations**: A data structure might be correct with the data still incorrect or inaccurate. Content on a web page could be displayed properly and still contain the wrong information.
    6. Code validation: Code validation means checking that the application code systematically performs any of the previously mentioned validations during user data input.
        - **Purpose:** Check that the application code systematically performs any of the previously mentioned validations during user data input.
        - **Example:** Common problems discovered during code validation include: more than one data type allowed, data range checking not done, or ending of text strings not well defined.
        - **Limitations:** Code validation might not validate all possible variations with data input.
    
    ## USING SQL WITH TEMPORARY TABLES
    
    ### Temporary tables
    
    Temporary tables, or temp tables, store subsets of data from standard data tables for a certain period. They are automatically deleted when you end your SQL session. They are useful when you only need a table for a short time to complete analysis tasks, like calculations.
    
    Instead of filtering the data over and over to return a subset, you can filter the data once and store it in a temporary table. This will let you run several queries about your data without having to keep filtering it.
    
    Naming and using temp tables can help you deal with a lot of data in a more streamlined way, so you don’t get lost repeating query after query with the same code that you could just include in a temporary table.
    
    They can be used as a holding area for storing values if you are making a series of calculations. This is sometimes referred to as **pre-processing** of the data.
    
    They can collect the results of multiple, separate queries. This is sometimes referred to as data **staging**. Staging is useful if you need to perform a query on the collected data or merge the collected data.
    
    ### Create a temporary table in SQL (BigQuery)
    
    We will use the **WITH** clause. The WITH clause is a type of temporary table that you can query from multiple times. It creates something that does the same thing as a temporary table. Even if it doesn’t add a table to the database you are working in for others to see, you can still see your results and anyone who needs to review your work can see the code that leads to your results.
    
    The general syntax for this method is:
    
    ```sql
    WITH *new_temp_table_name* AS(
    	SELECT *
    	FROM
    		*existing_table*
    	WHERE
    		tripduration >=60
    )
    ```
    
    • The opening parenthesis after the **AS** clause creates the subquery that filters the data from an existing table. The subquery is a regular **SELECT** statement along with a **WHERE** clause to specify the data to be filtered.
    
    • The closing parenthesis ends the subquery created by the **AS** clause.
    
    When the database executes this query, it will first complete the subquery and assign the values that result from that subquery to “*new_temp_table_name*,” which is the temporary table. You can then run multiple queries on this filtered data without having to filter the data every time.
    
    ![Untitled](Google_Data_Analytics_images/Untitled%2057.png)
    
    This will give us an error, but it will create a temporary table. Now, it is time to write a query.
    
    We just need to name the temporary query in another query. It is important to use the ## to describe the purpose of your query. This will help you remember the purpose of your query as you’re writing it. It can also help you share your work with others.
    
    ![Untitled](Google_Data_Analytics_images/Untitled%2058.png)
    
    If you need to end your session and start a new runtime later more servers store the code (Query history) using temp tables. You just need to recreate the table by running the code.
    
    ### Example
    
    ![Untitled](Google_Data_Analytics_images/Untitled%2059.png)
    
    ### Other types of temp tables.
    
    Instead of using the WITH clause, you can use the SELECT INTO or the CREATE TABLE clauses.
    
    The **SELECT INTO** clause copies data from one table into a new table, but doesn’t add the new table to the database. It’s useful if you want to make a copy of a table with a specific condition, like a query with a WHERE clause. BigQuery doesn't currently recognize the SELECT INTO command. Instead, here's an example of how a SELECT INTO statement might look in another RDBMS. Using SELECT and INTO, you can create a temporary table based on conditions defined by a WHERE clause to locate the information you need for the temporary table.
    
    ![Untitled](Google_Data_Analytics_images/Untitled%2060.png)
    
    - The INTO clause tells the database to store the data that is being requested in a new temporary table named, in this case, “AfricaSales.”
    
    Using **SELECT INTO** is a good practice when you want to keep the database uncluttered and you don't need other people using the table.
    
    The **CREATE TABLE** clause is a good option when several people need to access the same temp table. This statement adds the table into the database. In most relational database management systems or RDBMSs, you can add metadata to describe the data that's contained in the table you've created. This can help make the table easier to understand for anyone using it.
    
    ```sql
    CREATE TABLE *table_name*(
    	*column1 datatype*,
    	*column2 datatype*,
    	*column3 datatype*,
    ...
    )
    ```
    
    ![Untitled](Google_Data_Analytics_images/Untitled%2061.png)
    
    The **CREATE TABLE** statement is also useful for more complex tables. For example, if the code's difficult to replicate, then making a temp table in this way means it'll be safe for you to access later.
    
    After you have finished working with the table, you would then delete or drop it from the database at the end of your session.
    
    ```sql
    DROP TABLE *table_name*
    ```
    
    **Note:** BigQuery uses **CREATE TEMP TABLE** instead of **CREATE TABLE**, but the general syntax is the same.
    
    Which clause you use depends on your preference and the project’s demands. Different clauses have their own strengths, so understanding how each of them work is helpful for using them effectively.
    
    You may also find that you're working in an RDBMS that uses a different syntax. For example, you might need to use a **CREATE TEMP TABLE** statement instead of CREATE TABLE.
    
    You can repeat your code over and over instead of making a temp table but that usually leaves your queries less readable and more vulnerable to typos.
    
    ### Best practices when working with temporary tables
    
    - Global vs. local temporary tables: Global temporary tables are made available to all database users and are deleted when all connections that use them have closed. Local temporary tables are made available only to the user whose query or connection established the temporary table. You will most likely be working with local temporary tables. If you have created a local temporary table and are the only person using it, you can drop the temporary table after you are done using it.
    - Dropping temporary tables after use: Dropping a temporary table is a little different from deleting a temporary table. Dropping a temporary table not only removes the information contained in the rows of the table, but removes the table variable definitions (columns) themselves. Deleting a temporary table removes the rows of the table but leaves the table definition and columns ready to be used again. Although local temporary tables are dropped after you end your SQL session, it may not happen immediately. If a lot of processing is happening in the database, dropping your temporary tables after using them is a good practice to keep the database running smoothly.
    
    ## USING CONNECTED SHEETS WITH BIGQUERY
    
    Connected Sheets is a tool that allows data professionals to use basic spreadsheet functions to analyze large datasets housed in BigQuery. With Connected Sheets users don’t need to know SQL. Instead, anyone, not just data professionals, can generate insights with basic spreadsheet operations such as formulas, charts, and pivot tables.
    
    Recall that BigQuery allows users to analyze petabytes (a million gigabytes) of data using complex queries. A benefit of BigQuery is that it reduces the time needed to develop insights from large datasets.
    
    Google Sheets, on the other hand, is a spreadsheet tool that is easy to use and shareable with a familiar interface. It also allows simple and flexible analysis with tools like pivot tables, charts, and formulas.
    
    Connected Sheets integrates both BigQuery and Google Sheets, allowing the user to analyze billions of rows of data in Sheets without any need for specialized knowledge, such as SQL.
    
    Additionally, Connected Sheets is built to handle big data. Users won’t experience the same limitations or performance issues they’ve had in the past (such as data loss) when working with large data sets in spreadsheets.
    
    ### Why would a data analytics professional use Connected Sheets?
    
    As a data analytics professional, Connected Sheets can help with several tasks, such as:
    
    - Collaborating with partners, analysts, or other stakeholders in a familiar spreadsheet interface;
    - Ensuring a single source of truth for data analysis without additional .csv exports;
    - Defining variables so that all users are working with the same data;
    - Sharing insights with your team in a secure environment; and
    - Streamlining your reporting and dashboard workflows.
    
    Many teams and industries benefit from Connected Sheets such as finance, marketing, and operations teams.
    
    A few example use cases of Connected Sheets include:
    
    - **Business planning:** A user can build and prepare datasets, and then find insights from the data. For example, a data analyst can analyze sales data to determine which products sell better in different locations.
    - **Customer service:** A user can find out which stores have the most complaints per 10,000 customers.
    - **Sales:** A user can create internal finance and sales reports. After completing, they can share revenue reports with sales reps.
    - **Logistics, fulfillment, and delivery:** A user can run real-time inventory management and intelligent analytics tools.
- 6 - **SHARE** DATA THROUGH THE ART OF VISUALIZATION
    
    One question to ask yourself is: “what is the best way to tell the story within my data?”
    
    Tableau helps us create visualizations from our analysis so that we can share our findings more effectively.
    
    Data visualization is putting information into an image to make it easier for other people to understand.
    
    Scientists and mathematicians began to truly embrace the idea of arranging data visually in the 1700s and 1800s.
    
    As we keep learning how to more efficiently communicate with visuals, the quality of our insights continues to grow too.
    
    As an analyst in today’s world, we split your time with data visuals in two ways:
    
    1. Looking at visuals to understand and draw conclusions about data.
    2. Creating visuals using raw data to tell a story.
    
    A well-made data visualization has the power to change people’s minds. Plus, it can help someone who doesn’t have the same technical background or experience as you from their own opinions.
    
    ## UNDERSTAND DATA VISUALIZATION
    
    A quick rule for creating visualizations is: that your audience should know exactly what they’re looking at **within the first five seconds of seeing it**. In the five seconds after that, your audience should understand the conclusion your visualization is making, even if they aren’t familiar with the research you’ve been doing.
    
    When creating data visualizations, you must strike a balance between presenting enough information for your audience to understand the meaning of the visualization and not overwhelming them with too much detail.
    
    If a visualization looks confusing, then it probably is confusing.
    
    Part of why data visualization is so effective is because people’s eyes are drawn to colors, shapes, and patterns, which makes those visual elements perfect for telling a story that goes beyond just numbers.
    
    ### Rules about what makes a helpful data visualization
    
    - **Five-second rule:** A data visualization should be **clear, effective, and convincing** enough to be absorbed in five seconds or less.
    - **Color contrast:** Graphs and charts should use a **diverging color palette** to show contrast between elements.
    - **Conventions and expectations:** Visuals and their organization should align with **audience expectations** and **cultural conventions**. For example, if the majority of your audience associates green with a positive concept and red with a negative one, your visualization should reflect this.
    - **Minimal labels:** Titles, axes, and annotations should use as **few labels** as it takes to make sense. Having too many labels makes your graph or chart too busy. It takes up too much space and prevents the labels from being shown clearly.
    
    ### Frameworks for organizing your thoughts about visualization
    
    Frameworks help organize your thoughts about data visualization and give you a useful checklist to reference as you plan and evaluate your data visualization. Here are two frameworks that employ slightly different techniques. Both are intended to improve the quality of your visuals.
    
    - The McCandless method: set of guidelines for presentations. It suggests that you start with broad, general ideas and then work your way into the details.. This method lists four elements of good data visualization:
        1. **Information (data):** the data with which you’re working. Without information or data, you cannot communicate your findings successfully.
        2. **Story (concept):** a clear and compelling narrative or concept. The story allows you to share your data in meaningful and interesting ways. Without a story, your visualization is informative, but not really inspiring.
        3. **Goal (function):** a specific objective or function for the visual. The goal of your data visualization makes the data useful and usable. This is what you are trying to achieve with your visualization. Without a goal, your visualization might still be informative, but can’t generate actionable insights.
        4. **Visual form (metaphor):** an effective use of metaphor or visual expression. The visual form element is what gives your data visualization structure and makes it beautiful. Without visual form, your data is not visualized yet.
        
        ![Untitled](Google_Data_Analytics_images/Untitled%2062.png)
        
        And this method has an order, you start with the most basic information:
        
        1. Introduce the graphic by name
        2. Answer obvious questions before they’re asked. Start with the high-level information and work your way into the lowest level of detail that’s useful to your audience (where is this data from? what does it cover?, when?, how?)
        3. State the insight of your graphic.
        4. Call out data to support that insight.
        5. Tell your audience why it matters (the “so what” moment). It’s a good time to present the possible business impact of the solution and clear actions stakeholders can take
    - Kaiser Fung’s Junk Charts trifecta checkup: This approach is a set of questions that can help consumers of data visualization critique what they are consuming and determine how effective it is. You can also use these questions to determine if your data visualization is effective:
        1. What is the practical question?
        2. What does the data say?
        3. What does the visual say?
        
        A well-designed visual effectively answers all three of those questions at once. Moreover, this framework helps you think about your data viz from the perspective of your audience.
        
    
    ### Pre-attentive attributes
    
    Pre-attentive attributes are the elements of a data visualization that people recognize automatically and without conscious effort. The essential, basic building blocks that make visuals immediately understandable are called marks and channels.
    
    - **Marks:** are basic visual objects such as points, lines, and shapes. Every mark can be broken down into four qualities:
        1. Position: Where is a specific mark in space relative to a scale or other marks?. For example, if you’re looking at two different trends, position allows you to compare the pattern of one element relative to another.
            
            ![Untitled](Google_Data_Analytics_images/Untitled%2063.png)
            
        2. Size: How big, small, long, or tall is a mark? This can be very useful for conveying the relationship between categories or data points. Controlling the scale of a visual is important even when comparative sizes are not intended to offer information.
            
            ![Untitled](Google_Data_Analytics_images/Untitled%2064.png)
            
        3. Shape: Does the shape of a specific object communicate something about it? Rather than using simple dots or lines, a bit of creativity can enhance how quickly people are able to interpret a visual by using shapes that align with a given application.
            
            ![Untitled](Google_Data_Analytics_images/Untitled%2065.png)
            
        4. Color: What color is a mark? Colors can be used both as a simple differentiator of groupings or as a way to communicate other concepts such as profitable versus unprofitable, or hot versus cold.
            
            ![Untitled](Google_Data_Analytics_images/Untitled%2066.png)
            
    - **Channels:** are visual aspects or variables that represent characteristics of the data in a visualization. They are basically specialized marks that have been used to visualize data. It’s important to understand that channels vary in terms of how effective they are at communicating data based on three elements:
        1. Accuracy: Are the channels helpful in accurately estimating the values being represented? For example, color is very accurate when communicating categorical differences, such as apples and oranges. But it is much less effective when distinguishing quantitative data, such as 5 from 5.5.
        2. Popout: How easy is it to distinguish certain values from others? There are many ways of drawing attention to specific parts of a visual, and lots of them leverage pre-attentive attributes including line length, size, line width, shape, enclosure, hue, and intensity.
            
            ![Untitled](Google_Data_Analytics_images/Untitled%2067.png)
            
        3. Grouping: How effective is a channel at communicating groups that exist in the data? Consider the proximity, similarity, enclosure, connectedness, and continuity of the channel.
            
            ![Untitled](Google_Data_Analytics_images/Untitled%2068.png)
            
        
        But, remember: The more you emphasize one single thing, the more that counts. Emphasis diminishes with each item you emphasize because the items begin to compete with one another.
        
    
    ### Types data visualizations
    
    - Bar graphs: Use size contrast to compare two or more values. A bar chart is ideal for comparing similar data side by side.
        - In bar graphs with vertical bars, the x-axis is used to represent categories, time periods, or other variables.
        - The y-axis usually has a scale of values for the variables.
        - By making the y-axis start at zero, we’re changing the visual proportions to be more accurate and more honest.
        - A bar chart should always be ranked by value, unless there is a natural order of the data like age or time, for example.
        - Bar charts with horizontal bars effectively show data that are ranked, with bars arranged in ascending or descending order
    - Histogram: A histogram resembles a bar graph, but it’s a chart that shows how often data values fall into certain ranges. A histogram is ideal for comparing the distribution of two variables by individual grouping.
    - Line graphs: Help your audience understand shifts or changes in your data.
        - They’re usually used to track changes or trends through a period of time, but they can be paired with other factors too.
        - When smaller changes exist, line charts are better to use than bar graphs.
        - Line charts can also be used to compare changes over the same period of time for more than one group.
        - The last line chart example is a combo chart which can include a line chart
        
        ![Untitled](Google_Data_Analytics_images/Untitled%2069.png)
        
    - Pie charts: It’s a circular graph that is divided into segments representing proportions corresponding to the quantity it represents, especially when dealing with parts of a whole.
    - Maps: help organize data geographically.
    - Correlation charts: Show relationships among data.
        - It should be used with caution because they might lead viewers to think that the data shows causation.
    - Column chart:  A column chart allows you to display and compare multiple categories of data by their values. use size to contrast and compare two or more values, using height or lengths to represent the specific values.
    
    ![Untitled](Google_Data_Analytics_images/Untitled%2070.png)
    
    - Area charts: allow you to track changes in value across multiple categories of data.
    - Heatmap: Similar to bar charts, heatmaps also use color to compare categories in a data set. They are mainly used to show relationships between two variables and use a system of color-coding to represent different values.
    
    ![Untitled](Google_Data_Analytics_images/Untitled%2071.png)
    
    - Scatterplot: show relationships between different variables.
        - typically used for showing the relationship between two variables., although additional variables can be displayed.
        - scatterplots are typically used to display trends in numeric data.
        
        ![Untitled](Google_Data_Analytics_images/Untitled%2072.png)
        
    - Distribution graph: displays the spread of various outcomes in a dataset.
    - Combo: combo charts use multiple visual markers like columns and lines to showcase different aspects of the data in one visualization. The example below is a combo chart that has a column and line chart together.
        
        ![Untitled](Google_Data_Analytics_images/Untitled%2073.png)
        
    
    ### Guides - resources:
    
    Below is a list of resources that can inspire your next data-driven decisions, as well as teach you how to make your data more accessible to your audience:
    
    [The data visualization catalogue](https://datavizcatalogue.com/#google_vignette)
    
    [The 25 best data visualizations](https://visme.co/blog/best-data-visualizations/)
    
    [10 data visualization blogs](https://www.tableau.com/learn/articles/best-data-visualization-blogs)
    
    [Information is beautiful](https://informationisbeautiful.net/wdvp/gallery-2019/)
    
    [Data studio gallery](https://datastudio.google.com/gallery?category=visualization)
    
    ### Tips to create a powerful visualization
    
    One of your biggest considerations when creating a data visualization is where you’d like your audience to focus.
    
    - As long as it’s not misleading, you should visually represent only the data that your audience needs in order to understand your findings.
    - with your visualization is important to show:
        - Change over time. The time period relevant to your objective.
        - How your data is distributed
    - Reviewing each of these visual examples, where do you notice that they fit in relation to your type of data? One way to answer this is by evaluating patterns in data. Meaningful patterns can take many forms, such as:
        - **Change:** This is a trend or instance of observations that become different over time. A great way to measure change in data is through a line or column chart.
        - **Clustering:** A collection of data points with similar or different values. This is best represented through a distribution graph.
        - **Relativity:** These are observations considered in relation or in proportion to something else. You have probably seen examples of relativity data in a pie chart.
        - **Ranking:** This is a position in a scale of achievement or status. Data that requires ranking is best represented by a column chart.
        - **Correlation:** This shows a mutual relationship or connection between two or more things. A scatterplot is an excellent way to represent this type of data pattern.
    
    ### Correlation and causation
    
    - Correlation: ****in statistics is the measure of the degree to which two variables move in relationship to each other.
        - Correlation doesn’t mean that one event causes another. But, it does indicate that they have a pattern with or a relationship to each other.
        - It one variable goes up and the other variable goes up, it is a positive correlation.
        - If one variable goes up and the other variable goes down, it is a negative or inverse correlation.
        - If one variable goes up and the other variable stays about the samethere is no correlation.
    - Causation: refers to the idea that an event leads to a specific outcome.
    
    When you make conclusions from data analysis, you need to make sure that you don’t assume a causal relationship between elements of your data when there is only a correlation.
    
    In your data analysis, remember to:
    
    - Critically analyze any correlations that you find
    - Examine the data’s context to determine if a causation makes sense (and can be supported by all of the data)
    - Understand the limitations of the tools that you use for analysis
    
    ### Static and Dynamic visualizations
    
    - Static visualization: do not change over time unless they’re edited.
        - Useful when you want to control your data and your data story.
        - Any visualization printed on paper is automatically static
        - Charts and graphs created on spreadsheets are often static too.
    - Dynamic visualization: visualizations that are interactive or change over time.
        - The interactive nature of these graphics means that users have some control over what they see.
        - Helpful when the stakeholders want to adjust what they’re able to view.
        - Visualizations in Tableau are automatically interactive.
        - Other dynamic visualizations upload new data automatically.
    
    The choice between using a static or dynamic visualization usually depends on:
    
    - the data you’re visualizing.
    - The audience you are presenting to
    - How you’re giving your presentation.
    
    ### Data grows on decision trees
    
    A decision tree is a decision-making tool that allows you to make decisions based on key questions that you can ask yourself. There are many different types of decision trees that vary in complexity, and can provide more in-depth decisions.
    
    ![Untitled](Google_Data_Analytics_images/Untitled%2074.png)
    
    Start off by evaluating the type of data you have and go through a series of questions to determine the best visual source:
    
    - Does your data have only one numeric value? If you have data that has one, continuous, numerical variable, then a histogram or density plot are the best methods of plotting your categorical data. Depending on your type of data, a bar chart can even be appropriate in this case.
    - Are there multiple datasets? For cases dealing with more than one set of data, consider a line or pie chart for an accurate representation of your data. A line chart will connect multiple data sets over a single, continuous line, showing how numbers have changed over time. A pie chart is good for dividing a whole into multiple categories or parts.
    - Are you measuring changes over time? A line chart is usually adequate for plotting trends over time. However, when the changes are larger, a bar chart is the better option.
    - Do relationships between the data need to be shown? When you have two variables for one set of data, it is important to point out how one affects the other. Variables that pair well together are best plotted on a scatterplot. However, if there are too many data points, the relationship between variables can be obscured so a heat map can be a better representation in that case.
    
    More decision trees examples:
    
    - Picture taken from: [Marina R. on LinkedIn: #datavisualization #data #dataanalytics #análisisdedatos #data…](https://www.linkedin.com/posts/activity-7150034395276419072-WXwD/?utm_source=share&utm_medium=member_android)
        
        ![Untitled](Google_Data_Analytics_images/Untitled%2075.png)
        
    - [From data to visualization](https://www.data-to-viz.com/)
    - [Selecting the best chart](https://www.youtube.com/watch?v=C07k0euBpr8)
    
    ## DESIGN DATA VISUALIZATIONS
    
    Communicating data visually is a form of art.
    
    The elements of art are:
    
    - Line: can be curved or straight, thick or thin, vertical, horizontal, or diagonal. They can add visual form to your data, and build a structure for your visualization.
    - Shape: should always be two-dimensional, because three-dimensional objects in a visualization can complicate the visual and confuse the audience. It is a great way to add eye-catching contrast, especially size contrast to your data story.
    - color: colors can be described by their:
        - hue: its name.
        - intensity: how bright or dull a color is
        - value: how light or dark the colors are in a visualization. Varying the color value can be a very effective way to draw our audience’s attention to specific areas.
    - Space: the area between, around, and in the objects. There should always be space in data visualizations, just not too much or too little.
    - Movement: used to create a sense of flow or action in a visualization. This is something that should be used sparingly, there is a fine line between attracting attention and distracting the audience.
    
    These particular ones can add value to your data viz by making them more visually effective and compelling.
    
    ### Principles of design
    
    There are nine basic principles of design that data analysts should think about when building their visualizations.
    
    1. **Balance:** The design of a data visualization is balanced when the key visual elements, like color and shape, are distributed evenly. Your visualization shouldn’t have one side distracting from the other.
    2. **Emphasis:** Your visualizations should emphasize the most important data so that users recognize it first. Using color and value is one effective way to make this happen. By using contrasting colors, you can make certain that graphic elements—and the data shown in those elements—stand out.
    3. **Movement:** Movement can refer to the path the viewer’s eye travels as they look at a data visualization, or literal movement created by animations. Movement in data visualization should mimic the way people usually read. You can use lines and colors to pull the viewer’s attention across the page.
    4. **Pattern:** You can use similar shapes and colors to create patterns in your data visualization. This can be useful in a lot of different ways. For example, you can use patterns to highlight similarities between different data sets, or break up a pattern with a unique shape, color, or line to create more emphasis.
    5. **Repetition:** Repeating chart types, shapes, or colors adds to the effectiveness of your visualization.
    6. **Proportion:** Using various colors and sizes helps demonstrate that you are calling attention to a specific visual over others. If you make one chart in a dashboard larger than the others, then you are calling attention to it. It is important to make sure that each chart accurately reflects and visualizes the relationship among the values in it.
    
    These first six principles of design are key considerations that you can make while you are creating your data visualization. These next three principles are useful checks once your data visualization is finished.
    
    1. **Rhythm:** This refers to creating a sense of movement or flow in your visualization. Rhythm is closely tied to the movement principle. If your finished design doesn’t successfully create a flow, you might want to rearrange some of the elements to improve the rhythm.
    2. **Variety:** Your visualizations should have some variety in the chart types, lines, shapes, colors, and values you use. Variety keeps the audience engaged. But it is good to find balance since too much variety can confuse people. The variety you include should make your dashboards and other visualizations feel interesting and unified.
    3. **Unity:** your final data visualization should be cohesive. If the visual is disjointed or not well organized, it will be confusing and overwhelming.
    
    ### Data visualization impact
    
    Choosing the right visualization for your data findings can often come down to one question: **which one will make it easiest for the user to understand the point you’re trying to make?**
    
    - When comparing over time line graphs, bar graphs, stacked bar graphs, and area charts are good ways to visualize how data changes over time.
    - When you are comparing distinct objects, ordered bar, ordered column charts, and group bar graphs are useful.
    - Some charts show parts of a whole (data composition) like stack bars, donuts, stacked areas, pie charts, and tree maps.
    - To show the relationship in your data, you might want to use a scatterplot, bubble charts, column/line charts, and heat maps.
    
    A successful data visualization results in a happy audience, so it’s important to understand how your audience is viewing your data visualization since they should always be top of mind.
    
    Visual journalist Dana Wong proposes that effective visuals have three essential elements:
    
    1. Clear meaning. Good visualizations clearly communicate their intended insight.
    2. Sophisticated use of contrast. It helps separate the most important data from the rest using visual context that our brains naturally look for.
    3. Refined execution. Visuals with refined execution include deep attention to detail. This is done by using visual elements (elements of art)
    
    ### Design thinking
    
    It is a process used to solve complex problems in a user-centric way.
    
    There are five phases of the design process:
    
    1. Empathize. You think about the emotions and needs of the target audience of your data viz
    2. Define. It helps you to find your audience's needs, their problems, and your insights. You will use what you learned in the empathize face to help you spell out exactly what your audience needs from your visualization
    3. Ideate. Here you start to generate your data viz ideas. You’ll use all of your findings from the empathize and define phases to brainstorm potential data viz solutions.
    4. Prototype. You’ll start putting your charts, dashboards, or other visualizations together
    5. Test. By showing them to team members before presenting them to stakeholders.
    
    ## VISUALIZATION CONSIDERATIONS
    
    ### Pro tips for highlighting key information
    
    - **Headlines that pop:** a line of words printed in large letters at the top of a visualization to communicate what data is being presented. It is the attention grabber that makes your audience want to read more.
    - **Subtitles that clarify:** They support the headline by adding more context and description. Adding a subtitle will help the audience better understand the details associated with your chart. Typically, the text for subtitles has a smaller font size than the headline.
    - **Labels that identify:** it identifies data in relation to other data. Most commonly, labels in a chart identify what the x-axis and y-axis show. Always make sure you label your axes.
        
        ![Untitled](Google_Data_Analytics_images/Untitled%2076.png)
        
        Data can also be labeled directly in a chart instead of through a chart legend. This makes it easier for the audience to understand data points without having to look up symbols or interpret the color coding in a legend.
        
        ![Untitled](Google_Data_Analytics_images/Untitled%2077.png)
        
    - **Annotations that focus:** An annotation briefly explains data or helps focus the audience on a particular aspect of the data in a visualization.
        
        ![Untitled](Google_Data_Analytics_images/Untitled%2078.png)
        
    
    | **Visualization
    components** | **Guidelines** | **Style checks** |
    | --- | --- | --- |
    | Headlines | - **Content**: Briefly describe the data
    - **Length**: Usually the width of the data frame
    - **Position**: Above the data | - Use brief language 
    - Don’t use all caps
    - Don’t use italic
    - Don’t use acronyms
    - Don't use abbreviations
    - Don’t use humor or sarcasm |
    | Subtitles | - **Content**: Clarify context for the data
    - **Length**: Same as or shorter than headline 
    - **Position**: Directly below the headline | - Use smaller font size than headline
    - Don’t use undefined words 
    - Don’t use all caps, bold, or italic
    - Don’t use acronyms 
    - Don't use abbreviations |
    | Labels | - **Content**: Replace the need for legends
    - **Length**: Usually fewer than 30 characters
    - **Position**: Next to data or below or beside axes | - Use a few words only
    - Use thoughtful color-coding
    - Use callouts to point to the data
    - Don’t use all caps, bold, or italic |
    | Annotations | - **Content**: Draw attention to certain data 
    - **Length**: Varies, limited by open space
    - **Position**: Immediately next to data annotated | - Don’t use all caps, bold, or italic
    - Don't use rotated text
    - Don’t distract viewers from the data |
    
    ### Ways to make data visualizations accessible:
    
    - Labeling
    - Text alternatives, so that it can be changed into other forms people need such as large print, braille, or speech.
    - Text-based format
    - Distinguishing
    - Simplify
    
    ### Design a chart in 60 minutes
    
    A chart is a graphical representation of data.
    
    ![Untitled](Google_Data_Analytics_images/Untitled%2079.png)
    
    - **Prep (5 min):** Create the mental and physical space necessary for an environment of comprehensive thinking. This means allowing yourself room to brainstorm *how* you want your data to appear while considering the amount and type of data that you have.
    - **Talk and listen (15 min):** Identify the object of your work by getting to the “ask behind the ask” and establishing expectations. Ask questions and concentrate on feedback from stakeholders regarding your projects to help you hone how to lay out your data.
    - **Sketch and design (20 min):** Draft your approach to the problem. Define the timing and output of your work to get a clear and concise idea of what you are crafting.
    - Prototype and improve (20 min): Generate a visual solution and gauge its effectiveness at accurately communicating your data. Take your time and repeat the process until a final visual is produced. It is alright if you go through several visuals until you find the perfect fit.
    
    ## TABLEAU
    
    Tableau is a Business Intelligence and analytics platform that helps people see, understand, and make decisions with data.
    
    ### Which chart or Graph should I use?
    
    - [Which chart or graph is right for you?](http://www.tableau.com/sites/default/files/media/which_chart_v6_final_0.pdf) This presentation covers 13 of the most popular charts in Tableau.
    - [The Ultimate Cheat Sheet on Tableau Charts](https://towardsdatascience.com/the-ultimate-cheat-sheet-on-tableau-charts-642bca94dde5). This blog describes 24 chart variations in Tableau and guidelines for use.
    
    In addition to more traditional charts, Tableau also offers some more specific visualizations that you can use in your dashboard design:
    
    - **Highlight tables** appear like tables with conditional formatting. Review the [steps to build a highlight table](https://help.tableau.com/current/pro/desktop/en-us/buildexamples_highlight.htm).
    - **Heat maps** show intensity or concentrations in the data. Review the [steps to build a heat map](https://help.tableau.com/current/pro/desktop/en-us/buildexamples_highlight.htm).
    - **Density maps** illustrate concentrations (such as a population density map). Refer to [instructions to create a heat map for density](https://help.tableau.com/current/pro/desktop/en-us/maps_howto_heatmap.htm).
    - **Gantt charts** demonstrate the duration of events or activities on a timeline. Review the [steps to build a Gantt chart](https://help.tableau.com/current/pro/desktop/en-us/buildexamples_gantt.htm).
    - **Symbol maps** display a mark over a given longitude and latitude. Learn more from this [example of a symbol map](https://interworks.com/blog/ccapitula/2014/08/18/tableau-essentials-chart-types-symbol-map/).
    - **Filled maps** are maps with areas colored based on a measurement or dimension. Explore an [example of a filled map](https://interworks.com/blog/ccapitula/2014/09/23/tableau-essentials-chart-types-filled-map/).
    - **Circle views** show comparative strength in data. Learn more from this [example of a circle view](https://interworks.com/blog/ccapitula/2014/10/17/tableau-essentials-chart-types-circle-view/).
    - **Box plots**, also known as **box and whisker charts,** illustrate the distribution of values along a chart axis. Refer to the [steps to build a box plot](https://help.tableau.com/current/pro/desktop/en-us/buildexamples_boxplot.htm).
    - **Bullet graphs** compare a primary measure with another and can be used instead of dial gauge charts. Review the [steps to build a bullet graph](https://help.tableau.com/current/pro/desktop/en-us/qs_bullet_graphs.htm).
    - **Packed bubble charts** display data in clustered circles. Review the [steps to build a packed bubble chart](https://help.tableau.com/current/pro/desktop/en-us/buildexamples_bubbles.htm).
    
    ### Creating a Report-Tips
    
    - Once you upload data to your worksheet, it will populate the Connections pane.
    
    ![Untitled](Google_Data_Analytics_images/Untitled%2080.png)
    
    - You can add more connections to other data sources in order to build visualizations that compare different datasets. Simply drag and drop tables from the Sheets section in order to join tables and generate those connections:
    
    ![Untitled](Google_Data_Analytics_images/Untitled%2081.png)
    
    - Tableau uses dimensions and measures to generate customized charts. For example, check out this chart focusing on CO2 emissions per country. The Country Name dimension can be used to show a map of the countries on the planet with dots indicating which countries are represented in the data.
    
    ![Untitled](Google_Data_Analytics_images/Untitled%2082.png)
    
    - Tableau has a wide variety of options for depicting the measure for a given dimension. Most of these options are contained near the main display and the column with dimensions and measures.
    
    ![Untitled](Google_Data_Analytics_images/Untitled%2083.png)
    
    - One option in Tableau is choosing between a vertical or horizontal layout. A vertical layout adjusts the height, a horizontal layout resizes the width of the views and objects it contains. Evenly distributing the items within your layout helps create a clear and organized data visual.
    - You can select either tiled or floating layouts. Tiled items are part of a single-layer grid that automatically resizes based on the overall dashboard size. Floating items can be layered over other objects.
    
    ## DESIGN VISUALIZATIONS IN TABLEAU
    
    There are some tools and options that will help us design a more optimal data visualization.
    
    ### Essential design principles
    
    | **Principle** | **Description** |
    | --- | --- |
    | Choose the right visual | One of the first things you have to decide is which visual will be the most effective for your audience. Sometimes, a simple table is the best visualization. Other times, you need a more complex visualization to illustrate your point. Be sure that you are not creating misleading or deceptive charts. |
    | Optimize the data-ink ratio | The data-ink entails focusing on the part of the visual that is essential to understanding the point of the chart. Try to minimize non-data ink like boxes around legends or shadows to optimize the data-ink ratio. |
    | Use orientation effectively | Make sure the written components of the visual, like the labels on a bar chart, are easy to read. You can change the orientation of your visual to make it easier to read and understand. |
    | Color | There are a lot of important considerations when thinking about using color in your visuals. These include using color consciously and meaningfully, staying consistent throughout your visuals, being considerate of what colors mean to different people, and using inclusive color scales that make sense for everyone viewing them. |
    | Numbers of elements | Think about how many elements you include in any visual. If your visualization uses lines, try to plot five or fewer. If that isn’t possible, use color or hue to emphasize important lines. Also, when using visuals like pie charts, try to keep the number of segments to less than seven since too many elements can be distracting. |
    
    ### How to avoid misleading visualizations
    
    | **What to avoid** | **Why** |
    | --- | --- |
    | Cutting off the y-axis | Changing the scale on the y-axis can make the differences between different groups in your data seem more dramatic, even if the difference is actually quite small. |
    | Misleading use of a dual y-axis | Using a dual y-axis without clearly labeling it in your data visualization can create extremely misleading charts. |
    | Artificially limiting the scope of the data | If you only consider the part of the data that confirms your analysis, your visualizations will be misleading because they don’t take all of the data into account. |
    | Problematic choices in how data is binned or grouped | It is important to make sure that the way you are grouping data isn’t misleading or misrepresenting your data and disguising important trends and insights. |
    | Using part-to-whole visuals when the totals do not sum up appropriately | If you are using a part-to-whole visual like a pie chart to explain your data, the individual parts should add up to equal 100%. If they don’t, your data visualization will be misleading. |
    | Hiding trends in cumulative charts | Creating a cumulative chart can disguise more insightful trends by making the scale of the visualization too large to track any changes over time. |
    | Artificially smoothing trends | Adding smooth trend lines between points in a scatter plot can make it easier to read that plot, but replacing the points with just the line can actually make it appear that the point is more connected over time than it actually was. |
    
    ### Optimize the color palette in data visualization
    
    A diverging color palette displays two ranges of values using color intensity to show the magnitude of the number and the actual color to show which range the number is from. It’s a good way to show the difference between numbers.
    
    The colors you choose should fit within the scope of the audience’s expectations.
    
    ### How to choose a data visualization - Extra Google PDF
    
    https://d3c33hcgiwev3.cloudfront.net/XsaUfemhQ-qGlH3poXPqMg_4b74b6280a7a4a10a83e6b5ca9138630_How-to-choose-a-data-visualization.pdf?Expires=1719360000&Signature=kHXaDtgXv-4NH4YSAgNPcKkWzG-SwFQuUgt60O11tnqNzckm4trZPKF5eDhet7PAK0luSwGgujFqQkXz3exwANNv43MBzzfwPhiceytvkBHv-~Rar3H9GZUsX-uKBcv4UU4VCxnqscZ0gAglvvN9hvaWoG2Rz5keHQDsp267BbA_&Key-Pair-Id=APKAJLTNE6QMUY6HBC5A
    
    Check the guide for a better understanding of how to choose a data visualization.
    
    ### Live Vs Static insights
    
    Static data involves providing screenshots or snapshots in presentations or building dashboards using snapshots of data.
    
    - Can tightly control a point-in-time narrative of the data and insight
    - Allows for complex analysis to be explained in-depth to a larger audience
    - Insight immediately begins to lose value and continues to do so the longer the data remains in a static state
    - Snapshots can't keep up with the pace of data change
    
    Live data means that you can build dashboards, reports, and views connected to automatically updated data. 
    
    - Dashboards can be built to be more dynamic and scalable
    - Gives the most up-to-date data to the people who need it at the time when they need it
    - Allows for up-to-date curated views into data with the ability to build a scalable “single source of truth” for various use cases
    - Allows for immediate action to be taken on data that changes frequently
    - Alleviates time/resources spent on processes for every analysis
    - Can take engineering resources to keep pipelines live and scalable, which may be outside the scope of some companies' data resource allocation
    - Without the ability to interpret data, you can lose control of the narrative, which can cause data chaos (i.e. teams coming to conflicting conclusions based on the same data)
    - Can potentially cause a lack of trust if the data isn’t handled properly
    
    ## DATA-DRIVEN STORYTELLING. 3 DATA STORYTELLING STEPS
    
    Data storytelling is communicating the meaning of a dataset with visuals and a narrative that are customized for each particular audience. Stories make people care.
    
    ### 1. Engage your audience.
    
    Engagement is capturing and holding someone’s interest and attention.
    
    1. Know your audience. Think about how your data project might affect them. What role does this audience play? What is their take in the project? What do they hope to get from the data insights I deliver?
    2. Choose your primary message. To get the key message you need to take a few steps back and pinpoint only the most useful pieces, because not every piece of data is relevant to the questions you’re trying to answer. To do a spotlight, you write each insight from your analysis in a piece of paper, spread them out, and display them in a white board, then you examine it. Look for broad and universal idea messages.
    
    ### 2. Create compelling visuals.
    
    There are key parts of your data story. The narrative you share with your stakeholders needs:
    
    - Characters. People affected by your story.
    - Setting. Describes what’s going on, how often is happening, what tasks are involved, and other background information about the data project that describes the current situation.
    - Plot. Conflict. It’s what creates tension in the current situation.
    - Big reveal. How the data has shown that you can solve the problem the characters are facing by becoming more competitive, improving a process, or whatever the ultimate goal of your data project may be.
    - Aha moment. You share your recommendation and explain why you think they’ll help your company be successful.
    
    ### 3. Tell the story in an interesting narrative.
    
     It should connect the data you’ve collected to the project objective and clearly explain important insights from your analysis.
    
    The presentation reflects on you. So ask yourself: what’s the single most important thing I want my audience to learn from my analysis?
    
    There are a lot of advantages on visuals:
    
    - Visuals help the audience quickly understand the content of each slide.
    - Great visuals don’t leave room for interpretation, because the meaning is instantly understood.
    - When you include visuals on a slide, try no to share too many details all at once, choose just the data points that support your points.
    - If you have several important things you need to include, create a new visual for each point. Then, add an arrow, a call-out, or another clearly labeled element to direct your audience’s attention toward what you want them to look at.
    - When you get to your big reveal and aha moment, your visuals must communicate these messages with clarity and excitement.
    - To ensure your audience is focused on what is being said, rather than reading slides, keep text to fewer than five lines and 25 words per slide.
    
    ## THE ART AND SCIENCE OF PRESENTATIONS
    
    The framework of your presentation starts with your understanding of the business task. When creating a presentation to share with stakeholders, the purposes of a framework are:
    
    - Give your audience context to better understand your data
    - Help you focus on the most important information
    - Create logical connections that tie back to the business task
    
    Does this data point or chart support the point I want people to walk away with?
    
    ### Effective presentation practices
    
    - Include a title, subtitle, and date. To start you should see the title slide: the title, who is presenting, and when it occurred.
    - Use a logical sequence of slides. Organizing your slides in an order that makes sense guides your audience through your narrative, building understanding step by step.
    - Provide an agenda with a timeline.
        
        ![image.png](Google_Data_Analytics_images/image.png)
        
    - Limit the amount of text on slides. Aim for your audience to scan it within 5 seconds.
    - Start with the business task.
    - Establish the initial hypothesis.
    - Show what business metrics you used.
    - Use visualizations.
    - Introduce the graphic by name.
    - Provide a title for each graph.
    - Go from the general to the specific.
    - Use speaker notes to help you remember talking points.
    - Include key takeaways. Summarize the main points at the end of your presentation.
    - Create an evaluation table. It gives you a checklist for each slide in a presentation so you can identify any changes that need to be made in an organized fashion.
        
        
        | **Slide #** | **What works well** | **What could be improved** |
        | --- | --- | --- |
        | 1 |  |  |
        | 2 |  |  |
        | 3 |  |  |
        | 4 |  |  |
    - As a data analyst, you have 2 key responsibilities: Analyze the data, and present your findings effectively.
    
    ### Presentation tips
    
    - Channel your excitement. Take deep, controlled breaths to calm your body down.
    - Start with the broader ideas.
    - Use the five second rule:
        - Wait five seconds after showing a data visualization
        - Ask if they understand it. If not, take time to explain it.
        - Give your audience another five seconds
        - Tell them the conclusion
    - Preparation is key.
    - You audience will not always see the steps you took to reach a conclusion. Focus on what information they need to reach the same conclusion you did.
    - You audience has a lot on their mind, so try to keep your presentation focused and to the point to keep their minds from wandering.
    - Your audience is easily distracted, so try to avoid including information in your presentations that you don’t think will be productive to discussions with your audience.
    - Pay attention on how you speak. Keep your sentences short, build in intentional pauses to give your audience time to think about what you’ve just said, keep the pitch of your sentences level.
    - Be mindful of nervous habits. Stay still and move with purpose, practice good posture, and make positive eye contact.
    
    Example of a great slide deck presentation: [https://docs.google.com/presentation/d/1jyZeBt2PizsVU4KdODvzAnUbcz7CIOq6Udvp0d5_jKs/template/preview?resourcekey=0-2M-Yk3_73NwAVg-PaLfvVA](https://docs.google.com/presentation/d/1jyZeBt2PizsVU4KdODvzAnUbcz7CIOq6Udvp0d5_jKs/template/preview?resourcekey=0-2M-Yk3_73NwAVg-PaLfvVA)
    
    ### Guide: Share data findings in presentations
    
    Sharing your data findings in presentations: Tips & tricks: https://d3c33hcgiwev3.cloudfront.net/_LwuiIoNSYq8LoiKDUmKxw_e8ff903b66b943ddaea3b8517fe8a3af_Sharing-your-data-findings-in-presentations-_-Tips-and-Tricks.pdf?Expires=1726790400&Signature=HFbk68lQOuZQ~8VZWo7SOap8K2izkmc0cPtmFyebw~AHa446EuilgoXTDcPCAMgH7omQKwGNkcVaHJ~psA1cBcCN3f5zz51BrAT7wPSEjxKsKrHf5os92Tu3ZvfXnEFQaIML6T1Z9md~wdEz4orq1c02LFIHFXo41wVKWT3yFrM_&Key-Pair-Id=APKAJLTNE6QMUY6HBC5A
    
    ## DATA CAVEATS AND LIMITATIONS
    
    The checklist below identifies 10 tasks that you should engage in to be prepared for your Q&A:
    
    Before the presentation
    
    1. Assemble and prepare your questions.
    2. Discuss your presentation with your manager, other analysts, or other friendly contacts in your organization.
    3. Ask a manager or other analysts what sort of questions were normally asked by your specific audience in the past.
    4. Seek comments, feedback, and questions on the deck or the document of your analysis.
    5. At least 24 hours ahead of the presentation, try and brainstorm tricky questions or unclear parts you may come across- this helps avoid surprises.
    6. It never hurts to practice what you will be presenting, to account for any missing information or simply to calm your nerves.
    
    During the presentation
    
    1. Be prepared to respond to the things that you find and effectively and accurately explain your findings.
    2. Address potential questions that may come up.
    3. Avoid having a single question derail a presentation and propose following-up offline.
    4. Put supplementary visualizations and content in the appendix to help answer questions.
    
    Be prepared to consider any limitations of your data by:
    
    - Critically analyzing the correlations.
    - Looking at the context
    - Understanding the strengths and weakness of the tools.
    
    ### Tips to handle objections
    
    Usually, these objections are:
    
    - About the data. You can include this information in the beginning of your presentation to set up the data context.
        - Where you got the data?
        - What systems it came from?
        - What transformations happened to it?
        - How fresh and accurate is the data?
    - Your analysis.
        - Is your analysis reproducible?
        - Who did you get feedback from?
    - Your findings
        - Do these findings exist in previous time periods?
        - Did you control for the differences in your data?
    
    Responding to possible objections:
    
    - Communicate any assumptions.
    - Explain why your analysis might be different than expected.
    - acknowledge that those objections are valid and take steps to investigate further.
    - Take steps to investigate further
    
    ### Q&A best practices
    
    - Listed to  the whole question.
    - Repeat the question (if necessary)
    - Use the Appendix. It’s a great place to keep extra information that might not be necessary for our presentation but could be useful for answering questions afterwards.
    - Understand the context.
    - Involve the whole audience.
    - Keep your responses short and to the point.
    
    ### Important aspects to a presentation
    
    - Define your purpose
    - Keep it concise
    - Have some logical flow to your presentation
    - Make the presentation visually compelling
    - How easy is it to understand?
    
- 7 - DATA ANALYSIS WITH **R** PROGRAMMING
    
    **Account at the RStudio Cloud** [Cloud sign-up page](https://rstudio.cloud/plans/free)
    
    The R programming language is useful for organizing, cleaning, and analyzing data.
    
    Programming helps you to:
    
    - Clarify the steps of your analysis
    - Save time
    - Reproduce and share your work
    
    R is a data-centric programming language frequently used for statistical analysis, visualization, and other data analysis. It’s based on another programming language named S.
    
    You can use R in many specific situations, like:
    
    - Reproducing your analysis
    - Processing lots of data
    - Creating data visualizations
    
    ### The R-versus-Python debate
    
    | Languages | R | Python |
    | --- | --- | --- |
    | **Common 
    features** | Open-source. Data stored in data frames. Formulas and functions readily available. Community for code development and support | Open-source. Data stored in data frames. Formulas and functions readily available. Community for code development and support |
    | **Unique advantages** | Data manipulation, data visualization, and statistics packages. "Scalpel" approach to data: *find packages to do what you want with the data* | Easy syntax for machine learning needs. Integrates with cloud platforms like Google Cloud, Amazon Web Services, and Azure |
    | **Unique challenges** | Inconsistent naming conventions. Make it harder for beginners to select the right functions. Methods for handling variables may be a little complex for beginners to understand | Many more decisions for beginners to make about data input/output, structure, variables, packages, and objects. "Swiss army knife" approach to data: *figure out a way to do what you want with the data* |
    
    R has been used by professionals who have a statistical or research-oriented approach to solving problems; among them are scientists, statisticians, and engineers. Python has been used by professionals looking for solutions in the data itself, those who must heavily mine data for answers; among them are data scientists, machine learning specialists, and software developers.
    
    ### Spreadsheets, SQL, and R: a comparison
    
    | **Key question** | **Spreadsheets** | **SQL** | **R** |
    | --- | --- | --- | --- |
    | **What is it?** | A program that uses rows and columns to organize data and allows for analysis and manipulation through formulas, functions, and built-in features | A database programming language used to communicate with databases to conduct an analysis of data | A general purpose programming language used for statistical analysis, visualization, and other data analysis |
    | **What is a primary advantage?** | Includes a variety of visualization tools and features | Allows users to manipulate and reorganize data as needed to aid analysis | Provides an accessible language to organize, modify, and clean data frames, and create insightful data visualizations |
    | **Which datasets does it work best with?** | Smaller datasets | Larger datasets | Larger datasets |
    | **What is the source of the data?** | Entered manually or imported from an external source | Accessed from an external database | Loaded with R when installed, imported from your computer, or loaded from external sources |
    | **Where is the data from my analysis usually stored?** | In a spreadsheet file on your computer | Inside tables in the accessed database | In an R file on your computer |
    | **Do I use formulas and functions?** | Yes | Yes | Yes |
    | **Can I create visualizations?** | Yes | Yes, by using an additional tool like a database management system (DBMS) or a business intelligence (BI) tool | Yes |
    
    ## R AND RSTUDIO
    
    Most analyst who work with the R language use the RStudio enviroment to interact with R, and not the basic interface. R is an IDE, this means that RStudio brings together all the tools you might want to use in a single place.
    
    Packages are units of reproducible R code. Members of the R community create packages to keep track of the R functions that they write and reuse. Packages offer a helpful combination of code, reusable R functions, descriptive documentation, tests for checking your code, and sample data sets. We will install and load the core *tidyverse* packages.
    
    ```r
    install.packages("tidyverse")
    ```
    
    To load the cores:
    
    ```r
    library(tidyverse)
    library(lubridate) #This is already part of the tidyverse package.
    ```
    
    - The RStudio enviroment has four main windows called panes. One of them is hidden (File > New File > R Script).
        
        ![image.png](Google_Data_Analytics_images/image%201.png)
        
        - Console: the place where you give commands to R.
        - Source editor: when working with R Scripts. If you write code directly in the R source editor, RStudio can save your code when you close your current session.
        - Environment: you’ll find all the data you currently have loaded and can easily organize and save it.
        - A pane that has tabs for Files, Plots, Packages, and Help.
    - R is key sensitive.
    
    ### The basic concepts of R
    
    The assignment operator is
    
    ```r
    <-
    ```
    
    Factors (R) store categorical data in R where the data values are limited and usually based on a finite group like country or year.
    
    - Functions. A body of reusable code used to perform specific tasks in R. Functions are key sensitive.
        - Argument (R): Information that a function in R needs in order to run
    - Comments. Helpful when you want to describe or explain what’s going on in your code. We use a #.
    - Variables. A representation of a value in R that can be stored for use later during programming. They can also be called objects.
    - Data types. Numeric, alphabetic, logical, data, and date time.
    - Vectors. A group of data elements of the same type stored in a sequence in R.
        
        ```r
        #c() function (called the "combine" function).
        vec_1 <- c(10,5,0.26,20,48.5)
        integer_vector <- c(1L, 5L, 15L) #must place the "L" after each number
        ```
        
    - Pipes. A tool in R for expressing a sequence of multiple operations, represented with “%>%”. It’s used to apply the output of one function into another function
    
    A data structure is a format for organizing and storing data. The most common data structures in the R programming language include:
    
    - Vectors
    - Data frames. The most common way of storing and analyzing data in R.
    - Matrices
    - Arrays
    
    ### Vectors in R
    
    Every vector you create will have two key properties: type and length. You can determine what type of vector you are working with by using:
    
    ```r
    typeof()
    typeof(c("a","b"))
    ```
    
    You can determine the length of an existing vector by using:
    
    ```r
    length()
    x <- c(33.5, 57.75, 120.05)
    length(x) #The result is 3
    ```
    
    You can use the next function to assign a different name to each element of the vector
    
    ```r
    names()
    x <- c(1, 3, 5)
    names(x) <- c("a", "b", "c")
    ```
    
    There are 2 types of vectors:
    
    - Atomic vectors. There are six primart types of atomic vectors:
        - Logical. True/False.
        - Integer. Positive and negative whole values. Known as numeric vectors
        - Double. Decimal values. Known as numeric vectors
        - Character (which contains strings). String/Character values
        - Complex. Not that common.
        - Raw. Not that common
        
        ![image.png](Google_Data_Analytics_images/image%202.png)
        
    - Lists
    
    ### Lists in R
    
    Lists are different from atomic vectors because their elements can be of any type.
    
    You can create a list with:
    
    ```r
    list()
    list("a", 1L, 1.5, TRUE)
    ```
    
    You can find out what types of elements a list contains, and the number of elements using:
    
    ```r
    str()
    z <- list(list(list(1 , 3, 5)))
    str(z)
    #output:
    	#> List of 1
    	#>  $ :List of 1
    	#>   ..$ :List of 3
    	#>   .. ..$ : num 1
    	#>   .. ..$ : num 3
    	#>   .. ..$ : num 5
    ```
    
    The indentation of the $ symbols reflect the nested structure of this list. Here, there are three levels (so there is a list within a list within a list).  
    
    Lists, like vectors, can be named. You can name the elements of a list when you first create it with the list() function:
    
    ```r
    list('Chicago' = 1, 'New York' = 2, 'Los Angeles' = 3)
    
    $`Chicago`
    
    [1] 1
    
    $`New York`
    
    [1] 2
    
    $`Los Angeles`
    
    [1] 3
    ```
    
    ### Dates and times in R
    
    Before you get started working with dates and times, you should load both tidyverse and lubridate. Lubridate is part of tidyverse. 
    
    ```r
    install.packages("tidyverse")
    library(tidyverse) #Loading tidyverse package
    library(lubridate) #Loading lubridate package
    ```
    
    In R, there are three types of data that refer to an instant in time:
    
    - A date ("2016-08-16"). R creates dates in the standard yyyy-mm-dd format by default.
    - A time within a day (“20:11:59 UTC")
    - And a date-time. This is a date plus a time ("2018-03-31 18:15:48 UTC")
    
    Some useful functions:
    
    ```r
    today() #To get the current date
    now() #To get the current date-time
    ymd("2021-01-20") #To convert a string into dates. It will return as a yyyy-mm-dd format
    mdy("January 20th, 2021") #To convert a string into dates. It will return as a yyyy-mm-dd format
    dmy("20-Jan-2021") #To convert a string into dates. It will return as a yyyy-mm-dd format
    mdy_hm("01/20/2021 08:01") #To convert a string into datetime
    	#> [1] "2021-01-20 08:01:00 UTC"
    as_date() #To convert a date-time to a date.
    ```
    
    ### Data Frames
    
    It’s a collection of columns containing data, similar to a spreadsheet or SQL table. Each column has a name that represents a variable and includes one observation per row. Data frames summarize data and organize it into a format that is easy to read and use. 
    
    Data Frames and tibbles are the building blocks for analysis in R.
    
    ```r
    data.frame() #To manually create a data frame.  
    data.frame(x = c(1, 2, 3) , y = c(1.5, 5.5, 7.5)) #x and y are the columns
    z <- data.frame(x = c(1, 2, 3) , y = c(1.5, 5.5, 7.5))
    z[2,1] #Extract operator to extract a subset from a data frame.
    	#[1] 2
    ```
    
    - The data.frame() function takes vectors as input.
    - When you use the extract operator [] on a data frame, it takes two arguments: the row(s) and column(s) you’d like to extract, separated by a comma.
    
    There are many ways to create a Date Frame. One of the most common is to create individual vectors of data and then combine them into a data frame using the `data.frame()` function.
    
    ```r
    #Example of Creating a DataFrame manually:
    names <- c("Peter", "Jennifer", "Julie", "Alex") #First, create a vector of names
    age <- c(15, 19, 21, 25) #Then create a vector of ages
    people <- data.frame(names, age) #With these two vectors, you can create a new data frame called `people`.
    ```
    
    There are some important things to know about DataFrames:
    
    - Columns should be named.
    - Data stored can be many different types, like numeric, factor, or character.
    - Each column should contain the same number of data items.
    
    ```r
    str() #To see the information about the DataFrame structure
    colnames() #To see the column names of the DataFrame
    mutate(*DataFrame*, *NewColumnName=Values*) #To make changes to the DataFrame. Part of the dplyr package.
    ```
    
    ### Tibbles
    
    Tibbles are like streamlined data frames that are automatically set to pull up only the first 10 rows of a dataset, and only as many columns as can fit on the screen. In the tidyverse, they:
    
    - Never change the data types of the inputs.
    - Never change the names of your variables.
    - Never create row names
    - Make printing in R easier. They’re automatically set to pull up only the first 10 rows and as many columns as fit on screen.
    
    ```r
    as_tibble() #To create a tibble from existing data
    ```
    
    ### Files
    
    ```r
    file.create() #To create a  blank file
    file.create("new_text_file.txt") 
    file.create("new_word_file.docx") 
    file.create("new_csv_file.csv") 
    file.copy("new_text_file.txt", "destination_folder") #To copy a file
    unlink("some_.file.csv") #To delete R files
    
    bookings_df <- read_csv("hotel_bookings.csv") #Example of the readr package.
    new_df <- select(bookings_df, `adr`, adults) #To create a new DF using specific columns from another DF
    ```
    
    - If the file is successfully created when you run the function, R will return a value of TRUE. Otherwise, R will return a value of FALSE.
    
    ### Matrices
    
    A matrix is a two-dimensional collection of data elements. This means it has both rows and columns. Matrices can only contain a single data type.
    
    ```r
    matrix(c(3:8), nrow = 2) #To create a matrix
    #Output>
         [,1] [,2] [,3]
    [1,]    3    5    7
    [2,]    4    6    8
    matrix(c(3:8), ncol = 2)
    #Output>
         [,1] [,2]
    [1,]    3    6
    [2,]    4    7
    [3,]    5    8
    ```
    
    - Matrix() function has two main arguments:
        - a vector
        - atleast one matrix dimension. You can choose to specify the number of rows or the number of columns by using the code nrow = or ncol =.
    
    ### Operators
    
    An operator is a symbol that identifies the type of operation or calculation to be performed in a formula. There are different types of operators:
    
    - Assignment operators: Used to assign values to variables and vectors.
        - “<-”, “<<-”, “=”. Leftwards assignment
        - “->”, “->>”. Rightwards assignment
    - Arithmetic operators: Used to complete math calculations. “+”, ”-”, “*”, “/”, “%%” (modulus, returns the remainder after division), “%/%” (integer, returns an integer value after division), “^” (Exponent).
    - Relational operators: also known as comparators, allow you to compare values. “<”, “>”, “!=”, “==”,…
    - Logical operators: Return a logical data type such as TRUE or FALSE. There are three primary types of logical operators (Zero is considered FALSE and non-zero numbers are taken as TRUE):
        - AND (sometimes represented as & or && in R)
        - OR (sometimes represented as | or || in R)
        - NOT (!). !TRUE evaluates to FALSE, and !FALSE evaluates to TRUE.
    - Conditional statement. A declaration that if a certain condition holds, then a certain event must take place.
        - if()
        - else if()
        - else()
    
    ### Packages (R)
    
    Units of reproducible R code. By default, R included a set of packages called base R. Packages include:
    
    - Reusable R functions
    - Documentation about the functions
    - Sample datasets
    - Tests for checking your code.
    
    We need to load our package first with a library command.
    
    You can find repositories on **[Bioconductor](http://bioconductor.org/), [R-Forge](https://r-forge.r-project.org/),  [rOpenSci](https://ropensci.org/)** or **[GitHub](https://github.com/),** but the most commonly used repository is the Comprehensive R Archive Network or [**CRAN**](https://cran.r-project.org/). CRAN stores code and documentation so that you can install packages into your own RStudio space. CRAN makes sure any R content open to the public meets the requires quality standards.
    
    The most useful package is tidyverse. Tidyverse is a system of packages in R with a common design philosophy for data manipulation, exploration, and visualization. The core tidyverse packages are (the first four are an essential part of the workflow for data analysts):
    
    - ggplot2. Create a variety of data viz by applying different visual properties to the data variables in R. Specifically plots.
    - tidyr. Used for data cleaning to make tidy data.
    - readr. Used for importing data. To accurately read a dataset you combine the function with a column specification, which describes how each column should be converted to the most appropriate data type. This isn’t usually necessary.
        - The readr package in R is a great tool for reading rectangular data. Rectangular data is data that fits nicely inside a rectangle of rows and columns, with each column referring to a single variable and each row referring to a single observation.
    - dplyr. Offers a consistent set of functions that help you complete some common data manipulations tasks.
        
        ```r
        data %>%
            filter(variable1 == "DS") %>%  
            ggplot(aes(x = weight, y = variable2, colour = variable1)) +  
            geom_point(alpha = 0.3,  position = position_jitter()) + stat_smooth(method = "lm")
        ```
        
    - tibble. Works with DataFrames
    - purrr. Works with functions and vectors, helping make your code easier to write and more expressive
    - stringr. Includes functions that make it easier to work with strings.
    - forcats. Provides tools that solve common problems with factors.
    
    ![image.png](Google_Data_Analytics_images/image%203.png)
    
    Conflicts happen when packages have functions with the same names as other functions. Conflict notifications are just one type of message that can show up in the console.
    
    Another useful packages are:
    
    - “here”. Makes referencing files easier.
    - “Skimr”. Simplify data cleaning tasks. Makes summarizing data really easy and let’s you skim through it more quickly.
    - “Janitor”. Simplify data cleaning tasks. It has functions for cleaning data.
    
    ### Pipes
    
    A tool in R that helps make your code more efficient and easier to read and understand. A tool in R for expressing a sequence of multiple operations, represented with “%>%”. It’s used to apply the output of one function into another function. This is described as nested.
    
    Nested in programming, describes code that performs a particular function and is contained within code that performs a broader function. A nested function is a function that is completely contained within another function.
    
    ![image.png](Google_Data_Analytics_images/image%204.png)
    
    Now we used a pipe:
    
    ![image.png](Google_Data_Analytics_images/image%205.png)
    
    Important things to keep in mind when using pipes:
    
    - Add the pipe operator at the end of each line of the piped operation except the last one.
    - Check your code after you’ve programmed your pipe.
    - Revisit piped operations to check for parts of your code to fix.
    
    ### Exploring and Cleaning Data in R
    
    Cleaning functions help you preview and rename data so that it’s easier to work with.
    
    Consistent data structures like Data Frames make it easier to operate on an entire dataset. Tidy data refers to the principles that make data structures meaningful and easy to understand. The tidy data standards are:
    
    - Variables are organized into columns.
    - Observations are organized into rows.
    - Each value must have its own cell.
    
    ```r
    data() #To load a specific dataset
    ```
    
    Some of the most important packages to clean data are “head”, “skimr” and “Janitor”.
    
    - Functions to get summaries of our DataFrame:
        - skim_without_charts(). Gives us a comprehensive summary of a dataset.
        - glimpse(). Show us a summary of the data.
        - head(). Get a preview of the column names and the first few rows of the dataset.
        - select(). To specify certain columns, or to exclude columns we don’t need right now. It’s useful for pulling just a subset of variables from a large dataset.
    - Functions to modify the dataset/DataFrame:
        
        ```r
        #"penguins" is the name of our Dataset
        penguins %>% rename(island_new=island)
        rename_with(penguins, tolower) #Makes all column name lowercase
        clean_names(penguins)
        ```
        
        - rename(). Makes it easy to change column names.
        - rename_with(). Can change column names to be more consistent. It can reformat column names to be upper or lower case.
        - clean_names(). This ensure there is only characters, numbers, and underscores in the names.
    
    ### Organizing your data in R
    
    Organizational functions help you sort, filter, and summarize your data.
    
    All packages we’ll need here are part of the core tidyverse
    
    ```r
    penguins %>% arrange(bill_length_mm) #Data sorted by bill_length_mm column in ascending order
    penguins %>% arrange(-bill_length_mm) #Data sorted by bill_length_mm column in descending order
    penguins 
    	%>% group_by(island) #groups data
    	%>% drop_na() #Addresses any missing values in our dataset.
    	%>% summarize(mean_bill_length_mm = mean(bill_length_mm)) #provides the mean of the column
    penguins %>% filter(species == "Adelie")
    ```
    
    ![image.png](Google_Data_Analytics_images/image%206.png)
    
    ![image.png](Google_Data_Analytics_images/image%207.png)
    
    - arrange(). To choose which variable we want to sort by.
    - group_by(). It’s usually combined with other functions.
    - filter().
    - summarize(). lets us get high level information about our data.
    
    ```r
    penguins %>% arrange(*DF_name,* bill_length_mm) #Data sorted by bill_length_mm column in ascending order
    penguins %>% arrange(*DF_name,* -bill_length_mm) #Data sorted by bill_length_mm column in descending order
    penguins %>% arrange(desc(bill_length_mm)) #Data sorted by bill_length_mm column in descending order
    penguins 
    	%>% group_by(island) #groups data
    	%>% drop_na() #Addresses any missing values in our dataset.
    	%>% summarize(mean_bill_length_mm = mean(bill_length_mm)) #provides the mean of the column
    penguins %>% filter(species == "Adelie")
    ```
    
    When organizing or tidying your data using R, you might need to convert wide data to long data or long to wide. For that, we use    
    
    - Wide data has observations across several columns. Each column contains data from a different condition of the variable. We use `pivot_wider()` to have more columns and fewer rows.
        
        ![image.png](Google_Data_Analytics_images/image%208.png)
        
    - Long data has all the observations in a single column, and the variable conditions are placed into separate rows. We use `pivot_longer()` to have more rows and fewer columns.
        
        ![image.png](Google_Data_Analytics_images/image%209.png)
        
    
    ### Transforming data in R
    
    Transformational functions help you separate and combine data, as well as create new variables.
    
    ```r
    #"employee" is the name of our Data Frame.
    separate(employee, name, into=c('first_name','last_name'), sep=' ')
    unite(employee,'name', first_name,last_name, sep=' ')
    ```
    
    - separate(). Splits one column into separate columns.
    - unite(). Allows us to merge columns together. It does the opposite of separate().
    - mutate(). Create new columns on our DataFrame.
    
    ### The bias function
    
    In R, we can actually quantify bias by comparing the actual outcome
    of our data with the predicted outcome. There's a pretty complicated statistical explanation behind this. But with the bias function in R, we don't have to perform this calculation by hand. Basically the bias function finds the average amount that the actual outcomeis greater than the predicted outcome. It's included in the sim design package.
    
    If the model is unbiased, the outcome should be pretty close to zero. A high result means that your data might be biased. A good thing to know before you analyze it.
    
    ![image.png](Google_Data_Analytics_images/image%2010.png)
    
    You can also use the “sample()” function to find bias. This function allows you to take a random sample of elements from a data set.
    
    ### Visualizations in R
    
    Some of the most popular packages for visualizations are:
    
    - ggplot2
    - Plotly
    - Lattice
    - RGL
    - Dygraphs
    - Leaflet
    - Highcharter
    - Patchwork
    - gganimate
    - ggridges
    
    We’ll use mainly ggplot2. The benefits of using this package are:
    
    - Create different types of plots
    - Customize the look and feel of plots. You can change the colors, layout, and dimensions of your plots and add text elements.
    - Create high quality visuals with just a little bit of code.
    
    The core concepts in ggplot2 are:
    
    - Aesthetics. A visual property of an object in your plot (position, color, shape, or size).
    - Geoms. The geometric object used to represent your data (lines, points, bars, etc)
    - Facets. Let you display smaller groups, or subsets, of your data.
    - Labels and annotations. Let you customize your plot.
    
    To create a plot, follow the next steps:
    
    1. Start with the ggplot function and choose a dataset to work with
    2. Add a geom_function to display your data
    3. Map the variables you want to plot in the arguments of the aes() function
    
    ![image.png](Google_Data_Analytics_images/image%2011.png)
    
    ```r
    ggplot(data = penguins) + geom_point(mapping = aes(x = flipper_length_mm, y = body_mass_g, shape=species, color=species, size=species))
    ggplot(data = penguins, mapping = aes(x = flipper_length_mm, y = body_mass_g)) +  geom_point()
    ```
    
    - The ggplot() function creates a coordinate system that you can add layers to.
    - Then, you add a “+” symbol to add a new layer to your plot. You complete your plot by adding one or more layers to ggplot().
    - Next, you choose a geom by adding a geom function. The geom_point() function uses points to create scatterplots, the geom_bar function uses bars to create bar charts, and so on.
    - Mapping in R means matching up a specific variable in your dataset with a specific aesthetic. Each geom function in ggplot2 takes a mapping argument. This defines how variables in your dataset are mapped to visual properties.
    - The mapping argument is always paired with the aes() function. This two tells R what aesthetics to use for the plot. You use the “aes” function to define the mapping between your data and your plot.
    - We can map more than one aesthetic to the same variable. Basic aesthetics for points are:
        - X
        - Y
        - Color. Change the color of all of the points on your plot, or the color of each data group.
        - Shape. Change the shape of the points on your plot by data group.
        - Size. Change the size of the points on your plot by data group.
        - Alpha. is a good option whe you’ve got a dense plot with lots of data points. This will make some points more transparent or see-through than others.
    - You can write the same section of code above using a different syntax with the mapping argument inside the ggplot() call
    - To change the appearance of our overall plot without regard to specific variables, we need to write the code outside of the aes function and quotation marks for the value, because all the code inside of the aes function tells R how to map aesthetics to variables.
        
        ```r
        ggplot(data = penguins) + geom_point(mapping = aes(x = flipper_length_mm, y = body_mass_g),color="purple")
        ```
        
    - ggplot [Cheat Sheet](https://ggplot2.tidyverse.org/)
    
    ### Geoms
    
    We use Geom functions for some graphics.
    
    ```r
    ggplot(data=penguins)+geom_smooth(mapping=aes(x=flipper_length_mm,y=body_mass_g))+geom_point(mapping=aes(x=flipper_length_mm,y=body_mass_g))
    ggplot(data=penguins)+geom_smooth(mapping=aes(x=flipper_length_mm,y=body_mass_g))+geom_jitter(mapping=aes(x=flipper_length_mm,y=body_mass_g))
    ggplot(data=diamonds)+geom_bar(mapping=aes(x=cut,fill=cut))
    ```
    
    - geom_point. Uses points to create scatter plots
    - geom_bar. Uses bars to create bar charts. We don’t need a variable for the y-axis. R automatically counts how many times each X value appears in the data and then shows the counts on the y-axis. The default is to count rows.
        - The “color” aesthetic adds color to the outline of each bar.
        - The “fill” aesthetic adds color to the inside of each bar. If we map it to a different variable to X, it will display a stacked bar chart.
    - geom_line. Uses lines to create a line chart.
    - geom_smooth. It’s useful for showing general trends in our data. It adds a smoothing line as another layer to a plot. There are 2 types of smoothing:
        - Loess: best for smoothing plots with less than 1000 points.
        - Gam: o generalized additive model smoothing, is useful for smoothing plots with a large number of points.
    - geom_jitter. It creates a scatter plot and then adds a small amount of random noise to each point in the plot. Jittering helps us deal with over-plotting, which happens when the data points in a plot overlap with each other.
    
    Some characteristics are:
    
    - We can use two geoms in the same plot, adding a + symbol after the first geom function.
    
    ### Facets
    
    A facet is a side or section of the object
    
    - It shows different sides of your data, by placing each subset on its own plot.
    - Faceting can help you discover new patterns in your data, and focus on relationships between different variables.
    - tilde operator (~) helps to define that dependent variable depends on the independent variable(s) that are on the right-hand side of tilde operator.
    
    There are 2 functions for faceting:
    
    ```r
    #the independent variable at the end of the syntax line would be species, and the dependent variable in this case is facet_wrap. 
    ggplot(data=penguins)+geom_point(mapping=aes(x=flipper_length_mm,y=body_mass_g,color=species))+facet_wrap(~species)
    #the independent variable at the end of the syntax line would be species, and the dependent variable in this case is sex.
    ggplot(data=penguins)+geom_point(mapping=aes(x=flipper_length_mm,y=body_mass_g,color=species))+facet_grid(sex~species)
    ```
    
    - facet_wrap(). Facet your plot by a single variable.
    - facet_grid(). Facet your plot with two variables. It’ll split the plot into facets vertically by the values of the first variable, and horizontally bt the facets of the second variable.
    
    ### Annotate and Label
    
    Label function is useful for adding informative labels to a plot such as
    
    - titles
    
    ```r
    ggplot(data=penguins)+
    	geom_point(mapping=aes(x=flipper_length_mm,y=body_mass_g,color=species))+
    		labs(title="Palmer Penguins: Body Mass vs. Flipper Length")
    ```
    
    - Subtitles
    
    ```r
    ggplot(data=penguins)+
    	geom_point(mapping=aes(x=flipper_length_mm,y=body_mass_g,color=species))+
    		labs(title="Palmer Penguins: Body Mass vs. Flipper Length", subtitle="Sample of Three Penguin Species")
    ```
    
    - Captions. Let us show the source of our data
    
    ```r
    ggplot(data=penguins)+
    	geom_point(mapping=aes(x=flipper_length_mm,y=body_mass_g,color=species))+
    		labs(title="Palmer Penguins: Body Mass vs. Flipper Length", subtitle="Sample of Three Penguin Species", caption="Data collected by Dr.Kristen Gorman")
    ```
    
    Annotate is to add notes to a document or diagram to explain or comment upon it. Adding annotation to your plot can help explain the plot’s purpose, or highlight important data. 
    
    ```r
    ggplot(data=penguins)+
    	geom_point(mapping=aes(x=flipper_length_mm,y=body_mass_g,color=species))+
    		labs(title="Palmer Penguins: Body Mass vs. Flipper Length", subtitle="Sample of Three Penguin Species", caption="Data collected by Dr.Kristen Gorman")+
    		annotate("text",x=220,y=3500,label="The Gentoos are the largest")
    ```
    
    - You can change the color, the font, angle or the size of the text
    
    ```r
    ggplot(data=penguins)+
    	geom_point(mapping=aes(x=flipper_length_mm,y=body_mass_g,color=species))+
    		labs(title="Palmer Penguins: Body Mass vs. Flipper Length", subtitle="Sample of Three Penguin Species", caption="Data collected by Dr.Kristen Gorman")+
    		annotate("text",x=220,y=3500,label="The Gentoos are the largest",color="purple",fontface="bold", size=4.5,angle=45)
    ```
    
    ### Saving your visualizations
    
    You will use the Export option on the plots tab, or the ggsave() function, provided by the ggplot2 package. 
    
    When you make a plot in R, it has to be “sent” to a specific graphics device. To save images without using ggsave(), you can open an R graphics device like **png()** or **pdf()**; these will allow you to save your plot as a .png or .pdf file. You can also choose to print the plot and then close the device using **dev.off()**.
    
    ![image.png](Google_Data_Analytics_images/image%2012.png)
    
    ggsave() function defaults to saving the last plot that you displayed, and use the size of the current graphics device.
    
    ```r
    ggsave("Three Penguin Species.png")
    ```
    
    ## DOCUMENTATION AND REPORTS IN R
    
    ### R Makrdown
    
    It’s a file format for making dynamic documents with R. It lets you create a record of your analysis and conclusions in a document.
    
    Besides text, R Markdown also includes an interactive option called an R notebook, that lets users run your code and show the graphs and charts that visualize the code.
    
    Any Markdown document can be used as a notebook. You can create:
    
    - HTML, PDF, and Word documents
    - Slite presentation
    - Dashboard
    
    Other notebook options are:
    
    - Jupyter
    - Kaggle
    - Google Colab
    
    To install the Markdown package:
    
    ```r
    install.packages("rmarkdown")
    ```
    
    - We will use the HTML default mode
    - Click on “Knit” to produce a report containing all text, code and results converted to  HTML, PDF or Word file.
    
    There are two shortcuts to adding code. On your keyboard, you can press **Ctrl** + **Alt** + **I** (PC) or **Cmd** + **Option** + **I** (Mac). Or you can click the **Add Chunk** command in the editor toolbar:
    
    ![image.png](Google_Data_Analytics_images/image%2013.png)
    
    A code chunk is code added in an .Rmd file.
    
    If you need to create a certain type of document over and over, or you want to customize the appearance of your final report, you can create a template.
    
- 8- CAPSTONE PROJECT
    
    What’s important here is to show off your thought process so that hte interviewers can understand how you approach the problem.
    
    Things to keep in mind in your Portfolio:
    
    - Make sure your case study answers the question being asked
    - Make sure that you’re communicating the steps you’ve taken and the assumptions you’ve made.
    - The best portfolios are personal, unique, and simple
    - Make sure that your portfolio is relevant and presentable
    
    ### What to include in your portfolio
    
    - Biography. Write a concise and clear introduction of yourself. The goal is to capture your audience’s interest and compel them to want to meet you to learn more.
    - Contact page.
    - Resume.
    - Accomplishments.
    - An image of you (optional).
    
    ### What to include in a case study
    
    - Introduction. State the purpose of the case study. This includes what the scenario is and an explanation on how it relates to a real-world obstacle.
    - Problems. You need to identify what the major problems are, explain how you have analyzed the problem, and present any facts you are using to support your findings.
    - Solutions. Outline a solution that would alleviate the problem and have a few alternatives in mind to show that you have given the case study considerable thought. Don’t forget to include pros and cons for each solution.
    - Conclusion. End your presentation by summarizing key takeaways of all of the problem-solving you conducted, highlighting what you have learned from this.
    - Next steps. Choose the best solution and propose recommendations for the client or business to take. Explain why you made your choice and how this will affect the scenario in a positive way. Be specific and include what needs to be done, who should enforce it, and when.
    
    ## CASE STUDY 1: HOW DOES A BIKE-SHARE NAVIGATE SPEEDY SUCCESS?
    
    ### Deliverable
    
    - A clear statement of the business task (**ASK**)
    - A description of all data sources used (**PREPARE**)
    - Documentation of any cleaning or manipulation of data (**PROCESS**)
    - A summary of your analysis (**ANALYZE**)
    - Supporting visualizations and key findings (**SHARE**)
    - Your top three recommendations based on your analysis (**ACT**)
    
    ### Roadmap
    
    1. **Ask.** How do annual members and casual riders use Cyclistic bikes defferently? 
    
    | Guiding questions | Answers | Key Tasks |
    | --- | --- | --- |
    | What is the problem you are trying to solve? |  | Identify the business task |
    | How can your insights drive business decisions? |  | Consider key Stakehorlders |
    1. **Prepare.** We’ll download the previous 12 months of Cyclistic trip data [here](https://divvy-tripdata.s3.amazonaws.com/index.html) (From January to December, 2023).
    
    | Guiding questions | Answers | Key Tasks |
    | --- | --- | --- |
    | Where is your data located? |  | Download data and store it appropriately |
    | How is the data organizes? |  | Identify how it’s organized |
    | Are there issues with bias or credibility in this data? |  | Sort and filter the data |
    | How are you addressing licensing, privacy, security, and accessibility? |  | Determine the credibility of the data |
    | How did you verify the data’s integrity? |  |  |
    | How does it help you answer your question? |  |  |
    | Are there any problems with the data? |  |  |
    1. **Process**.
    
    | Guiding questions | Answers | Key Tasks |
    | --- | --- | --- |
    | What tools are you choosing and why? |  | Check the data for errors |
    | Have you ensured your data’s integrity? |  | Choose your tools |
    | What steps have you taken to ensure that your data is clean? |  | Transform the data so you can work with it effectively |
    | How can you verify that your data is clean and ready to analyze? |  | Document the cleaning process |
    | Have you documented your cleaning process so you can review and share those results? |  |  |
    
    *** Threre are some useful steps to follow on the guide for this ***
    
    1. **Analyze**.
    
    | Guiding questions | Answers | Key Tasks |
    | --- | --- | --- |
    | How should you organize your data to perform analysis on it? |  | Aggegate your data so it’s useful and accessible |
    | Has your data been properly formatted? |  | Organize and format your data |
    | What surprises did you discover in the data? |  | Perform calculations |
    | What trends or relationships did you find in the data? |  | Identify trends and relationships |
    | How will these insights help answer your business quesitons? |  |  |
    
    *** Threre are some useful steps to follow on the guide for this ***
    
    1. **Share**. 
    
    | Guiding questions | Answers | Key Tasks |
    | --- | --- | --- |
    | Were you able to answer the question of how anual members and casual riders use Cyclistic bikes differently? |  | Determine the best way to share your findings |
    | What story does your data tell? |  | Create effective data visualizations |
    | How do your findings relate to your original question? |  | Present your findings |
    | Who is your audience? What is the best way to communicate with them? |  | Ensure your work is accessible |
    | Can data visualization help you share your findings? |  |  |
    | Is your presentation accessible to your audience? |  |  |
    
    *** Threre are some useful steps to follow on the guide for this ***
    
    1. **Act**.
    
    | Guiding questions | Answers | Key Tasks |
    | --- | --- | --- |
    | What is your final conclusion based on your analysis? |  | Create your portfolio |
    | How could your team and business apply your insights? |  | Add your case study |
    | What next steps would you or your stakeholders take based on your findings? |  | Practice presenting your case study to a friend or family member |
    | Is there additional data you could use to expand on your finidings? |  |  |
    
    *** Threre are some useful steps to follow on the guide for this ***
    
    ### Shate your portfolio
    
    When you’re thinking about where you want to share your portfolio, there’s two questions that can help you decide:
    
    1. What platforms align with your interests and passions?
    2. Where do you want to spend more time after this program?
    
    There are different options where you can share your portfolio:
    
    - Kaggle. Great option if you enjoy connecting with other data analysts
        - Broad data science community
        - Hosts a lot of competitions for users to join in
        - Offers all kinds of learning opportunities
    - GitHub.
        - Primarily used for programming languages like R or Python.
        - It has a more technical setup than other platforms, but it’s a great place to share your code and the how behind your analysis with other users.
        - Great place if you want to learn from other data analysts’ work
    - Blog platforms like Medium, WordPress and Google Sites
        - Are personalized and ownable.
        - You’ll have to store your code somewhere else
        - Opportunity to practice thought-leadership
        - Show off your expertise and write in your own voice.
    - Tableau.
        - Great option if you’re focused on the data viz side of things
        - You can create interactive dashboards
    
    ### How to add content to your potfolio
    
    | Platform | Information to help you manage your portfolio |
    | --- | --- |
    | GitHub | [8 steps to publishing your portfolio on GitHub](https://medium.com/tunapanda-institute/8-steps-to-publish-your-portfolio-on-github-9d6e6e3d2e84) |
    | Kaggle | [Publishing your first dataset on Kaggle](https://medium.com/analytics-vidhya/publishing-your-first-dataset-on-kaggle-6be8c37e59e8) |
    | Tableau | Any visualization created in Tableau Public is already public by default. A lot more is involved to add a Tableau visualization to another hosted site. For that reason, it is probably best to link to Tableau visualizations when your portfolio is hosted on a personal website or on a different platform, like GitHub. |
    | Mediu | [Getting started with a Medium publication](https://help.medium.com/hc/en-us/articles/115004681607-Getting-started-with-a-Medium-publication) |
    | WordPress | [Publish your site and share it publicly](https://wordpress.com/learn/get-published/) |
    | Google Sites | [Publish & share your site](https://support.google.com/sites/answer/6372880) or [Use a custom domain for your site](https://support.google.com/sites/answer/9068867) |
    
    ### Polish your portfolio
    
    There are some useful questions your should ask yourself to polish your portfolio:
    
    - Is there anything missing? Are you missing steps in your projects, or details in your descriptions?
        - If you have a website, are all the pages you need accounted for?
        - If you are hosting your portfolio on an existing platform, are all your projects uploaded properly?
    - Is there too much info?
        - Could any descriptions be revised for brevity?
        - Are there places where you include more data than you need? Could something be cut without losing the meaning or context of your project?
    - Is there anything you think you shouldn’t include?
        - Have you included references to others’ work that helped you without citing them? Can you remove them and instead include links to external work?
        - Are there any other components that might seem extraneous or unprofessional?
    - Is your portfolio hosted on the most appropriate platform?
        - There are many options for a data analytics platform, such as GitHub, Kaggle, and more. Is the one you’re using (or plan on using) the most appropriate for your needs?
    
    ### Tips for interview
    
    - [Interview Warmup](https://www.cloudskillsboost.google/interview-warmup). It’s a tool that helps you become more confident and comfortable throughout the interview process. Interview Warmup asks you interview questions to practice delivering your responses verbally. Your answers will be transcribed in real time, allowing you to review how you responded. In addition, Interview Warmup's machine learning algorithm can detect insights that can help you learn more about your answers and improve the way you communicate.
    
    There are some possible questions you might be asked in an interview:
    
    - Technical questions:
        - *“What are your preferred tools for analysis?”*
        - *“How do you maintain integrity in your data?”*
        - *“Do you understand different SQL functions and the roles they play?”*
    - Personal experience questions:
        - “*Was there a time when you took initiative during a project and what was the outcome?*” the goal is to understand your leadership abilities and how you have used them in the past.
        - “*What was the most challenging project you have ever been faced with*?” This question is usually meant to assess your problem-solving and interpersonal skills. Come to the interview prepared with several different examples of how you successfully navigated a difficult project or situation in the past.
        - “*How would you explain a complex topic to a stakeholder who was unfamiliar with it*?” This question helps your interviewer get a sense of how skilled you are at communicating effectively in high-pressure or sensitive circumstances.
        - “*How do you cope when things don’t go according to plan?*” This question provides a great opportunity for you to explain how you coped with unexpected changes and adapted quickly to a different course of action.
    - Questions to ask the interviewer:
        - “What are some upcoming projects I’d be working on?”
        - “What current goals is the company focused on?”
        - Can you tell me about the team I’ll be working with?”
    - Focus your experience on data
        
        ![image.png](Google_Data_Analytics_images/image%2014.png)
        
    
    ## AI FOR DATA ANALYTICS
    
    Artificial intelligence (AI) refers to computer systems that can perform cognitive tasks typically associated with human intelligence. You can use AI tools to augment and automate various data analysis tasks, such as:
    
    - Data cleaning and preparation
    - exploratory data analysis
    - Generating visualizations and reports.
    
    You can interact with a generative AI tool by entering a **prompt**, which is input that provides instructions to an AI tool about how to generate output. The tool then creates new content based on that prompt.
    
    A good prompt follows a simple framework: Task, Context, References, Evaluate, and Iterate, or T-C-R-E-I. If you ever forget a step, just remember: Thoughtfully Create Really Excellent Inputs. The more specific our prompt, the better the output.
    
    Gen AI Tools like Gemini can help data analysts identify data quality issues, standardize date formats, detect and remove duplicates, identify potential data set features, in record time.
    
    ### Tips for a good prompt
    
    - Providing the AI tool with more specific guidance about what you need.
    - Providing examples in your prompt can help the AI tool understand what you want.
    - Try breaking your instructions into shorter sentences. This can help you improve the output the next time.
    - One way you can iterate on your prompt is to ask the AI tool to show you YouTube videos or links from Google Search about the types of charts it suggested.
    - If you get an error when running a script, you can describe the issue to an AI tool, and it will suggest possible solutions.
    
    ### Prompt general examples
    
    - Clean data:
        
        ![image.png](Google_Data_Analytics_images/image%2015.png)
        
    - Organize data and build formulas:
        
        ![image.png](Google_Data_Analytics_images/image%2016.png)
        
    - Ask more effective questions:
        
        ![image.png](Google_Data_Analytics_images/image%2017.png)
        
    - Generate the steps for creating visualizations, as we aren’t uploading any actual data.
        
        ![image.png](Google_Data_Analytics_images/image%2018.png)
        
    - Improve your R code:
        
        ![image.png](Google_Data_Analytics_images/image%2019.png)
    
    - Data cleaning and preparation
    - exploratory data analysis
    - Generating visualizations and reports.
    
    You can interact with a generative AI tool by entering a **prompt**, which is input that provides instructions to an AI tool about how to generate output. The tool then creates new content based on that prompt.
    
    A good prompt follows a simple framework: Task, Context, References, Evaluate, and Iterate, or T-C-R-E-I. If you ever forget a step, just remember: Thoughtfully Create Really Excellent Inputs. The more specific our prompt, the better the output.
    
    Gen AI Tools like Gemini can help data analysts identify data quality issues, standardize date formats, detect and remove duplicates, identify potential data set features, in record time.
    
    ### Tips for a good prompt
    
    - Providing the AI tool with more specific guidance about what you need.
    - Providing examples in your prompt can help the AI tool understand what you want.
    - Try breaking your instructions into shorter sentences. This can help you improve the output the next time.
    - One way you can iterate on your prompt is to ask the AI tool to show you YouTube videos or links from Google Search about the types of charts it suggested.
    - If you get an error when running a script, you can describe the issue to an AI tool, and it will suggest possible solutions.
    
    ### Prompt general examples
    
    - Clean data:
        
        ![image.png](Google_Data_Analytics_images/image%2015.png)
        
    - Organize data and build formulas:
        
        ![image.png](Google_Data_Analytics_images/image%2016.png)
        
    - Ask more effective questions:
        
        ![image.png](Google_Data_Analytics_images/image%2017.png)
        
    - Generate the steps for creating visualizations, as we aren’t uploading any actual data.
        
        ![image.png](Google_Data_Analytics_images/image%2018.png)
        
    - Improve your R code:
        
        ![image.png](Google_Data_Analytics_images/image%2019.png)